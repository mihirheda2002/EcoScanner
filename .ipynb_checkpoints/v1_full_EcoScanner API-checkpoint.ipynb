{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from firebase import firebase\n",
    "import datetime\n",
    "from selenium import webdriver\n",
    "from urllib.request import urlopen as uReq\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import requests\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "import json\n",
    "\n",
    "fb_connect = firebase.FirebaseApplication(\"https://ecoscanner-cb66d.firebaseio.com/\",None)\n",
    "\n",
    "sw = stopwords.words(\"english\")\n",
    "\n",
    "# make sure \"v2_organic_operations_dict.json\" in directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def endpoints(x,lower=0,upper=10):\n",
    "    if x<lower:\n",
    "        return lower\n",
    "    elif x>upper:\n",
    "        return upper\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## NLP filtration method for products\n",
    "def check_nlp(itemvarieties,X_list,product,sw=sw):\n",
    "\n",
    "    # if cosine function doesn't work, use own alternative\n",
    "    def own_similarity_function(product,itemvarieties):\n",
    "\n",
    "\n",
    "        if product in itemvarieties:\n",
    "            return endpoints((len(product)*2/len(itemvarieties)))\n",
    "        elif itemvarieties in product:\n",
    "            return endpoints((len(itemvarieties)*2/len(product)))     \n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "\n",
    "    l1 =[];l2 =[] \n",
    "\n",
    "    # tokenization \n",
    "\n",
    "    Y_list = word_tokenize(itemvarieties) \n",
    "\n",
    "    # sw contains the list of stopwords \n",
    "\n",
    "\n",
    "    # remove stop words from string \n",
    "    X_set = {w for w in X_list if not w in sw} \n",
    "    Y_set = {w for w in Y_list if not w in sw} \n",
    "\n",
    "    # form a set containing keywords of both strings \n",
    "    rvector = X_set.union(Y_set) \n",
    "    for w in rvector: \n",
    "        if w in X_set:\n",
    "            l1.append(1) # create a vector \n",
    "        else:\n",
    "            l1.append(0) \n",
    "        if w in Y_set:\n",
    "            l2.append(1) \n",
    "        else:\n",
    "            l2.append(0) \n",
    "\n",
    "    c = 0\n",
    "\n",
    "    # cosine formula \n",
    "    for i in range(len(rvector)): \n",
    "        c+= l1[i]*l2[i] \n",
    "    \n",
    "    try:\n",
    "        cosine = c / float((sum(l1)*sum(l2))**0.5)\n",
    "    except ZeroDivisionError:\n",
    "        return own_similarity_function(product,itemvarieties)\n",
    "    \n",
    "    if cosine ==0:\n",
    "        return own_similarity_function(product,itemvarieties)\n",
    "    \n",
    "    return cosine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_product_values = {}\n",
    "query = \"038000014741\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creates a dictionary with all known info from upcitemdb.com\n",
    "\n",
    "def lookup(query):\n",
    "    ## append search UPC to the database website\n",
    "    my_url = \"https://www.upcitemdb.com/upc/\"+query\n",
    "    \n",
    "    #open urllib client, accounting for page not-existing\n",
    "    try:\n",
    "        uClient = uReq(my_url)\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "    # grab page html, save as soup\n",
    "    page_html = uClient.read()\n",
    "    uClient.close()\n",
    "    page_soup = soup(page_html,\"html.parser\")\n",
    "    \n",
    "    # find the table of item details, use first bc only 1\n",
    "    container = page_soup.findAll(\"table\",{\"class\":\"detail-list\"})[0]\n",
    "    \n",
    "    ## create blank dictionary\n",
    "    info_dict={}\n",
    "    \n",
    "    ## find product name\n",
    "    \n",
    "    # climb into blah is associated with blah\n",
    "    #pieces = str(page_soup.head.findAll(\"meta\")[1].meta).split(\",\")\n",
    "    #get rid of first thing\n",
    "    #pieces.pop(0)\n",
    "    \n",
    "    piece = page_soup.findAll(\"p\",{\"class\":\"detailtitle\"})[0]\n",
    "    \n",
    "    product_name = piece.b.text\n",
    "\n",
    "    \n",
    "    def clean(x):\n",
    "        x=x.replace(\"&amp\",\"&\")\n",
    "        for char in x:\n",
    "            if char.isnumeric():\n",
    "                tokens = x.split(char)\n",
    "                y = tokens[0]\n",
    "        if not y:    \n",
    "            return x.split(\",\")[0]\n",
    "        else:\n",
    "            return x.split(\",\")[0]\n",
    "\n",
    "    product_name = clean(product_name)\n",
    "\n",
    "    \n",
    "    ## find all other info\n",
    "    trs = container.findAll(\"tr\")\n",
    "    \n",
    "    ## loop through info things\n",
    "    for tr in trs:\n",
    "        \n",
    "        # make list, containing key line and value line\n",
    "        tds = tr.findAll(\"td\")\n",
    "        \n",
    "        ## parse out key and value, save to info dict\n",
    "        key = str(tds[0]).strip(\"<td>\").strip(\"</\").strip(\":\")\n",
    "        value = str(tds[1]).strip(\"<td>\").strip(\"</\").strip().strip(\"\\t\")\n",
    "        # don't care ab last-scanned\n",
    "        if key != \"Last Scanned\":\n",
    "            if key==\"Brand\":\n",
    "                for part in value.split():\n",
    "                    for part2 in part.split(','):\n",
    "                        product_name = product_name.replace(part2,\"\")\n",
    "                \n",
    "                info_dict[\"product name\"] = product_name.strip()\n",
    "            info_dict[key]=value\n",
    "    return info_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(lookup(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(lookup(\"013562300587\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#open urllib client, accounting for page not-existing\n",
    "\n",
    "uClient = uReq(\"https://www.upcitemdb.com/upc/071641053649\")\n",
    "\n",
    "\n",
    "# grab page html, save as soup\n",
    "page_html = uClient.read()\n",
    "uClient.close()\n",
    "page_soup = soup(page_html,\"html.parser\")\n",
    "\n",
    "pieces = str(page_soup.head.findAll(\"meta\")[1].meta).split(\",\")\n",
    "pieces.pop(0)\n",
    "print(\"pieces before splicing:\")\n",
    "print(pieces)\n",
    "\n",
    "output_str = \"\"\n",
    "a=True\n",
    "for piece in pieces:    \n",
    "    if piece==\" upc lookup database\":\n",
    "        a=False\n",
    "    print(\"piece\",piece,\"a\",a)\n",
    "    if a:\n",
    "        output_str+=piece\n",
    "print(output_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "upc_info_dict = lookup(query)\n",
    "brand = upc_info_dict[\"Brand\"]\n",
    "all_product_values[\"brand\"] = brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_product(product,brand):\n",
    "    sucky_chars = [\"-\",\":\",\";\",\"(\",\")\",\"_\",\"%\",\"#\",\"^\",\"&\",\"!\",\"oz\",\"Oz\"]\n",
    "    for char in sucky_chars:\n",
    "        product = product.replace(char,\"\").strip()\n",
    "    for brand_part in brand.split(\",\"):\n",
    "        product = product.replace(brand_part,\"\").strip()\n",
    "    return product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(clean_product(\"Annie's Low Sodium Mac Mild Cheddar Cheese 6 oz\",\"Annie's\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'UPC-A': '0 38000 01474 1', 'EAN-13': '0 038000 014741', 'Amazon ASIN': 'B00BT2M1BE', 'Country of Registration': 'United States', 'product name': 'Crunch Breakfast Cereal\\xa0in a Cup', 'Brand': \"Kellogg's Raisin Bran\", 'Model #': 'KEB01474', 'Size': '1.47 pounds', 'Color': 'Black', 'Weight': '1.5 Pounds', 'Product Dimension': '8.4 X 12.3 X 3.5 inches'}\n"
     ]
    }
   ],
   "source": [
    "print(upc_info_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_name = clean_product(upc_info_dict[\"product name\"],upc_info_dict[\"Brand\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_product_values[\"product name\"] = product_name\n",
    "product = product_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_company(brand):\n",
    "    \n",
    "    result = fb_connect.get(f\"/brand-comp/{brand}\",None)\n",
    "    if result!=None:\n",
    "        return result\n",
    "\n",
    "    def side_box_check(brand,page_soup):\n",
    "        parents = []\n",
    "        side_box_list = page_soup.findAll(\"div\",{\"class\":\"zloOqf PZPZlf\"})\n",
    "        for item in side_box_list:\n",
    "            if \"Parent\" in str(item):\n",
    "                mini_set = item.findAll(\"span\",{\"class\":\"LrzXr kno-fv\"})\n",
    "                mini_set = str(mini_set[0]).split(\",\")\n",
    "                for f1 in mini_set:\n",
    "                    for snippet in str(f1).split(\">\"):\n",
    "                        if \"</a\" in snippet:\n",
    "                            parents.append(snippet.strip(\"</a\").replace(\"&amp;\",\"&\"))\n",
    "        if len(parents)!=0:\n",
    "            return parents\n",
    "        else:\n",
    "            return table_check(brand,page_soup)\n",
    "        \n",
    "    def table_check(brand,page_soup):\n",
    "        try:\n",
    "            table = page_soup.findAll(\"div\",{\"class\":\"webanswers-webanswers_table__webanswers-table\"})[0]\n",
    "        except:\n",
    "            return lame_box_check(brand,page_soup)\n",
    "        rows = table.findAll(\"tr\")\n",
    "        for tr in rows:\n",
    "            if \"Owner\" in str(tr):\n",
    "                owner_line = str(tr.findAll(\"td\")[1])\n",
    "                trim1 = owner_line.split(\">\")[1]\n",
    "                trim2 = trim1.strip(\"</td>\")\n",
    "                return trim2.split(\",\")\n",
    "    \n",
    "    def lame_box_check(brand,page_soup):\n",
    "        output = []\n",
    "        not_comp = [\"company\",brand,\"parent\"]\n",
    "        try:\n",
    "            info = page_soup.findAll(\"span\",{\"class\":\"e24Kjd\"})[0]\n",
    "        except IndexError:\n",
    "            return brand\n",
    "        bolded = info.findAll(\"b\")\n",
    "        for bold in bolded:\n",
    "            bold = str(bold).strip(\"<b>\").strip(\"</b>\")\n",
    "            if bold not in not_comp:\n",
    "                output.append(bold)\n",
    "        for option in output:\n",
    "            if option in brand or brand in option:\n",
    "                output.pop(output.index(option))\n",
    "        return output\n",
    "\n",
    "    \n",
    "    my_url = f\"https://www.google.com/search?q={brand}+parent+company\"\n",
    "    user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36\"\n",
    "\n",
    "    headers = {\"user-agent\":user_agent}\n",
    "    resp = requests.get(my_url, headers=headers)\n",
    "\n",
    "    if resp.status_code == 200:\n",
    "        page_soup = soup(resp.content, \"html.parser\")\n",
    "    else:\n",
    "        return brand\n",
    "    \n",
    "    list_tag = page_soup.findAll(\"a\",{\"class\":\"FLP8od\"})\n",
    "    if len(list_tag)==0:\n",
    "        list_tag = page_soup.findAll(\"div\",{\"class\":\"Z0LcW AZCkJd\"})\n",
    "        if len(list_tag)==0:\n",
    "            list_tag = page_soup.findAll(\"div\",{\"class\":\"Z0LcW\"})\n",
    "            if len(list_tag)==0:\n",
    "                output = []\n",
    "                intake = side_box_check(brand,page_soup)\n",
    "                if isinstance(intake,str):\n",
    "                    return intake\n",
    "                for item in intake:\n",
    "                    output.append(item.replace(\"&amp;\",\"&\"))\n",
    "                return output\n",
    "    full_tag = str(list_tag[0])\n",
    "    splitted = full_tag.split(\">\")\n",
    "    pre_company = splitted[1]\n",
    "    post_company = pre_company.split(\"</\")[0]\n",
    "    answer = post_company.replace(\"&amp;\",\"&\")\n",
    "    result = fb_connect.put(\"/brand-comp/\",brand,answer)\n",
    "    return answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "owner_company = find_company(brand)\n",
    "all_product_values[\"owner company\"] = owner_company\n",
    "if type(owner_company)==\"list\":\n",
    "    owner_company=owner_company[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'brand': \"Kellogg's Raisin Bran\", 'product name': 'Crunch Breakfast Cereal\\xa0in a Cup', 'owner company': \"Kellogg's Raisin Bran\"}\n"
     ]
    }
   ],
   "source": [
    "print(all_product_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_splits(lst1,splitters_list):\n",
    "    def split_single(lst2,splitter):\n",
    "        out = []\n",
    "        for thing in lst2:\n",
    "            out.extend([x.strip() for x in thing.split(splitter)])\n",
    "        return out\n",
    "    \n",
    "    for splitter in splitters_list:\n",
    "\n",
    "        lst1 = split_single(lst1,splitter)\n",
    "    \n",
    "    return lst1\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jimmy', 'Hendrix', 'Andrea herbert', 'sugar', 'white refined flour', 'sunflower', 'why']\n"
     ]
    }
   ],
   "source": [
    "lst1_trial = [\"Jimmy,Hendrix\",\"Andrea herbert\",\"sugar: white refined flour\",\"sunflower. why\"]\n",
    "splitters_trial = [\",\",\":\",\".\",\"and/or\",\"and\",\"or\",\" of \"]\n",
    "print(multi_splits(lst1_trial,splitters_trial))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ingredients(product,brand,company):\n",
    "\n",
    "    all_results=[]\n",
    "    my_api_key = \"9ZTbuCeX9dP5keqlGJZyWNdg1c1xCu3p9pLhJ1MS\"\n",
    "    query = product.replace(\" \",\"%20\") # format more friendly\n",
    "    # make request with product name, brand owner, limit to 25 results\n",
    "    response = requests.get(f\"https://api.nal.usda.gov/fdc/v1/foods/search?api_key={my_api_key}&query={query}&brandOwner={brand}&pageSize=25\")\n",
    "    big_dict = response.json() # json format\n",
    "    foods = big_dict[\"foods\"] # list of all foods that match result\n",
    "    \n",
    "    # if less than 10 foods match search with that brand, check again replacing brand with company\n",
    "    if len(foods)<10:\n",
    "        response2 = requests.get(f\"https://api.nal.usda.gov/fdc/v1/foods/search?api_key={my_api_key}&query={query}&brandOwner={company}&pageSize=25\")\n",
    "        big_dict_2 = response.json()\n",
    "        foods2 = big_dict[\"foods\"]\n",
    "        foods.extend(foods2)\n",
    "        \n",
    "    \n",
    "    ingredients = set([]) #initialize empty set\n",
    "    for food in foods:\n",
    "        if \"ingredients\" in food.keys(): # if food has ingredients, get the string\n",
    "            string_mini_ingred = food[\"ingredients\"]\n",
    "            ingreds = multi_splits(string_mini_ingred.split(\"(\"),splitters_trial)\n",
    "            for half in ingreds:\n",
    "                if \"FLAVOR\" not in half: # get rid of \"Natural and Artifical flavor\", \"maintains flavor\", etc\n",
    "                    ingredients.add(half.strip().strip(\")\")) #clean up and add to ingredients set\n",
    "    \n",
    "    # cleaning blank strings out of list\n",
    "    x =[]\n",
    "    for ingredient in ingredients:\n",
    "        ingredient = ingredient.strip()\n",
    "        if ingredient!=\"\" and ingredient!=\" \" and len(ingredient)>3:\n",
    "            a = True\n",
    "            if \" \" in ingredient:\n",
    "                if len(ingredient[0:ingredient.index(\" \")])<0 or len(ingredient[ingredient.index(\" \"):-1])<2:\n",
    "                    a = False\n",
    "            if a:\n",
    "                trash = [\"[\",\"]\",\"*\",\"/\",\"organic\",\"for\",\"with\"]\n",
    "                nutrient = ingredient\n",
    "                nutrient = nutrient.lower()\n",
    "                for stink in trash:\n",
    "                    nutrient = nutrient.replace(stink,\"\")\n",
    "                nutrient = nutrient.strip()\n",
    "                x.append(nutrient)\n",
    "    \n",
    "\n",
    "    return x\n",
    "            \n",
    "        #keys = food.keys()\n",
    "        #if \"ingredients\" in keys:\n",
    "            #ingred_str = food[\"ingredients\"]\n",
    "            #all_matches.append({\"name\":food[\"description\"],\"ingredients\":ingred_str})\n",
    "    #return all_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nutrients = find_ingredients(product,brand,owner_company)\n",
    "\n",
    "## initialize selenium\n",
    "chromedriver = \"C:\\\\Users\\\\14082\\\\Documents\\\\Random Projects\\\\LancerHacks\\\\chromedriver83\"\n",
    "driver = webdriver.Chrome(chromedriver)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_nutrient(nutrient,fb_connect=fb_connect,driver=driver):\n",
    "\n",
    "    print(nutrient)\n",
    "    \n",
    "    # check if score already in firebase\n",
    "\n",
    "    try:\n",
    "        result = fb_connect.get(f\"/ingredient scores/{nutrient}\",None)\n",
    "    except:\n",
    "        result = None\n",
    "        print(\"nutrient was\",nutrient)\n",
    "        \n",
    "    if result:\n",
    "        if \"N/A\" in result:\n",
    "            return (0,\"none\")\n",
    "        print(result,type(result))\n",
    "        result_parts = result.split(\",\") # split the score and source\n",
    "        result_parts[0] = float(result_parts[0]) #turn the first part into a float\n",
    "        result_parts[1] = result_parts[1].strip() # get rid of space in source string\n",
    "        \n",
    "        return tuple(result_parts)\n",
    "\n",
    "    \n",
    "    # replace with location of chromedriver app on VM\n",
    "    # https://chromedriver.chromium.org/downloads\n",
    "    #chromedriver = \"C:\\\\Users\\\\14082\\\\Documents\\\\Random Projects\\\\LancerHacks\\\\chromedriver83\"\n",
    "\n",
    "    # open Chrome driver and try going to Hazard page for search chemical\n",
    "    #driver = webdriver.Chrome(chromedriver)\n",
    "    try:\n",
    "        driver.get(f\"https://comptox.epa.gov/dashboard/dsstoxdb/results?search={nutrient}#toxicity-values\")\n",
    "        \n",
    "        # find the label containing eco radio button, which will be visible; if error finding, page doesn't exist\n",
    "        label = driver.find_element_by_css_selector(\"label[class='b-radio radio button is-medium']\")\n",
    "    except:\n",
    "        #driver.close()\n",
    "        result = fb_connect.put(f\"/ingredient scores\",nutrient,\"N/A\") # add in firebase, don't waste time checking again\n",
    "        return (0,\"none\")\n",
    "\n",
    "    # empty string for hazard values\n",
    "    hazard_vals = []\n",
    "    \n",
    "    if not label.get_attribute(\"disabled\"): # check that eco is not disabled (arsenic example)\n",
    "        try:\n",
    "            label.click()\n",
    "        except:\n",
    "            exit = driver.find_element_by_css_selector(\"button[title='No thanks']\")\n",
    "            exit.click()\n",
    "            label.click()\n",
    "        source = \"eco\"\n",
    "    else:\n",
    "        source=\"human\"\n",
    "    \n",
    "    body = driver.find_element_by_css_selector(\"tbody\")\n",
    "    rows = body.find_elements_by_css_selector(\"tr\")\n",
    "    for row in rows:\n",
    "        cells = row.find_elements_by_css_selector(\"td\")\n",
    "        hazard_vals.append(int(cells[1].text)) # convert text from second cell per row into int, append\n",
    "    \n",
    "    hazard_vals.sort(reverse=True) #sort in descending order\n",
    "    hazard_vals = hazard_vals[0:3] #take only 3 highest scores\n",
    "    ave = sum(hazard_vals)/len(hazard_vals) #find ave of scores\n",
    "    score = ave-4 #subtract 4\n",
    "    \n",
    "    #driver.close() # close the browser\n",
    "    out_tup = (score,source) # conversion to tuple\n",
    "    \n",
    "    # save to firebase\n",
    "    result = fb_connect.put(f\"/ingredient scores\",nutrient,str(out_tup).strip(\")\").strip(\"(\").replace(\"\\'\",\"\"))\n",
    "\n",
    "    return out_tup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "barley\n",
      "beta-carotene\n",
      "vitamin a palmitate\n",
      "corn bran\n",
      "baking soda\n",
      "3.0, eco <class 'str'>\n",
      "vitamin b12\n",
      "vegetable glycerin\n",
      "fructose\n",
      "0.33333333333333304, eco <class 'str'>\n",
      "n fiber\n",
      "soybean and palm oil  tbhq  freshness\n",
      "vitamin b1\n",
      "niacinamide\n",
      "beta carotene  color\n",
      "nonfat milk\n",
      "honey\n",
      "annatto extract col\n",
      "cocoa\n",
      "sugar\n",
      "0, eco <class 'str'>\n",
      "calcium carbonate\n",
      "3.0, eco <class 'str'>\n",
      "turmeric extract  color\n",
      "corn syrup\n",
      "vitamin b12\n",
      "1.666666666666667, eco <class 'str'>\n",
      "freshness\n",
      "folic acid\n",
      "rice\n",
      "soybean andor cottonseed oil\n",
      "iron\n",
      "annatto extract  color\n",
      "thiamin hydrochl\n",
      "vitamin e)  freshness\n",
      "reduced iron\n",
      "annatto extract color\n",
      "hazelnuts\n",
      "minerals\n",
      "contains 2% or less of corn syrup\n",
      "preservative\n",
      "bic acid\n",
      "raisins\n",
      "tbhq  freshness\n",
      "wheat bran\n",
      "ascorbic acid and sodium ascorbate\n",
      "vitamin b1\n",
      "-1.0, human <class 'str'>\n",
      "vegetable oil\n",
      "3.0, human <class 'str'>\n",
      "vitamin d3\n",
      "salt\n",
      "3.0, eco <class 'str'>\n",
      "soluble wheat fiber\n",
      "cinnamon\n",
      "brown sugar\n",
      "salt\n",
      "3.0, eco <class 'str'>\n",
      "vitamin a palmitate\n",
      "3.0, eco <class 'str'>\n",
      "molasses\n",
      "rice flour\n",
      "sodium asc\n",
      "contains 2% or less of salt\n",
      "whole grain oats\n",
      "whole grain oats\n",
      "reduced iron\n",
      "vitamins\n",
      "almonds\n",
      "contains two percent or less of glycerin\n",
      "palm kernel\n",
      "spices\n",
      "n starch\n",
      "wheat bran\n",
      "malt flav\n",
      "riboflavin vitamin b2\n",
      "whole grain wheat\n",
      "vitamin b6\n",
      "allspice\n",
      "n flour blend\n",
      "wheat flour\n",
      "red 40\n",
      "inulin from chicory root\n",
      "cinnamon oleoresin\n",
      "nstarch\n",
      "modified food starch\n",
      "citric acid\n",
      "3.0, eco <class 'str'>\n",
      "cinnamon\n",
      "soy protein isolate\n",
      "n syrup\n",
      "canola\n",
      "chicory root fiber\n",
      "molasses\n",
      "buckwheat\n",
      "vitamin d3\n",
      "3.0, eco <class 'str'>\n",
      "iron\n",
      "3.0, eco <class 'str'>\n",
      "bate\n",
      "whole grain barley\n",
      "ascorbic acid\n",
      "whole grain yellow corn flour\n",
      "magnesium oxide\n",
      "3.0, eco <class 'str'>\n",
      "hydrogenated coconut\n",
      "palm oil\n",
      "artificial flav\n",
      "pyridoxine hydrochl\n",
      "ginger\n",
      "toasted coconut\n",
      "whole grain yellow c\n",
      "soy lecithin\n",
      "1.666666666666667, eco <class 'str'>\n",
      "hydrogenated coconut\n",
      "annatto color\n",
      "whole grain oat flour\n",
      "folic acid\n",
      "3.0, eco <class 'str'>\n",
      "vitamin b2 riboflavin\n",
      "pyridoxine hydrochloride\n",
      "brown sugar syrup\n",
      "palm andor canola oil\n",
      "rice cereal\n",
      "cane syrup\n",
      "oat fiber\n",
      "hard red wheat\n",
      "vitamin b6\n",
      "3.0, human <class 'str'>\n",
      "dextrose\n",
      "3.0, eco <class 'str'>\n",
      "zinc oxide\n",
      "bht  freshness\n",
      "dried cane syrup\n",
      "mixed tocopherols  freshness\n",
      "riboflavin\n",
      "glycerin\n",
      "contains 2% or less of coconut oil\n",
      "natural flav\n",
      "wheat flour\n",
      "dried apples\n",
      "palm andor canola\n",
      "processed  alkali\n",
      "whole grain wheat\n",
      "kashi seven whole grains and sesame blend\n",
      "palm kernel andor palm oil\n",
      "caramel color\n",
      "brown rice syrup\n",
      "sugar\n",
      "0, eco <class 'str'>\n",
      "niacinamide\n",
      "3.0, eco <class 'str'>\n",
      "pecans\n",
      "expeller pressed canola oil\n",
      "vanilla extract\n",
      "whole\n",
      "whole grain oat flour\n",
      "whole grain sorghum\n",
      "contains 2%\n",
      "soybean\n",
      "oat fiber\n",
      "baking soda\n",
      "3.0, eco <class 'str'>\n",
      "turmeric extract color\n",
      "skim milk\n",
      "dried apples\n",
      "natural\n",
      "milk\n",
      "sesame seeds\n",
      "paprika extract color\n",
      "soybean oil\n",
      "mixed tocopherols\n",
      "coconut\n",
      "whole grain corn\n",
      "n flour\n",
      "raisins\n",
      "cornstarch\n",
      "3.0, human <class 'str'>\n",
      "cane sugar\n",
      "-1.0, eco <class 'str'>\n",
      "triticale\n",
      "nonfat greek yogurt powder cultured skim milk\n",
      "oats\n",
      "vegetable oil\n",
      "3.0, human <class 'str'>\n",
      "less\n",
      "rice\n",
      "fruit juice  color\n",
      "palm oil\n",
      "cottonseed\n",
      "brown rice\n",
      "honey\n",
      "heat-treated after culturing)\n",
      "glycerin\n",
      "3.0, eco <class 'str'>\n",
      "whole wheat flour\n",
      "hydrogenated vegetable oil\n",
      "bifidobacterium lactis hn019\n",
      "soy flakes\n",
      "thiamin hydrochloride\n",
      "riboflavin\n",
      "-1.0, eco <class 'str'>\n",
      "sugar\n",
      "0, eco <class 'str'>\n",
      "chocolate\n",
      "vitamins and minerals\n",
      "soybean and palm oil\n",
      "vitamin b2\n",
      "soybean andor cottonseed\n",
      "vitamin b2\n",
      "-1.0, eco <class 'str'>\n",
      "brown sugar syrup\n",
      "turmeric extract col\n",
      "contains 2% or less of  dextrose\n",
      "zinc oxide\n",
      "3.0, eco <class 'str'>\n",
      "niacinamide\n",
      "3.0, eco <class 'str'>\n",
      "apple juice concentrate\n",
      "vitamin e acetate\n",
      "nutmeg\n",
      "degerminated yellow c\n",
      "modified corn starch\n",
      "contains 2% or less of brown sugar syrup\n",
      "yogurty probiotic piece\n",
      "-2.126666666666667\n"
     ]
    }
   ],
   "source": [
    "total_score = 0\n",
    "sources = []\n",
    "\n",
    "# for each ingredient in the list\n",
    "for nutrient in nutrients:\n",
    "    nutrient_score = score_nutrient(nutrient) # find the score \n",
    "    total_score+=nutrient_score[0] # add to total score\n",
    "    sources.append(nutrient_score[1]) # add source to a list of sources\n",
    "    \n",
    "num_used = len(sources)-sources.count(\"none\") # find the num of scores that aren't none\n",
    "ave_score = total_score/num_used \n",
    "\n",
    "# set upper and lower cap\n",
    "ave_score = endpoints(ave_score,lower=0,upper=4)\n",
    "\n",
    "ingredient_end_score = ave_score * -1\n",
    "all_product_values[\"ingredient score\"] = ingredient_end_score\n",
    "print(ingredient_end_score)\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DOWNLOAD ZIP FILE\n",
    "\n",
    "response = requests.get(f\"https://organicapi.ams.usda.gov/IntegrityPubDataServices/OidPublicDataService.svc/rest/GetAllOperationsPublicData?api_key={my_api_key}\",stream=True)\n",
    "target_path = 'organic_ops.zip'\n",
    "handle = open(target_path, \"wb\")\n",
    "for chunk in response.iter_content(chunk_size=512):\n",
    "    if chunk:  # filter out keep-alive new chunks\n",
    "        handle.write(chunk)\n",
    "handle.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as soup\n",
    "\n",
    "my_api_key = \"9ZTbuCeX9dP5keqlGJZyWNdg1c1xCu3p9pLhJ1MS\"\n",
    "json = {\"startldx\":\"1\",\"count\":\"1000\",\"countries\":[\"USA\"]}\n",
    "response = requests.post(f\"https://organicapi.ams.usda.gov/IntegrityPubDataServices/OidPublicDataService.svc/rest/Operations?api_key={my_api_key}\",json=json)\n",
    "all_text = response.text\n",
    "\n",
    "page_soup = soup(all_text,\"lxml\")\n",
    "page2 = soup(str(page_soup),\"html.parser\")\n",
    "operations_list = page2.html.body.operationsresult.operations.findAll(\"a:operation\")\n",
    "\n",
    "state_dict = {}\n",
    "def midinfo(x):\n",
    "    x = str(x)\n",
    "    return x.split(\">\")[1].split(\"<\")[0]\n",
    "for operation in operations_list:\n",
    "    operationname = midinfo(operation.findAll(\"a:operationname\")[0])\n",
    "    operationstatus = midinfo(operation.findAll(\"a:nopoperationstatus\")[0])\n",
    "    addresses = operation.findAll(\"a:addresses\")\n",
    "    address = addresses[0].findAll(\"a:address\")\n",
    "    state_untrimmed = str(address[0].findAll(\"a:stateorprovince\")[0])\n",
    "    state = midinfo(state_untrimmed)\n",
    "    print(operationname,operationstatus,state)\n",
    "    if state in state_dict.keys():\n",
    "        state_dict[state]+=1\n",
    "    else:\n",
    "        state_dict[state]=1\n",
    "print(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all_text = \"\"\"    <Operation>\n",
    "      <op_certifierName>[OTCO] Oregon Tilth Certified Organic</op_certifierName>\n",
    "      <op_nopOpID>8150000053</op_nopOpID>\n",
    "      <op_name>Wong Potatoes, Inc</op_name>\n",
    "      <op_clientID>OT-017900</op_clientID>\n",
    "      <op_contFirstName>Daniel</op_contFirstName>\n",
    "      <op_contLastName>Chin</op_contLastName>\n",
    "      <op_status>Certified</op_status>\n",
    "      <op_statusEffectiveDate>2014-11-06</op_statusEffectiveDate>\n",
    "      <op_nopAnniversaryDate>2020-04-01</op_nopAnniversaryDate>\n",
    "      <op_lastUpdatedDate>2019-05-15</op_lastUpdatedDate>\n",
    "      <opSC_HANDLING>Certified</opSC_HANDLING>\n",
    "      <opSC_HANDLING_ED>2014-11-06</opSC_HANDLING_ED>\n",
    "      <op_phone>(541) 798-5353</op_phone>\n",
    "      <op_email>chinfarms@gmail.com</op_email>\n",
    "      <opMA_line1>17600 Hwy 39</opMA_line1>\n",
    "      <opMA_city>Klamath Falls</opMA_city>\n",
    "      <opMA_state>Oregon</opMA_state>\n",
    "      <opMA_country>United States of America (the)</opMA_country>\n",
    "      <opMA_zip>97603</opMA_zip>\n",
    "    </Operation>\n",
    "    <Operation>\n",
    "      <op_certifierName>[OTCO] Oregon Tilth Certified Organic</op_certifierName>\n",
    "      <op_nopOpID>8150000052</op_nopOpID>\n",
    "      <op_name>High Mountain LLC</op_name>\n",
    "      <op_clientID>OT-004440</op_clientID>\n",
    "      <op_contFirstName>Kevin</op_contFirstName>\n",
    "      <op_contLastName>Christensen</op_contLastName>\n",
    "      <op_status>Suspended</op_status>\n",
    "      <op_statusEffectiveDate>2011-11-16</op_statusEffectiveDate>\n",
    "      <op_lastUpdatedDate>2017-12-27</op_lastUpdatedDate>\n",
    "      <opSC_CR>Suspended</opSC_CR>\n",
    "      <opMA_line1>PO Box 968</opMA_line1>\n",
    "      <opMA_city>Mattawa</opMA_city>\n",
    "      <opMA_state>Washington</opMA_state>\n",
    "      <opMA_country>United States of America (the)</opMA_country>\n",
    "      <opMA_zip>99349</opMA_zip>\n",
    "    </Operation>\n",
    "    <Operation>\n",
    "      <op_certifierName>[OTCO] Oregon Tilth Certified Organic</op_certifierName>\n",
    "      <op_nopOpID>8150000051</op_nopOpID>\n",
    "      <op_name>Marvin Lynch</op_name>\n",
    "      <op_otherNames>DBA O'lynch Dairy</op_otherNames>\n",
    "      <op_clientID>OT-007458</op_clientID>\n",
    "      <op_contFirstName>Marvin</op_contFirstName>\n",
    "      <op_contLastName>Lynch</op_contLastName>\n",
    "      <op_status>Certified</op_status>\n",
    "      <op_statusEffectiveDate>2009-05-04</op_statusEffectiveDate>\n",
    "      <op_nopAnniversaryDate>2020-01-01</op_nopAnniversaryDate>\n",
    "      <op_lastUpdatedDate>2019-05-15</op_lastUpdatedDate>\n",
    "      <opSC_CR>Certified</opSC_CR>\n",
    "      <opSC_CR_ED>2009-05-04</opSC_CR_ED>\n",
    "      <opSC_LS>Certified</opSC_LS>\n",
    "      <opSC_LS_ED>2009-05-04</opSC_LS_ED>\n",
    "      <op_phone>(563) 852-5285</op_phone>\n",
    "      <op_email>marvlynch@yahoo.com</op_email>\n",
    "      <opMA_line1>24764 Hwy 151 W</opMA_line1>\n",
    "      <opMA_city>Cascade</opMA_city>\n",
    "      <opMA_state>Iowa</opMA_state>\n",
    "      <opMA_country>United States of America (the)</opMA_country>\n",
    "      <opMA_zip>52033</opMA_zip>\n",
    "    </Operation>\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "huge_dict={}\n",
    "all_text = all_text.replace(\"\\n\",\"\")\n",
    "all_operations = all_text.split(\"</Operation>\")\n",
    "concerns = [\"op_nopOpID\",\"op_name\",\"opMA_state\"]\n",
    "for operation in all_operations:\n",
    "    if \"United States\" in operation and \"status>Certified<\" in operation:\n",
    "        internal_dict = {}\n",
    "        deets = operation.split(\"</\")\n",
    "        for deet in deets:\n",
    "            for concern in concerns:\n",
    "                if \"<\"+concern+\">\" in deet:\n",
    "                    internal_dict[concern]=deet.split(\">\")[-1]\n",
    "        huge_dict[internal_dict[\"op_name\"]] = internal_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##CREATING DICTIONARY OF ALL ORGANIC OPERATIONS\n",
    "\n",
    "all_text = \"\"\n",
    "op_file = open(\"organic_ops_file.txt\",\"r\",encoding=\"utf-8\")\n",
    "for line in op_file.readlines():\n",
    "    all_text+=line.strip(\"\\n\")\n",
    "op_file.close()\n",
    "\n",
    "huge_dict={}\n",
    "all_text = all_text.replace(\"\\n\",\"\")\n",
    "all_operations = all_text.split(\"</Operation>\")\n",
    "concerns = [\"op_nopOpID\",\"op_name\",\"opMA_state\"]\n",
    "for operation in all_operations:\n",
    "    if \"United States\" in operation and \"status>Certified<\" in operation:\n",
    "        internal_dict = {}\n",
    "        deets = operation.split(\"</\")\n",
    "        for deet in deets:\n",
    "            for concern in concerns:\n",
    "                if \"<\"+concern+\">\" in deet:\n",
    "                    internal_dict[concern]=deet.split(\">\")[-1]\n",
    "        huge_dict[internal_dict[\"op_name\"]] = internal_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WRITING INTO JSON FILE\n",
    "\n",
    "import json\n",
    "\n",
    "json_format = json.dumps(mega_dict)\n",
    "f = open(\"v2_organic_operations_dict.json\",\"w\")\n",
    "f.write(json_format)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating MegaDict\n",
    "\n",
    "def create_dict(filename=\"v2_organic_operations_dict.json\"):    \n",
    "    with open(filename,\"r\") as json_file:\n",
    "        mega_dict = json.load(json_file)\n",
    "        if type(mega_dict)==\"str\":\n",
    "            print(\"oops string error\")\n",
    "        else:\n",
    "            return mega_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## trying to fix capitalization in the dictionary\n",
    "import string\n",
    "mega_dict = {}\n",
    "for key in huge_dict.keys():\n",
    "    if key.isupper():\n",
    "        fixed = string.capwords(key)\n",
    "        huge_dict[key][\"op_name\"] = fixed\n",
    "        mega_dict[fixed] = huge_dict[key]\n",
    "    else:\n",
    "        mega_dict[key] = huge_dict[key]\n",
    "print(mega_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try:\n",
    " #   y = mega_dict\n",
    "#except NameError:\n",
    " #   mega_dict = create_dict()\n",
    "\n",
    "mega_dict = create_dict()\n",
    "    \n",
    "def find_operation_id(brand,owner_company=owner_company,mega_dict=mega_dict):\n",
    "    \n",
    "    # first check firebase\n",
    "    result = fb_connect.get(f\"/brand-opID/{brand}\",None)\n",
    "    if result:\n",
    "        return result\n",
    "    \n",
    "    brand = brand.replace(\",\",\"\")\n",
    "    found_keys = []\n",
    "    dict_keys=mega_dict.keys()\n",
    "    for key in dict_keys:\n",
    "        if brand in key:\n",
    "            found_keys.insert(0,[key,mega_dict[key][\"op_nopOpID\"]]) #make first term in found_keys a list with key, and opID\n",
    "    if len(found_keys)>1:\n",
    "        \n",
    "        # default input to similarity function, only need to tokenize and find stopwords once\n",
    "        X_list = word_tokenize(brand) \n",
    "\n",
    "        for hit in found_keys:\n",
    "            sim_dict = {}\n",
    "            name = hit[0]\n",
    "            similarity_score = check_nlp(name,X_list,brand)\n",
    "            sim_dict[similarity_score] = hit\n",
    "        max_sim = max(sim_dict.keys())\n",
    "        found_keys = sim_dict[max_sim]\n",
    "\n",
    "    else:\n",
    "        if owner_company!=\"\":\n",
    "            found_keys = find_operation_id(owner_company,owner_company=\"\")\n",
    "    if len(found_keys)==0:\n",
    "        return None\n",
    "    if type(found_keys)==\"list\":\n",
    "        opID = found_keys[1]\n",
    "    else:\n",
    "        opID = found_keys\n",
    "    response = fb_connect.put(\"/brand-opID\",brand,opID)\n",
    "    return opID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opID = find_operation_id(brand,owner_company)\n",
    "opID = find_operation_id(\"Annie's\",\"General Mills\")\n",
    "\n",
    "all_product_values[\"organic operation ID\"] = opID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(opID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Receiving Operation Products based on op_code\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as soup\n",
    "#\n",
    "op_id = \"5561001956\"\n",
    "#\n",
    "my_api_key = \"9ZTbuCeX9dP5keqlGJZyWNdg1c1xCu3p9pLhJ1MS\"\n",
    "json = {\"startIdx\":\"1\",\"count\":\"1000\",\"operationId\":op_id}\n",
    "response = requests.post(f\"https://organicapi.ams.usda.gov/IntegrityPubDataServices/OidPublicDataService.svc/rest/Items?api_key={my_api_key}\",json=json)\n",
    "# parsing into a list of items\n",
    "all_text = response.text\n",
    "page_soup = soup(all_text,\"lxml\")\n",
    "page2 = soup(str(page_soup),\"html.parser\")\n",
    "\n",
    "items = page2.findAll(\"a:item\")\n",
    "\n",
    "if len(items)==0:\n",
    "    return \"Error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Receiving Operation Products based on op_code\n",
    "\n",
    "def find_itemvarieties(item,tag=\"itemvarieties\"):\n",
    "    deets = item.split(\"</\")\n",
    "    for deet in deets:\n",
    "        if f\"<a:{tag}>\" in deet:\n",
    "            itemvarieties = deet.split(\">\")[-1]\n",
    "            return itemvarieties\n",
    "        \n",
    "def organic_info(op_id,product,brand):\n",
    "        \n",
    "    \n",
    "    do_trash=True\n",
    "    if do_trash:\n",
    "        # get rid of common words that are clutter, this list should grow with testing\n",
    "        trash_list = [\"Organic \",brand,\"Super \",\"Green \",\"Ounce\",\"ounce\",\"count\",\"Count\"]\n",
    "        for stink in trash_list:\n",
    "            product = product.replace(stink,\"\").strip()\n",
    "        if \" \" in product:\n",
    "            for word in product.split():\n",
    "                for stink in trash_list:\n",
    "                    if stink in word or word.isnumeric():\n",
    "                        product = product.replace(word,\"\").strip()\n",
    "        product = product.split(\",\")[0]\n",
    "        product = product.replace(brand,\"\").strip()\n",
    "    \n",
    "    # default input to similarity function, only need to tokenize and find stopwords once\n",
    "    X_list = word_tokenize(product) \n",
    "\n",
    "    ## API request\n",
    "    my_api_key = \"9ZTbuCeX9dP5keqlGJZyWNdg1c1xCu3p9pLhJ1MS\"\n",
    "    json = {\"startIdx\":\"1\",\"count\":\"100\",\"operationId\":op_id}\n",
    "    response = requests.post(f\"https://organicapi.ams.usda.gov/IntegrityPubDataServices/OidPublicDataService.svc/rest/Items?api_key={my_api_key}\",json=json)\n",
    "\n",
    "    # parsing into a list of items\n",
    "    all_text = response.text\n",
    "    page_soup = soup(all_text,\"lxml\")\n",
    "    page2 = soup(str(page_soup),\"html.parser\")\n",
    "    items = page2.findAll(\"a:item\")\n",
    "\n",
    "    if len(items)==0:\n",
    "        return None\n",
    "\n",
    "    # TEST INFORMATION\n",
    "    #company = \"Forager Project\"\n",
    "    #brand = \"Forager Project\"\n",
    "    #product = \"Super Green Leafy Greens Chips\"\n",
    "    #\n",
    "\n",
    "    # create list of API response terms we care about\n",
    "    concerns = [\"itemname\",\"itemvarieties\",\"madewithorganic\",\"organic\",\"organic100\",\"otheritems\"]\n",
    "    \n",
    "    possible_matches = []\n",
    "    best_choice = {\"similarity score\":0.2} # initial threshold for any match\n",
    "    for item in items:\n",
    "        cont = True\n",
    "        item = str(item)\n",
    "        if \"status>Certified<\" in item:\n",
    "            #try:\n",
    "            itemvarieties = find_itemvarieties(item)\n",
    "            if itemvarieties==\"\":\n",
    "                itemvarieties = find_itemvarieties(item,tag=\"otheritems\")\n",
    "            if itemvarieties ==\"\":\n",
    "                itemvarieties = find_itemvarieties(item,tag=\"itemname\")\n",
    "\n",
    "            similarity_score = check_nlp(itemvarieties,X_list,product)\n",
    "                \n",
    "            if similarity_score>best_choice[\"similarity score\"]:\n",
    "                best_choice = {\"similarity score\":similarity_score}\n",
    "                deets = item.split(\"</\")\n",
    "                for deet in deets:\n",
    "                    for concern in concerns:\n",
    "                        inclusion_str = \"<a:\"+concern\n",
    "                        exclusion_str = \"OASNFOIAJDFOAJFDOINOANEFADUB\"\n",
    "                        if concern==\"organic\":\n",
    "                            exclusion_str = \"organic100\"\n",
    "                        if inclusion_str in deet and exclusion_str not in deet:\n",
    "                            best_choice[concern]=deet.split(concern)[-1].strip(\">\").replace(\"&amp;\",\"&\")\n",
    "    return best_choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(organic_info(\"5520336912\",\"Lemon\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit except 1\n",
      "hit except 2\n",
      "hit except 3\n"
     ]
    }
   ],
   "source": [
    "if not opID:\n",
    "    organic = 0\n",
    "else:\n",
    "    \n",
    "    organic = 1 # default for if the rest doesn't work\n",
    "    \n",
    "    ## come here and write the actual function to produce organic score\n",
    "    organic_info_var = organic_info(opID,product,brand)\n",
    "    all_product_values[\"organic info\"] = organic_info_var\n",
    "    try:\n",
    "        if \"true\" == organic_info_var[\"organic100\"]:\n",
    "            organic = 3\n",
    "    except:\n",
    "        print(\"hit except 1\")\n",
    "        y=0\n",
    "    try:\n",
    "        if \"true\" == organic_info_var[\"organic\"]:\n",
    "            organic = 2\n",
    "    except:\n",
    "        print(\"hit except 2\")\n",
    "        y=0\n",
    "    try:\n",
    "        if \"true\" == organic_info_var[\"madewithorganic\"]:\n",
    "            print(\"worked where it was supposed to\")\n",
    "            organic = 1\n",
    "    except:\n",
    "        print(\"hit except 3\")\n",
    "        y=0\n",
    "\n",
    "all_product_values[\"organic\"] = organic\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(organic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####    # get rid of common words that are clutter, this list should grow with testing\n",
    "    trash_list = [\"Organic \",brand,company,\"Super \",\"Green \",\"Ounce\",\"ounce\",\"count\",\"Count\"]\n",
    "    for stink in trash_list:\n",
    "        product = product.replace(stink,\"\").strip()\n",
    "    for word in product.split():\n",
    "        for stink in trash_list:\n",
    "            if stink in word or word.isnumeric():\n",
    "                product = product.replace(word,\"\").strip()\n",
    "\n",
    "    #print(\"product before recombo is\",product)\n",
    "\n",
    "    # creating different product variations to test against\n",
    "    product_combos = [product]    \n",
    "    for word in product.split():\n",
    "        product_combos.append(product.replace(word,\"\").strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#organic_info(\"8150000355\",\"Annie's Homegrown Organic Pretzel Bunnies, 7 oz, Pack of 12\",\"Annie's Inc.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import nltk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nltk.download(\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Program to measure similarity between \n",
    "# two sentences using cosine similarity. \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "product = \"Super Green Leafy Greens Chips\"\n",
    "\n",
    "# default input to function, only need to tokenize and find stopwords once\n",
    "X_list = word_tokenize(product) \n",
    "sw = stopwords.words('english') \n",
    "\n",
    "def compare_terms(itemvarieties,X_list=X_list,sw=sw):\n",
    "    \n",
    "    l1 =[];l2 =[] \n",
    "\n",
    "    # tokenization \n",
    "\n",
    "    Y_list = word_tokenize(itemvarieties) \n",
    "\n",
    "    # sw contains the list of stopwords \n",
    "\n",
    "\n",
    "    # remove stop words from string \n",
    "    X_set = {w for w in X_list if not w in sw} \n",
    "    Y_set = {w for w in Y_list if not w in sw} \n",
    "\n",
    "    # form a set containing keywords of both strings \n",
    "    rvector = X_set.union(Y_set) \n",
    "    for w in rvector: \n",
    "        if w in X_set:\n",
    "            l1.append(1) # create a vector \n",
    "        else:\n",
    "            l1.append(0) \n",
    "        if w in Y_set:\n",
    "            l2.append(1) \n",
    "        else:\n",
    "            l2.append(0) \n",
    "\n",
    "    c = 0\n",
    "\n",
    "    # cosine formula \n",
    "    for i in range(len(rvector)): \n",
    "        c+= l1[i]*l2[i] \n",
    "    cosine = c / float((sum(l1)*sum(l2))**0.5) \n",
    "    return cosine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import requests\n",
    "\n",
    "product = \"Superco Green Cleaner & Degreaser\"\n",
    "safer_choice_url = f\"https://enviro.epa.gov/enviro/efservice/T_SAFERCHOICE/PRODUCT NAME/CONTAINING/{product}/JSON\"\n",
    "response = requests.get(safer_choice_url)\n",
    "\n",
    "dictionary = response.json()\n",
    "\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GREEN SEAL by Product "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef find_greenseal(product):\\n   \\n    # default input to similarity function, only need to tokenize and find stopwords once\\n    X_list = word_tokenize(product) \\n\\n    # create search url, define user agent, make request\\n    search_product = product.replace(\" \",\"%20\")\\n    my_url = f\"https://www.greenseal.org/api/search/{search_product}?service_type=\"\\n    user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36\"\\n    headers = {\"user-agent\":user_agent}\\n    resp = requests.get(my_url, headers=headers)\\n\\n    # if succesful, employ bs4; return None of get request not success\\n    if resp.status_code == 200:\\n        page_soup = soup(resp.content, \"html.parser\")\\n    else:\\n        return None\\n    \\n    # convert big json response to dictionary and enter results list\\n    results_dict = json.loads(page_soup.text)\\n    results_list = results_dict[\"results\"]    \\n    \\n    # return None if results is empty\\n    if len(results_list)==0:\\n        return None\\n    \\n    # initialize empty output list\\n    item_info = []\\n    \\n    # iterate through each result, pulling out info we care about\\n    for result in results_list:\\n        mini_dict = {} #empty dictionary to append to list for this product\\n        product_name = result[\"name\"]\\n        if len(item_info)>=1: # if  already found hit, start nlp cross checking\\n            similarity_score = check_nlp(product_name,X_list,product)\\n            if similarity_score >= 0.3+ 0.05*len(item_info):\\n                mini_dict[\"name\"] = product_name\\n                mini_dict[\"company\"] = result[\"mfg\"]\\n                mini_dict[\"category\"] = result[\"category\"]\\n                item_info.append(mini_dict)\\n        else:\\n                mini_dict[\"name\"] = product_name\\n                mini_dict[\"company\"] = result[\"mfg\"]\\n                mini_dict[\"category\"] = result[\"category\"]\\n                item_info.append(mini_dict)\\n\\n    return item_info \\n      \\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def find_greenseal(product):\n",
    "   \n",
    "    # default input to similarity function, only need to tokenize and find stopwords once\n",
    "    X_list = word_tokenize(product) \n",
    "\n",
    "    # create search url, define user agent, make request\n",
    "    search_product = product.replace(\" \",\"%20\")\n",
    "    my_url = f\"https://www.greenseal.org/api/search/{search_product}?service_type=\"\n",
    "    user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36\"\n",
    "    headers = {\"user-agent\":user_agent}\n",
    "    resp = requests.get(my_url, headers=headers)\n",
    "\n",
    "    # if succesful, employ bs4; return None of get request not success\n",
    "    if resp.status_code == 200:\n",
    "        page_soup = soup(resp.content, \"html.parser\")\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    # convert big json response to dictionary and enter results list\n",
    "    results_dict = json.loads(page_soup.text)\n",
    "    results_list = results_dict[\"results\"]    \n",
    "    \n",
    "    # return None if results is empty\n",
    "    if len(results_list)==0:\n",
    "        return None\n",
    "    \n",
    "    # initialize empty output list\n",
    "    item_info = []\n",
    "    \n",
    "    # iterate through each result, pulling out info we care about\n",
    "    for result in results_list:\n",
    "        mini_dict = {} #empty dictionary to append to list for this product\n",
    "        product_name = result[\"name\"]\n",
    "        if len(item_info)>=1: # if  already found hit, start nlp cross checking\n",
    "            similarity_score = check_nlp(product_name,X_list,product)\n",
    "            if similarity_score >= 0.3+ 0.05*len(item_info):\n",
    "                mini_dict[\"name\"] = product_name\n",
    "                mini_dict[\"company\"] = result[\"mfg\"]\n",
    "                mini_dict[\"category\"] = result[\"category\"]\n",
    "                item_info.append(mini_dict)\n",
    "        else:\n",
    "                mini_dict[\"name\"] = product_name\n",
    "                mini_dict[\"company\"] = result[\"mfg\"]\n",
    "                mini_dict[\"category\"] = result[\"category\"]\n",
    "                item_info.append(mini_dict)\n",
    "\n",
    "    return item_info \n",
    "      \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "results_dict = json.loads(page_soup.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "item_info = []\n",
    "results_list = results_dict[\"results\"]\n",
    "for result in results_list:\n",
    "    mini_dict = {}\n",
    "    mini_dict[\"name\"] = result[\"name\"]\n",
    "    mini_dict[\"company\"] = result[\"mfg\"]\n",
    "    mini_dict[\"category\"] = result[\"category\"]\n",
    "    item_info.append(mini_dict)\n",
    "\n",
    "print(item_info)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(find_greenseal(\"Tile Cleaner\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NSF International by Company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef find_NSF(company):\\n\\n    # default input to similarity function, only need to tokenize and find stopwords once\\n    X_list = word_tokenize(company) \\n\\n    company = company.replace(\" \",\"%20\")\\n    my_url = f\"http://info.nsf.org/Certified/Common/Company.asp?CompanyName={company}&x=0&y=0\"\\n    user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36\"\\n\\n    headers = {\"user-agent\":user_agent}\\n    resp = requests.get(my_url, headers=headers)\\n\\n    if resp.status_code == 200:\\n        page_soup = soup(resp.content, \"html.parser\")\\n    else:\\n        return None\\n    \\n    if \"No Matching Manufacturers Found\" in page_soup.text:\\n        return None\\n    \\n    content_area = page_soup.findAll(\"div\",{\"id\":\"content-area\"})[0]\\n    items = content_area.findAll(\"table\")\\n    comps_and_cats = []\\n    for item in items:\\n        company_name = str(item.td.a.strong.font).split(\">\")[1].split(\"</\")[0].strip()\\n        category = str(item.td).split(\"<br/>\")[1].strip(\"</td>\").strip().replace(\"\\r\",\"\").replace(\"\\n\",\"\").replace(\"  \",\"\")\\n        \\n        if len(comps_and_cats)<=1:\\n            comps_and_cats.append([company_name,category])\\n        else:\\n            similarity_score = check_nlp(company_name,X_list,company)\\n            allowable = 0.3+0.1*(len(comps_and_cats))\\n            if similarity_score >= allowable:\\n                omps_and_cats.append([company_name,category])\\n        \\n    return comps_and_cats\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def find_NSF(company):\n",
    "\n",
    "    # default input to similarity function, only need to tokenize and find stopwords once\n",
    "    X_list = word_tokenize(company) \n",
    "\n",
    "    company = company.replace(\" \",\"%20\")\n",
    "    my_url = f\"http://info.nsf.org/Certified/Common/Company.asp?CompanyName={company}&x=0&y=0\"\n",
    "    user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36\"\n",
    "\n",
    "    headers = {\"user-agent\":user_agent}\n",
    "    resp = requests.get(my_url, headers=headers)\n",
    "\n",
    "    if resp.status_code == 200:\n",
    "        page_soup = soup(resp.content, \"html.parser\")\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    if \"No Matching Manufacturers Found\" in page_soup.text:\n",
    "        return None\n",
    "    \n",
    "    content_area = page_soup.findAll(\"div\",{\"id\":\"content-area\"})[0]\n",
    "    items = content_area.findAll(\"table\")\n",
    "    comps_and_cats = []\n",
    "    for item in items:\n",
    "        company_name = str(item.td.a.strong.font).split(\">\")[1].split(\"</\")[0].strip()\n",
    "        category = str(item.td).split(\"<br/>\")[1].strip(\"</td>\").strip().replace(\"\\r\",\"\").replace(\"\\n\",\"\").replace(\"  \",\"\")\n",
    "        \n",
    "        if len(comps_and_cats)<=1:\n",
    "            comps_and_cats.append([company_name,category])\n",
    "        else:\n",
    "            similarity_score = check_nlp(company_name,X_list,company)\n",
    "            allowable = 0.3+0.1*(len(comps_and_cats))\n",
    "            if similarity_score >= allowable:\n",
    "                omps_and_cats.append([company_name,category])\n",
    "        \n",
    "    return comps_and_cats\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "content_area = page_soup.findAll(\"div\",{\"id\":\"content-area\"})[0]\n",
    "items = content_area.findAll(\"table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "comps_and_cats = []\n",
    "for item in items:\n",
    "    company_name = str(item.td.a.strong.font).split(\">\")[1].split(\"</\")[0].strip()\n",
    "    category = str(item.td).split(\"<br/>\")[1].strip(\"</td>\").strip().replace(\"\\r\",\"\").replace(\"\\n\",\"\").replace(\"  \",\"\")\n",
    "    comps_and_cats.append([company_name,category])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fair Trade Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_fair_trade(company,check_parts=True,fb_connect=fb_connect):\n",
    "    \n",
    "    # first check firebase\n",
    "    result = fb_connect.get(f\"/brand-Fair Trade/{company}\",None)\n",
    "    if result is not None:\n",
    "        return int(result)\n",
    "\n",
    "    company = company.replace(\" \",\"+\")\n",
    "    fair_trade_url = f\"https://www.fairtradecertified.org/search/fair-trade-products?product_type=All&name={company}\"\n",
    "    user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36\"\n",
    "    company = company.replace(\"+\",\" \")\n",
    "    \n",
    "    headers = {\"user-agent\":user_agent}\n",
    "    resp = requests.get(fair_trade_url, headers=headers)\n",
    "\n",
    "    if resp.status_code==200:\n",
    "        page_soup = soup(resp.content,\"html.parser\")\n",
    "\n",
    "    companies = page_soup.findAll(\"h3\")\n",
    "    if len(companies)==0:\n",
    "        if check_parts is False:\n",
    "            z = 0\n",
    "        else:\n",
    "            z = max(check_fair_trade(x.strip(),check_parts=False) for x in company.split())\n",
    "    else:\n",
    "        z=1\n",
    "    \n",
    "    result = fb_connect.put(\"/brand-Fair Trade/\",company,z)\n",
    "    return z\n",
    "\n",
    "    # default input to similarity function, only need to tokenize and find stopwords once\n",
    "   # X_list = word_tokenize(company) \n",
    "\n",
    "   # for company in companies:\n",
    "    #    company = company.text\n",
    "     #   similarity_score = check_nlp(company,X_list,company)\n",
    "      #  if similarity_score >= 0.35:\n",
    "       #     return 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(check_fair_trade(\"Test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "fair_trade = check_fair_trade(brand)\n",
    "all_product_values[\"fair trade\"] = fair_trade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(brand,fair_trade)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECHO API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_facility(mini_dict):\n",
    "    status = mini_dict[\"CURR_COMP_STATUS\"]\n",
    "    if \"serious\" in status:\n",
    "        return 2\n",
    "    elif \"(s)\" in status:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def violation_score(return_list):\n",
    "    score=0\n",
    "    for mini_dict in return_list:\n",
    "        score+=evaluate_facility(mini_dict)\n",
    "    return score\n",
    "\n",
    "def ECHO_violations(brand):\n",
    "    brand = brand.upper()\n",
    "    api_url = f\"https://enviro.epa.gov/enviro/efservice/t_compliance_echo/NAME/CONTAINING/{brand}/JSON\"\n",
    "    response = requests.get(api_url)\n",
    "    if response.status_code == 200:\n",
    "        return_list = json.loads(response.text)\n",
    "        if len(return_list)==0:\n",
    "            return 0\n",
    "        else:\n",
    "            vio_ratio = violation_score(return_list)/len(return_list)\n",
    "            return vio_ratio*10\n",
    "    else:\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "echo = ECHO_violations(brand)\n",
    "echo+= ECHO_violations(owner_company)\n",
    "\n",
    "echo = endpoints(echo,lower=0,upper=5)\n",
    "\n",
    "echo *= (0-1)\n",
    "\n",
    "all_product_values[\"echo\"] = echo\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(echo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-GMO Project Check \n",
    "still need to rewrite so that will check brand, not product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonGMO(product,brand,first_try=True):\n",
    "    \n",
    "    recurse = False # initialize value\n",
    "    api_url = f\"https://ws2.nongmoproject.org/api/v1/get_brands_products_by_keyword?keyword={product}&page=1\"\n",
    "    response = requests.get(api_url)\n",
    "    if response.status_code==200:\n",
    "        results_dict = json.loads(response.text)\n",
    "        results_list = results_dict[\"data\"]\n",
    "    else:\n",
    "        recurse = True\n",
    "    \n",
    "    if len(results_list)==0 or recurse is True:\n",
    "        if first_try:\n",
    "            return max(nonGMO(x,\"\",first_try=False) for x in brand.split())\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    # default input to similarity function, only need to tokenize and find stopwords once\n",
    "    X_list = word_tokenize(product) \n",
    "\n",
    "    for match in results_list:\n",
    "        name = match[\"name\"]\n",
    "        similarity_score = check_nlp(name,X_list,product)\n",
    "        if similarity_score >= 0.35:\n",
    "            return 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "nGMO = nonGMO(product,brand)\n",
    "all_product_values[\"nonGMO\"] = nGMO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(nGMO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CDP A-List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CDP_A(company):\n",
    "\n",
    "    with open(\"cdp_dict.json\",\"r\") as json_file:\n",
    "        cdp_dict = json.loads(json_file.readline())\n",
    "    \n",
    "    possible = []\n",
    "    company = company.lower()\n",
    "    \n",
    "    for key in cdp_dict.keys():\n",
    "        if company in key.lower() or key.lower() in company:\n",
    "            possible.append([key,cdp_dict[key]])\n",
    "    if len(possible)==0:\n",
    "        return 0\n",
    "    elif len(possible)==1:\n",
    "        return possible[0][1]\n",
    "    else:\n",
    "        # default input to similarity function, only need to tokenize and find stopwords once\n",
    "        X_list = word_tokenize(company) \n",
    "\n",
    "\n",
    "        round2_dict = {}\n",
    "        for option in possible:\n",
    "            sim_score = check_nlp(option[0],X_list,company)\n",
    "            round2_dict[sim_score] = option\n",
    "        best_list = round2_dict[max(round2_dict.keys())]\n",
    "        return best_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdpA = CDP_A(brand)\n",
    "cdpA+= CDP_A(owner_company)/2\n",
    "cdpA = endpoints(cdpA,lower=0,upper=3)\n",
    "all_product_values[\"cdpA\"] = cdpA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(cdpA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rainforest Alliance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ra_check(brand,fb_connect=fb_connect):\n",
    "    \n",
    "    result = fb_connect.get(f\"/brand-RA/{brand}\",None)\n",
    "    if result is not None:\n",
    "        return result\n",
    "    \n",
    "    # request module and parsing\n",
    "    ra_url = f\"https://www.rainforest-alliance.org/find-certified?location=330&category=&keyword={brand}&op=submit\"\n",
    "    user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36\"\n",
    "\n",
    "    headers = {\"user-agent\":user_agent}\n",
    "    resp = requests.get(ra_url, headers=headers)\n",
    "\n",
    "    if resp.status_code!=200:\n",
    "        return None\n",
    "    else:\n",
    "        page_soup = soup(resp.text,\"html.parser\")\n",
    "        \n",
    "    companies = page_soup.findAll(\"div\",{\"class\":\"cp-teaser-info-content\"})\n",
    "    \n",
    "    if len(companies)==0:\n",
    "        z = 0\n",
    "    else:\n",
    "        z = 1\n",
    "    \n",
    "    result2 = fb_connect.put(\"/brand-RA/\",brand,z)\n",
    "    return z\n",
    "    \n",
    "    #useful_dict = {}\n",
    "    #for company in companies:\n",
    "     #   return 1\n",
    "  #  return 0\n",
    "        \n",
    "        #products = company.div.findAll(\"span\")\n",
    "        #cleaned_products = []\n",
    "        #for product in products:\n",
    "        #    if products.index(product)!=0:\n",
    "         #       cleaned_products.append(product.text.strip().strip(\",\"))\n",
    "        #useful_dict[comp_name] = cleaned_products\n",
    "        \n",
    "        \n",
    " #   return useful_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "RA = ra_check(brand)\n",
    "RA += ra_check(owner_company)/2\n",
    "all_product_values[\"rainforest alliance\"] = RA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(RA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def find_ingredients(product,brand,company):\n",
    "\n",
    "    all_results=[]\n",
    "    my_api_key = \"9ZTbuCeX9dP5keqlGJZyWNdg1c1xCu3p9pLhJ1MS\"\n",
    "    query = product.replace(\" \",\"%20\") # format more friendly\n",
    "    # make request with product name, brand owner, limit to 25 results\n",
    "    response = requests.get(f\"https://api.nal.usda.gov/fdc/v1/foods/search?api_key={my_api_key}&query={query}&brandOwner={brand}&pageSize=25\")\n",
    "    big_dict = response.json() # json format\n",
    "    foods = big_dict[\"foods\"] # list of all foods that match result\n",
    "    \n",
    "    # if less than 10 foods match search with that brand, check again replacing brand with company\n",
    "    if len(foods)<10:\n",
    "        response2 = requests.get(f\"https://api.nal.usda.gov/fdc/v1/foods/search?api_key={my_api_key}&query={query}&brandOwner={company}&pageSize=25\")\n",
    "        big_dict_2 = response.json()\n",
    "        foods2 = big_dict[\"foods\"]\n",
    "        foods.extend(foods2)\n",
    "    \n",
    "    ingredients = set([]) #initialize empty set\n",
    "    for food in foods:\n",
    "        if \"ingredients\" in food.keys(): # if food has ingredients, get the string\n",
    "            string_mini_ingred = food[\"ingredients\"]\n",
    "            lst_mini_ingred = string_mini_ingred.split(',')\n",
    "            for term in lst_mini_ingred:\n",
    "                halves = term.split(\"(\") # splitting by commas leaves elements with parantheses, so split further\n",
    "                for half in halves:\n",
    "                    if \"FLAVOR\" not in half: # get rid of \"Natural and Artifical flavor\", \"maintains flavor\", etc\n",
    "                        ingredients.add(half.strip().strip(\".\").strip(\")\")) #clean up and add to ingredients set\n",
    "\n",
    "\n",
    "    return ingredients\n",
    "            \n",
    "        #keys = food.keys()\n",
    "        #if \"ingredients\" in keys:\n",
    "            #ingred_str = food[\"ingredients\"]\n",
    "            #all_matches.append({\"name\":food[\"description\"],\"ingredients\":ingred_str})\n",
    "    #return all_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating final score\n",
    "\n",
    "final_score = 5 # initialize\n",
    "\n",
    "used = 0 # num of keys that were not none\n",
    "keys = [\"ingredient score\",\"organic\",\"fair trade\",\"echo\",\"nonGMO\",\"cdpA\",\"rainforest alliance\"]\n",
    "for key in keys:\n",
    "    temp_val = all_product_values[key]\n",
    "    if temp_val != None:\n",
    "        final_score+=temp_val\n",
    "        used+=1\n",
    "        \n",
    "final_score = final_score / (used/len(keys)) # scale up score to take into account to avoid punishing for None\n",
    "        \n",
    "final_score = endpoints(final_score,lower=0,upper=10)\n",
    "\n",
    "all_product_values[\"pre-score\"] = final_score\n",
    "\n",
    "all_product_values[\"Eco-Score\"] = round(final_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'brand': \"Kellogg's Raisin Bran\", 'product name': 'Crunch Breakfast Cereal\\xa0in a Cup', 'owner company': \"Kellogg's Raisin Bran\", 'ingredient score': -2.126666666666667, 'organic operation ID': '8150000067', 'organic info': {'similarity score': 0.2}, 'organic': 1, 'fair trade': 1, 'echo': 0, 'nonGMO': 1, 'cdpA': 0.0, 'rainforest alliance': 0.0, 'pre-score': 5.873333333333333, 'Eco-Score': 6}\n"
     ]
    }
   ],
   "source": [
    "print(all_product_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
