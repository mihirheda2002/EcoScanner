{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "\n",
    "hostname = 'www.python.org'\n",
    "context = ssl._create_unverified_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_product_values = {}\n",
    "query = \"014100085478\"\n",
    "message = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time\n",
    "now = datetime.datetime.now()\n",
    "from firebase import firebase\n",
    "import datetime\n",
    "#from selenium import webdriver\n",
    "from urllib.request import urlopen as uReq\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import requests\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "import json\n",
    "\n",
    "fb_connect = firebase.FirebaseApplication(\"https://ecoscanner-cb66d.firebaseio.com/\",None)\n",
    "\n",
    "sw = stopwords.words(\"english\")\n",
    "\n",
    "later = datetime.datetime.now()\n",
    "duration = later- now\n",
    "time_dict = {\"imports\":duration}\n",
    "\n",
    "\n",
    "# make sure \"v2_organic_operations_dict.json\" in directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def endpoints(x,lower=0,upper=10):\n",
    "    if x<lower:\n",
    "        return lower\n",
    "    elif x>upper:\n",
    "        return upper\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## NLP filtration method for products\n",
    "def check_nlp(itemvarieties,X_list,product,sw=sw):\n",
    "\n",
    "    # if cosine function doesn't work, use own alternative\n",
    "    def own_similarity_function(product,itemvarieties):\n",
    "\n",
    "\n",
    "        if product in itemvarieties:\n",
    "            return endpoints((len(product)*2/len(itemvarieties)))\n",
    "        elif itemvarieties in product:\n",
    "            return endpoints((len(itemvarieties)*2/len(product)))     \n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "\n",
    "    l1 =[];l2 =[] \n",
    "\n",
    "    # tokenization \n",
    "\n",
    "    Y_list = word_tokenize(itemvarieties) \n",
    "\n",
    "    # sw contains the list of stopwords \n",
    "\n",
    "\n",
    "    # remove stop words from string \n",
    "    X_set = {w for w in X_list if not w in sw} \n",
    "    Y_set = {w for w in Y_list if not w in sw} \n",
    "\n",
    "    # form a set containing keywords of both strings \n",
    "    rvector = X_set.union(Y_set) \n",
    "    for w in rvector: \n",
    "        if w in X_set:\n",
    "            l1.append(1) # create a vector \n",
    "        else:\n",
    "            l1.append(0) \n",
    "        if w in Y_set:\n",
    "            l2.append(1) \n",
    "        else:\n",
    "            l2.append(0) \n",
    "\n",
    "    c = 0\n",
    "\n",
    "    # cosine formula \n",
    "    for i in range(len(rvector)): \n",
    "        c+= l1[i]*l2[i] \n",
    "    \n",
    "    try:\n",
    "        cosine = c / float((sum(l1)*sum(l2))**0.5)\n",
    "    except ZeroDivisionError:\n",
    "        return own_similarity_function(product,itemvarieties)\n",
    "    \n",
    "    if cosine ==0:\n",
    "        return own_similarity_function(product,itemvarieties)\n",
    "    \n",
    "    return cosine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creates a dictionary with all known info from upcitemdb.com\n",
    "\n",
    "def lookup(query):\n",
    "    ## append search UPC to the database website\n",
    "    my_url = \"https://www.upcitemdb.com/upc/\"+query\n",
    "    \n",
    "    #open urllib client, accounting for page not-existing\n",
    "    try:\n",
    "        uClient = uReq(my_url, context=context)\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "    # grab page html, save as soup\n",
    "    page_html = uClient.read()\n",
    "    uClient.close()\n",
    "    page_soup = soup(page_html,\"html.parser\")\n",
    "    \n",
    "    if \"invalid\" in page_soup.text:\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    # find the table of item details, use first bc only 1\n",
    "    container = page_soup.findAll(\"table\",{\"class\":\"detail-list\"})[0]\n",
    "    \n",
    "    ## create blank dictionary\n",
    "    info_dict={}\n",
    "    \n",
    "    ## find product name\n",
    "    \n",
    "    # climb into blah is associated with blah\n",
    "    #pieces = str(page_soup.head.findAll(\"meta\")[1].meta).split(\",\")\n",
    "    #get rid of first thing\n",
    "    #pieces.pop(0)\n",
    "    \n",
    "    piece = page_soup.findAll(\"p\",{\"class\":\"detailtitle\"})[0]\n",
    "    \n",
    "    product_name = piece.b.text\n",
    "\n",
    "    \n",
    "    def clean(x):\n",
    "        x=x.replace(\"&amp\",\"&\")\n",
    "        for char in x:\n",
    "            if char.isnumeric():\n",
    "                tokens = x.split(char)\n",
    "                y = tokens[0]\n",
    "                break\n",
    "        if not y:    \n",
    "            return x.split(\",\")[0]\n",
    "        else:\n",
    "            return y.split(\",\")[0].strip()\n",
    "\n",
    "    product_name = clean(product_name)\n",
    "\n",
    "    \n",
    "    ## find all other info\n",
    "    trs = container.findAll(\"tr\")\n",
    "    \n",
    "    ## loop through info things\n",
    "    for tr in trs:\n",
    "        \n",
    "        # make list, containing key line and value line\n",
    "        tds = tr.findAll(\"td\")\n",
    "        \n",
    "        ## parse out key and value, save to info dict\n",
    "        key = str(tds[0]).strip(\"<td>\").strip(\"</\").strip(\":\")\n",
    "        value = str(tds[1]).strip(\"<td>\").strip(\"</\").strip().strip(\"\\t\")\n",
    "        # don't care ab last-scanned\n",
    "        if key != \"Last Scanned\":\n",
    "            if key==\"Brand\":\n",
    "                for part in value.split():\n",
    "                    for part2 in part.split(','):\n",
    "                        product_name = product_name.replace(part2,\"\")\n",
    "                \n",
    "                info_dict[\"product name\"] = product_name.strip()\n",
    "            info_dict[key]=value\n",
    "    return info_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(lookup(query))\n",
    "#capath=\"Macintosh HD⁩/Users⁩/⁨mihirheda⁩/⁨Downloads/securly_ca_2034.crt⁩\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#my_url = \"https://www.upcitemdb.com/upc/\"+query\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(lookup(\"013562300587\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#open urllib client, accounting for page not-existing\n",
    "\n",
    "uClient = uReq(\"https://www.upcitemdb.com/upc/071641053649\")\n",
    "\n",
    "\n",
    "# grab page html, save as soup\n",
    "page_html = uClient.read()\n",
    "uClient.close()\n",
    "page_soup = soup(page_html,\"html.parser\")\n",
    "\n",
    "pieces = str(page_soup.head.findAll(\"meta\")[1].meta).split(\",\")\n",
    "pieces.pop(0)\n",
    "print(\"pieces before splicing:\")\n",
    "print(pieces)\n",
    "\n",
    "output_str = \"\"\n",
    "a=True\n",
    "for piece in pieces:    \n",
    "    if piece==\" upc lookup database\":\n",
    "        a=False\n",
    "    print(\"piece\",piece,\"a\",a)\n",
    "    if a:\n",
    "        output_str+=piece\n",
    "print(output_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'UPC-A': '0 14100 08547 8', 'EAN-13': '0 014100 085478', 'Amazon ASIN': 'B004CRNR0C', 'Country of Registration': 'United States', 'product name': 'Goldfish', 'Brand': 'Pepperidge Farm', 'Model #': 'Cheddar Goldfish', 'Size': '6.6-ounce bag', 'Color': 'Stainless steel', 'Weight': '0.5 Pounds', 'Product Dimension': '4 X 2.8 X 5.8 inches'}\n"
     ]
    }
   ],
   "source": [
    "now = datetime.datetime.now()\n",
    "upc_info_dict = lookup(query)\n",
    "later = datetime.datetime.now()\n",
    "duration = later-now\n",
    "time_dict[\"lookup upc\"] = duration\n",
    "\n",
    "if upc_info_dict is None:\n",
    "    message += \"lookup failed and returned None, barcode not found\"\n",
    "brand = upc_info_dict[\"Brand\"]\n",
    "all_product_values[\"brand\"] = brand\n",
    "print(upc_info_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_product(product,brand):\n",
    "    sucky_chars = [\"-\",\":\",\";\",\"(\",\")\",\"_\",\"%\",\"#\",\"^\",\"&\",\"!\",\"oz\",\"Oz\"]\n",
    "    for char in sucky_chars:\n",
    "        product = product.replace(char,\"\").strip()\n",
    "    for brand_part in brand.split(\",\"):\n",
    "        product = product.replace(brand_part,\"\").strip()\n",
    "    return product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(clean_product(\"Annie's Low Sodium Mac Mild Cheddar Cheese 6 oz\",\"Annie's\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'UPC-A': '0 14100 08547 8', 'EAN-13': '0 014100 085478', 'Amazon ASIN': 'B004CRNR0C', 'Country of Registration': 'United States', 'product name': 'Goldfish', 'Brand': 'Pepperidge Farm', 'Model #': 'Cheddar Goldfish', 'Size': '6.6-ounce bag', 'Color': 'Stainless steel', 'Weight': '0.5 Pounds', 'Product Dimension': '4 X 2.8 X 5.8 inches'}\n"
     ]
    }
   ],
   "source": [
    "print(upc_info_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_name = clean_product(upc_info_dict[\"product name\"],upc_info_dict[\"Brand\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goldfish\n"
     ]
    }
   ],
   "source": [
    "print(product_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_product_values[\"product name\"] = product_name\n",
    "product = product_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_company(brand):\n",
    "    \n",
    "    result = fb_connect.get(f\"/brand-comp/{brand}\",None)\n",
    "    if result!=None:\n",
    "        return result\n",
    "\n",
    "    def side_box_check(brand,page_soup):\n",
    "        parents = []\n",
    "        side_box_list = page_soup.findAll(\"div\",{\"class\":\"zloOqf PZPZlf\"})\n",
    "        for item in side_box_list:\n",
    "            if \"Parent\" in str(item):\n",
    "                mini_set = item.findAll(\"span\",{\"class\":\"LrzXr kno-fv\"})\n",
    "                mini_set = str(mini_set[0]).split(\",\")\n",
    "                for f1 in mini_set:\n",
    "                    for snippet in str(f1).split(\">\"):\n",
    "                        if \"</a\" in snippet:\n",
    "                            parents.append(snippet.strip(\"</a\").replace(\"&amp;\",\"&\"))\n",
    "        if len(parents)!=0:\n",
    "            return parents\n",
    "        else:\n",
    "            return table_check(brand,page_soup)\n",
    "        \n",
    "    def table_check(brand,page_soup):\n",
    "        try:\n",
    "            table = page_soup.findAll(\"div\",{\"class\":\"webanswers-webanswers_table__webanswers-table\"})[0]\n",
    "        except:\n",
    "            return lame_box_check(brand,page_soup)\n",
    "        rows = table.findAll(\"tr\")\n",
    "        for tr in rows:\n",
    "            if \"Owner\" in str(tr):\n",
    "                owner_line = str(tr.findAll(\"td\")[1])\n",
    "                trim1 = owner_line.split(\">\")[1]\n",
    "                trim2 = trim1.strip(\"</td>\")\n",
    "                return trim2.split(\",\")\n",
    "    \n",
    "    def lame_box_check(brand,page_soup):\n",
    "        output = []\n",
    "        not_comp = [\"company\",brand,\"parent\"]\n",
    "        try:\n",
    "            info = page_soup.findAll(\"span\",{\"class\":\"e24Kjd\"})[0]\n",
    "        except IndexError:\n",
    "            return brand\n",
    "        bolded = info.findAll(\"b\")\n",
    "        for bold in bolded:\n",
    "            bold = str(bold).strip(\"<b>\").strip(\"</b>\")\n",
    "            if bold not in not_comp:\n",
    "                output.append(bold)\n",
    "        for option in output:\n",
    "            if option in brand or brand in option:\n",
    "                output.pop(output.index(option))\n",
    "        return output\n",
    "\n",
    "    \n",
    "    my_url = f\"https://www.google.com/search?q={brand}+parent+company\"\n",
    "    user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36\"\n",
    "\n",
    "    headers = {\"user-agent\":user_agent}\n",
    "    resp = requests.get(my_url, headers=headers)\n",
    "\n",
    "    if resp.status_code == 200:\n",
    "        page_soup = soup(resp.content, \"html.parser\")\n",
    "    else:\n",
    "        return brand\n",
    "    \n",
    "    list_tag = page_soup.findAll(\"a\",{\"class\":\"FLP8od\"})\n",
    "    if len(list_tag)==0:\n",
    "        list_tag = page_soup.findAll(\"div\",{\"class\":\"Z0LcW AZCkJd\"})\n",
    "        if len(list_tag)==0:\n",
    "            list_tag = page_soup.findAll(\"div\",{\"class\":\"Z0LcW\"})\n",
    "            if len(list_tag)==0:\n",
    "                output = []\n",
    "                intake = side_box_check(brand,page_soup)\n",
    "                if isinstance(intake,str):\n",
    "                    return intake\n",
    "                for item in intake:\n",
    "                    output.append(item.replace(\"&amp;\",\"&\"))\n",
    "                return output\n",
    "    full_tag = str(list_tag[0])\n",
    "    splitted = full_tag.split(\">\")\n",
    "    pre_company = splitted[1]\n",
    "    post_company = pre_company.split(\"</\")[0]\n",
    "    answer = post_company.replace(\"&amp;\",\"&\")\n",
    "    result = fb_connect.put(\"/brand-comp/\",brand,answer)\n",
    "    return answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.datetime.now()\n",
    "owner_company = find_company(brand)\n",
    "later = datetime.datetime.now()\n",
    "duration = later-now\n",
    "time_dict[\"owner company\"] = duration\n",
    "all_product_values[\"owner company\"] = owner_company\n",
    "if type(owner_company)==\"list\":\n",
    "    owner_company=owner_company[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_splits(lst1,splitters_list):\n",
    "    def split_single(lst2,splitter):\n",
    "        out = []\n",
    "        for thing in lst2:\n",
    "            out.extend([x.strip() for x in thing.split(splitter)])\n",
    "        return out\n",
    "    \n",
    "    for splitter in splitters_list:\n",
    "\n",
    "        lst1 = split_single(lst1,splitter)\n",
    "    \n",
    "    return lst1\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#splitters = [\",\",\":\",\".\",\"and/or\",\"and\",\"or\",\" of \"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingredients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def find_ingredients(product,brand,company):\n",
    "\n",
    "    all_results=[]\n",
    "    my_api_key = \"9ZTbuCeX9dP5keqlGJZyWNdg1c1xCu3p9pLhJ1MS\"\n",
    "    query = product.replace(\" \",\"%20\") # format more friendly\n",
    "    # make request with product name, brand owner, limit to 25 results\n",
    "    response = requests.get(f\"https://api.nal.usda.gov/fdc/v1/foods/search?api_key={my_api_key}&query={query}&brandOwner={brand}&pageSize=25\")\n",
    "    big_dict = response.json() # json format\n",
    "    foods = big_dict[\"foods\"] # list of all foods that match result\n",
    "    \n",
    "    # if less than 10 foods match search with that brand, check again replacing brand with company\n",
    "    if len(foods)<10:\n",
    "        response2 = requests.get(f\"https://api.nal.usda.gov/fdc/v1/foods/search?api_key={my_api_key}&query={query}&brandOwner={company}&pageSize=25\")\n",
    "        big_dict_2 = response.json()\n",
    "        foods2 = big_dict[\"foods\"]\n",
    "        foods.extend(foods2)\n",
    "        \n",
    "    \n",
    "    ingredients = set([]) #initialize empty set\n",
    "    for food in foods:\n",
    "        if \"ingredients\" in food.keys(): # if food has ingredients, get the string\n",
    "            string_mini_ingred = food[\"ingredients\"]\n",
    "            ingreds = multi_splits(string_mini_ingred.split(\"(\"),splitters)\n",
    "            for half in ingreds:\n",
    "                if \"FLAVOR\" not in half: # get rid of \"Natural and Artifical flavor\", \"maintains flavor\", etc\n",
    "                    ingredients.add(half.strip().strip(\")\")) #clean up and add to ingredients set\n",
    "    \n",
    "    # cleaning blank strings out of list\n",
    "    x =[]\n",
    "    for ingredient in ingredients:\n",
    "        ingredient = ingredient.strip()\n",
    "        if ingredient!=\"\" and ingredient!=\" \" and len(ingredient)>3:\n",
    "            a = True\n",
    "            if \" \" in ingredient:\n",
    "                if len(ingredient[0:ingredient.index(\" \")])<0 or len(ingredient[ingredient.index(\" \"):-1])<2:\n",
    "                    a = False\n",
    "            if a:\n",
    "                trash = [\"[\",\"]\",\"*\",\"/\",\"organic\",\"for\",\"with\"]\n",
    "                nutrient = ingredient\n",
    "                nutrient = nutrient.lower()\n",
    "                for stink in trash:\n",
    "                    nutrient = nutrient.replace(stink,\"\")\n",
    "                nutrient = nutrient.strip()\n",
    "                x.append(nutrient)\n",
    "    \n",
    "\n",
    "    return x\n",
    "            \n",
    "        #keys = food.keys()\n",
    "        #if \"ingredients\" in keys:\n",
    "            #ingred_str = food[\"ingredients\"]\n",
    "            #all_matches.append({\"name\":food[\"description\"],\"ingredients\":ingred_str})\n",
    "    #return all_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now = datetime.datetime.now()\n",
    "\n",
    "nutrients = find_ingredients(product,brand,owner_company)\n",
    "\n",
    "later = datetime.datetime.now()\n",
    "duration = later- now\n",
    "time_dict[\"find ingredients\"] = duration\n",
    "\n",
    "#now = datetime.datetime.now()\n",
    "\n",
    "## initialize selenium\n",
    "#chromedriver = \"C:\\\\Users\\\\14082\\\\Documents\\\\Random Projects\\\\LancerHacks\\\\chromedriver83\"\n",
    "#driver = webdriver.Chrome(chromedriver)\n",
    "\n",
    "#later = datetime.datetime.now()\n",
    "#duration = later- now\n",
    "#time_dict[\"selenium initialization\"] = duration\n",
    "\n",
    "driver = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def score_nutrient(nutrient,driver,fb_connect=fb_connect):\n",
    "\n",
    "    print(nutrient)\n",
    "    \n",
    "    # check if score already in firebase\n",
    "\n",
    "    try:\n",
    "        result = fb_connect.get(f\"/ingredient scores/{nutrient}\",None)\n",
    "    except:\n",
    "        result = None\n",
    "        print(\"nutrient was\",nutrient)\n",
    "        \n",
    "    if result is not None:\n",
    "        if \"N/A\" in result:\n",
    "            return ((0,\"none\"),driver)\n",
    "        print(result,type(result))\n",
    "        result_parts = result.split(\",\") # split the score and source\n",
    "        result_parts[0] = float(result_parts[0]) #turn the first part into a float\n",
    "        result_parts[1] = result_parts[1].strip() # get rid of space in source string\n",
    "        \n",
    "        return (tuple(result_parts),driver)\n",
    "\n",
    "    if driver is None: \n",
    "        # replace with location of chromedriver app on VM\n",
    "        # https://chromedriver.chromium.org/downloads\n",
    "        ## initialize selenium\n",
    "        chromedriver = \"C:\\\\Users\\\\14082\\\\Documents\\\\Random Projects\\\\LancerHacks\\\\chromedriver83\"\n",
    "        driver = webdriver.Chrome(chromedriver)\n",
    "\n",
    "        later = datetime.datetime.now()\n",
    "        duration = later- now\n",
    "        time_dict[\"selenium initialization\"] = duration\n",
    "\n",
    "    \n",
    "    try:\n",
    "        driver.get(f\"https://comptox.epa.gov/dashboard/dsstoxdb/results?search={nutrient}#toxicity-values\")\n",
    "        \n",
    "        # find the label containing eco radio button, which will be visible; if error finding, page doesn't exist\n",
    "        label = driver.find_element_by_css_selector(\"label[class='b-radio radio button is-medium']\")\n",
    "    except:\n",
    "        #driver.close()\n",
    "        result = fb_connect.put(f\"/ingredient scores\",nutrient,\"N/A\") # add in firebase, don't waste time checking again\n",
    "        return ((0,\"none\"),driver)\n",
    "\n",
    "    # empty string for hazard values\n",
    "    hazard_vals = []\n",
    "    \n",
    "    if not label.get_attribute(\"disabled\"): # check that eco is not disabled (arsenic example)\n",
    "        try:\n",
    "            label.click()\n",
    "        except:\n",
    "            exit = driver.find_element_by_css_selector(\"button[title='No thanks']\")\n",
    "            exit.click()\n",
    "            label.click()\n",
    "        source = \"eco\"\n",
    "    else:\n",
    "        source=\"human\"\n",
    "    \n",
    "    body = driver.find_element_by_css_selector(\"tbody\")\n",
    "    rows = body.find_elements_by_css_selector(\"tr\")\n",
    "    for row in rows:\n",
    "        cells = row.find_elements_by_css_selector(\"td\")\n",
    "        hazard_vals.append(int(cells[1].text)) # convert text from second cell per row into int, append\n",
    "    \n",
    "    hazard_vals.sort(reverse=True) #sort in descending order\n",
    "    hazard_vals = hazard_vals[0:3] #take only 3 highest scores\n",
    "    ave = sum(hazard_vals)/len(hazard_vals) #find ave of scores\n",
    "    score = ave-4 #subtract 4\n",
    "    \n",
    "    #driver.close() # close the browser\n",
    "    out_tup = (score,source) # conversion to tuple\n",
    "    \n",
    "    # save to firebase\n",
    "    result = fb_connect.put(f\"/ingredient scores\",nutrient,str(out_tup).strip(\")\").strip(\"(\").replace(\"\\'\",\"\"))\n",
    "\n",
    "    return (out_tup,driver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "total_score = 0\n",
    "sources = []\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "# for each ingredient in the list\n",
    "for nutrient in nutrients:\n",
    "    print(type(driver))\n",
    "    nutrient_score,driver = score_nutrient(nutrient,driver) # find the score \n",
    "    print(type(driver))\n",
    "    total_score+=nutrient_score[0] # add to total score\n",
    "    sources.append(nutrient_score[1]) # add source to a list of sources\n",
    "    \n",
    "later = datetime.datetime.now()\n",
    "duration = later- now\n",
    "time_dict[\"score all ingredients\"] = duration\n",
    "    \n",
    "num_used = len(sources)-sources.count(\"none\") # find the num of scores that aren't none\n",
    "ave_score = total_score/num_used \n",
    "\n",
    "# set upper and lower cap\n",
    "ave_score = endpoints(ave_score,lower=0,upper=4)\n",
    "\n",
    "ingredient_end_score = ave_score * -1\n",
    "all_product_values[\"ingredient score\"] = ingredient_end_score\n",
    "print(ingredient_end_score)\n",
    "\n",
    "if driver is not None:\n",
    "    now = datetime.datetime.now()\n",
    "    driver.close()\n",
    "    later = datetime.datetime.now()\n",
    "    duration = later- now\n",
    "    time_dict[\"close selenium\"] = duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DOWNLOAD ZIP FILE\n",
    "\n",
    "response = requests.get(f\"https://organicapi.ams.usda.gov/IntegrityPubDataServices/OidPublicDataService.svc/rest/GetAllOperationsPublicData?api_key={my_api_key}\",stream=True)\n",
    "target_path = 'organic_ops.zip'\n",
    "handle = open(target_path, \"wb\")\n",
    "for chunk in response.iter_content(chunk_size=512):\n",
    "    if chunk:  # filter out keep-alive new chunks\n",
    "        handle.write(chunk)\n",
    "handle.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as soup\n",
    "\n",
    "my_api_key = \"9ZTbuCeX9dP5keqlGJZyWNdg1c1xCu3p9pLhJ1MS\"\n",
    "json = {\"startldx\":\"1\",\"count\":\"1000\",\"countries\":[\"USA\"]}\n",
    "response = requests.post(f\"https://organicapi.ams.usda.gov/IntegrityPubDataServices/OidPublicDataService.svc/rest/Operations?api_key={my_api_key}\",json=json)\n",
    "all_text = response.text\n",
    "\n",
    "page_soup = soup(all_text,\"lxml\")\n",
    "page2 = soup(str(page_soup),\"html.parser\")\n",
    "operations_list = page2.html.body.operationsresult.operations.findAll(\"a:operation\")\n",
    "\n",
    "state_dict = {}\n",
    "def midinfo(x):\n",
    "    x = str(x)\n",
    "    return x.split(\">\")[1].split(\"<\")[0]\n",
    "for operation in operations_list:\n",
    "    operationname = midinfo(operation.findAll(\"a:operationname\")[0])\n",
    "    operationstatus = midinfo(operation.findAll(\"a:nopoperationstatus\")[0])\n",
    "    addresses = operation.findAll(\"a:addresses\")\n",
    "    address = addresses[0].findAll(\"a:address\")\n",
    "    state_untrimmed = str(address[0].findAll(\"a:stateorprovince\")[0])\n",
    "    state = midinfo(state_untrimmed)\n",
    "    print(operationname,operationstatus,state)\n",
    "    if state in state_dict.keys():\n",
    "        state_dict[state]+=1\n",
    "    else:\n",
    "        state_dict[state]=1\n",
    "print(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all_text = \"\"\"    <Operation>\n",
    "      <op_certifierName>[OTCO] Oregon Tilth Certified Organic</op_certifierName>\n",
    "      <op_nopOpID>8150000053</op_nopOpID>\n",
    "      <op_name>Wong Potatoes, Inc</op_name>\n",
    "      <op_clientID>OT-017900</op_clientID>\n",
    "      <op_contFirstName>Daniel</op_contFirstName>\n",
    "      <op_contLastName>Chin</op_contLastName>\n",
    "      <op_status>Certified</op_status>\n",
    "      <op_statusEffectiveDate>2014-11-06</op_statusEffectiveDate>\n",
    "      <op_nopAnniversaryDate>2020-04-01</op_nopAnniversaryDate>\n",
    "      <op_lastUpdatedDate>2019-05-15</op_lastUpdatedDate>\n",
    "      <opSC_HANDLING>Certified</opSC_HANDLING>\n",
    "      <opSC_HANDLING_ED>2014-11-06</opSC_HANDLING_ED>\n",
    "      <op_phone>(541) 798-5353</op_phone>\n",
    "      <op_email>chinfarms@gmail.com</op_email>\n",
    "      <opMA_line1>17600 Hwy 39</opMA_line1>\n",
    "      <opMA_city>Klamath Falls</opMA_city>\n",
    "      <opMA_state>Oregon</opMA_state>\n",
    "      <opMA_country>United States of America (the)</opMA_country>\n",
    "      <opMA_zip>97603</opMA_zip>\n",
    "    </Operation>\n",
    "    <Operation>\n",
    "      <op_certifierName>[OTCO] Oregon Tilth Certified Organic</op_certifierName>\n",
    "      <op_nopOpID>8150000052</op_nopOpID>\n",
    "      <op_name>High Mountain LLC</op_name>\n",
    "      <op_clientID>OT-004440</op_clientID>\n",
    "      <op_contFirstName>Kevin</op_contFirstName>\n",
    "      <op_contLastName>Christensen</op_contLastName>\n",
    "      <op_status>Suspended</op_status>\n",
    "      <op_statusEffectiveDate>2011-11-16</op_statusEffectiveDate>\n",
    "      <op_lastUpdatedDate>2017-12-27</op_lastUpdatedDate>\n",
    "      <opSC_CR>Suspended</opSC_CR>\n",
    "      <opMA_line1>PO Box 968</opMA_line1>\n",
    "      <opMA_city>Mattawa</opMA_city>\n",
    "      <opMA_state>Washington</opMA_state>\n",
    "      <opMA_country>United States of America (the)</opMA_country>\n",
    "      <opMA_zip>99349</opMA_zip>\n",
    "    </Operation>\n",
    "    <Operation>\n",
    "      <op_certifierName>[OTCO] Oregon Tilth Certified Organic</op_certifierName>\n",
    "      <op_nopOpID>8150000051</op_nopOpID>\n",
    "      <op_name>Marvin Lynch</op_name>\n",
    "      <op_otherNames>DBA O'lynch Dairy</op_otherNames>\n",
    "      <op_clientID>OT-007458</op_clientID>\n",
    "      <op_contFirstName>Marvin</op_contFirstName>\n",
    "      <op_contLastName>Lynch</op_contLastName>\n",
    "      <op_status>Certified</op_status>\n",
    "      <op_statusEffectiveDate>2009-05-04</op_statusEffectiveDate>\n",
    "      <op_nopAnniversaryDate>2020-01-01</op_nopAnniversaryDate>\n",
    "      <op_lastUpdatedDate>2019-05-15</op_lastUpdatedDate>\n",
    "      <opSC_CR>Certified</opSC_CR>\n",
    "      <opSC_CR_ED>2009-05-04</opSC_CR_ED>\n",
    "      <opSC_LS>Certified</opSC_LS>\n",
    "      <opSC_LS_ED>2009-05-04</opSC_LS_ED>\n",
    "      <op_phone>(563) 852-5285</op_phone>\n",
    "      <op_email>marvlynch@yahoo.com</op_email>\n",
    "      <opMA_line1>24764 Hwy 151 W</opMA_line1>\n",
    "      <opMA_city>Cascade</opMA_city>\n",
    "      <opMA_state>Iowa</opMA_state>\n",
    "      <opMA_country>United States of America (the)</opMA_country>\n",
    "      <opMA_zip>52033</opMA_zip>\n",
    "    </Operation>\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "huge_dict={}\n",
    "all_text = all_text.replace(\"\\n\",\"\")\n",
    "all_operations = all_text.split(\"</Operation>\")\n",
    "concerns = [\"op_nopOpID\",\"op_name\",\"opMA_state\"]\n",
    "for operation in all_operations:\n",
    "    if \"United States\" in operation and \"status>Certified<\" in operation:\n",
    "        internal_dict = {}\n",
    "        deets = operation.split(\"</\")\n",
    "        for deet in deets:\n",
    "            for concern in concerns:\n",
    "                if \"<\"+concern+\">\" in deet:\n",
    "                    internal_dict[concern]=deet.split(\">\")[-1]\n",
    "        huge_dict[internal_dict[\"op_name\"]] = internal_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##CREATING DICTIONARY OF ALL ORGANIC OPERATIONS\n",
    "\n",
    "all_text = \"\"\n",
    "op_file = open(\"organic_ops_file.txt\",\"r\",encoding=\"utf-8\")\n",
    "for line in op_file.readlines():\n",
    "    all_text+=line.strip(\"\\n\")\n",
    "op_file.close()\n",
    "\n",
    "huge_dict={}\n",
    "all_text = all_text.replace(\"\\n\",\"\")\n",
    "all_operations = all_text.split(\"</Operation>\")\n",
    "concerns = [\"op_nopOpID\",\"op_name\",\"opMA_state\"]\n",
    "for operation in all_operations:\n",
    "    if \"United States\" in operation and \"status>Certified<\" in operation:\n",
    "        internal_dict = {}\n",
    "        deets = operation.split(\"</\")\n",
    "        for deet in deets:\n",
    "            for concern in concerns:\n",
    "                if \"<\"+concern+\">\" in deet:\n",
    "                    internal_dict[concern]=deet.split(\">\")[-1]\n",
    "        huge_dict[internal_dict[\"op_name\"]] = internal_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WRITING INTO JSON FILE\n",
    "\n",
    "import json\n",
    "\n",
    "json_format = json.dumps(mega_dict)\n",
    "f = open(\"v2_organic_operations_dict.json\",\"w\")\n",
    "f.write(json_format)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating MegaDict\n",
    "\n",
    "def create_dict(filename=\"v2_organic_operations_dict.json\"):    \n",
    "    with open(filename,\"r\") as json_file:\n",
    "        mega_dict = json.load(json_file)\n",
    "        if type(mega_dict)==\"str\":\n",
    "            print(\"oops string error\")\n",
    "        else:\n",
    "            return mega_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## trying to fix capitalization in the dictionary\n",
    "import string\n",
    "mega_dict = {}\n",
    "for key in huge_dict.keys():\n",
    "    if key.isupper():\n",
    "        fixed = string.capwords(key)\n",
    "        huge_dict[key][\"op_name\"] = fixed\n",
    "        mega_dict[fixed] = huge_dict[key]\n",
    "    else:\n",
    "        mega_dict[key] = huge_dict[key]\n",
    "print(mega_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try:\n",
    " #   y = mega_dict\n",
    "#except NameError:\n",
    " #   mega_dict = create_dict()\n",
    "\n",
    "\n",
    "now = datetime.datetime.now()    \n",
    "opID_dict = create_dict()\n",
    "later = datetime.datetime.now()\n",
    "duration = later- now\n",
    "time_dict[\"creating opID_dict\"] = duration\n",
    "    \n",
    "def find_operation_id(brand,owner_company=owner_company,opID_dict=opID_dict):\n",
    "    \n",
    "    # first check firebase\n",
    "    result = fb_connect.get(f\"/brand-opID/{brand}\",None)\n",
    "    if result is not None:\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    brand = brand.replace(\",\",\"\")\n",
    "    found_keys = list()\n",
    "    dict_keys=opID_dict.keys()\n",
    "    for key in dict_keys:\n",
    "        if brand in key or key in brand:\n",
    "            found_keys.insert(0,[key,opID_dict[key][\"op_nopOpID\"]]) #make first term in found_keys a list with key, and opID\n",
    "    if len(found_keys)>=1:\n",
    "        \n",
    "        # default input to similarity function, only need to tokenize and find stopwords once\n",
    "        X_list = word_tokenize(brand) \n",
    "\n",
    "        for hit in found_keys:\n",
    "            sim_dict = {}\n",
    "            name = hit[0]\n",
    "            similarity_sc\n",
    "            similarity_score = check_nlp(name,X_list,brand)\n",
    "            sim_dict[similarity_score] = hit\n",
    "        max_sim = max(sim_dict.keys())\n",
    "        if max_sim>=0.3:\n",
    "            found_keys = sim_dict[max_sim]\n",
    "        else:\n",
    "            found_keys = []\n",
    "\n",
    "    else:\n",
    "        if owner_company!=\"\":\n",
    "            found_keys = find_operation_id(owner_company,owner_company=\"\")\n",
    "    if len(found_keys)==0:\n",
    "        opID = \"None\"\n",
    "    elif isinstance(found_keys,list) and len(found_keys)>0:\n",
    "        if isinstance(found_keys[0],str):\n",
    "            opID = found_keys[1]\n",
    "        else:\n",
    "            opID = found_keys[0][1]\n",
    "    else:\n",
    "        opID = found_keys\n",
    "    response = fb_connect.put(\"/brand-opID\",brand,opID)\n",
    "    return opID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opID = find_operation_id(brand,owner_company)\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "opID = find_operation_id(brand,owner_company)\n",
    "later = datetime.datetime.now()\n",
    "duration = later- now\n",
    "time_dict[\"find opID\"] = duration\n",
    "\n",
    "all_product_values[\"organic operation ID\"] = opID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(opID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Receiving Operation Products based on op_code\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as soup\n",
    "#\n",
    "op_id = \"5561001956\"\n",
    "#\n",
    "my_api_key = \"9ZTbuCeX9dP5keqlGJZyWNdg1c1xCu3p9pLhJ1MS\"\n",
    "json = {\"startIdx\":\"1\",\"count\":\"1000\",\"operationId\":op_id}\n",
    "response = requests.post(f\"https://organicapi.ams.usda.gov/IntegrityPubDataServices/OidPublicDataService.svc/rest/Items?api_key={my_api_key}\",json=json)\n",
    "# parsing into a list of items\n",
    "all_text = response.text\n",
    "page_soup = soup(all_text,\"lxml\")\n",
    "page2 = soup(str(page_soup),\"html.parser\")\n",
    "\n",
    "items = page2.findAll(\"a:item\")\n",
    "\n",
    "if len(items)==0:\n",
    "    return \"Error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Receiving Operation Products based on op_code\n",
    "\n",
    "def find_itemvarieties(item,tag=\"itemvarieties\"):\n",
    "    deets = item.split(\"</\")\n",
    "    for deet in deets:\n",
    "        if f\"<a:{tag}>\" in deet:\n",
    "            itemvarieties = deet.split(\">\")[-1]\n",
    "            return itemvarieties\n",
    "        \n",
    "def organic_info(op_id,product,brand):\n",
    "        \n",
    "    \n",
    "    do_trash=True\n",
    "    if do_trash:\n",
    "        # get rid of common words that are clutter, this list should grow with testing\n",
    "        trash_list = [\"Organic \",brand,\"Super \",\"Green \",\"Ounce\",\"ounce\",\"count\",\"Count\"]\n",
    "        for stink in trash_list:\n",
    "            product = product.replace(stink,\"\").strip()\n",
    "        if \" \" in product:\n",
    "            for word in product.split():\n",
    "                for stink in trash_list:\n",
    "                    if stink in word or word.isnumeric():\n",
    "                        product = product.replace(word,\"\").strip()\n",
    "        product = product.split(\",\")[0]\n",
    "        product = product.replace(brand,\"\").strip()\n",
    "    \n",
    "    # default input to similarity function, only need to tokenize and find stopwords once\n",
    "    X_list = word_tokenize(product) \n",
    "\n",
    "    ## API request\n",
    "    my_api_key = \"9ZTbuCeX9dP5keqlGJZyWNdg1c1xCu3p9pLhJ1MS\"\n",
    "    json = {\"startIdx\":\"1\",\"count\":\"100\",\"operationId\":op_id}\n",
    "    response = requests.post(f\"https://organicapi.ams.usda.gov/IntegrityPubDataServices/OidPublicDataService.svc/rest/Items?api_key={my_api_key}\",json=json)\n",
    "\n",
    "    # parsing into a list of items\n",
    "    all_text = response.text\n",
    "    page_soup = soup(all_text,\"lxml\")\n",
    "    page2 = soup(str(page_soup),\"html.parser\")\n",
    "    items = page2.findAll(\"a:item\")\n",
    "\n",
    "    if len(items)==0:\n",
    "        return None\n",
    "\n",
    "    # TEST INFORMATION\n",
    "    #company = \"Forager Project\"\n",
    "    #brand = \"Forager Project\"\n",
    "    #product = \"Super Green Leafy Greens Chips\"\n",
    "    #\n",
    "\n",
    "    # create list of API response terms we care about\n",
    "    concerns = [\"itemname\",\"itemvarieties\",\"madewithorganic\",\"organic\",\"organic100\",\"otheritems\"]\n",
    "    \n",
    "    possible_matches = []\n",
    "    best_choice = {\"similarity score\":0.2} # initial threshold for any match\n",
    "    for item in items:\n",
    "        cont = True\n",
    "        item = str(item)\n",
    "        if \"status>Certified<\" in item:\n",
    "            #try:\n",
    "            itemvarieties = find_itemvarieties(item)\n",
    "            if itemvarieties==\"\":\n",
    "                itemvarieties = find_itemvarieties(item,tag=\"otheritems\")\n",
    "            if itemvarieties ==\"\":\n",
    "                itemvarieties = find_itemvarieties(item,tag=\"itemname\")\n",
    "\n",
    "            similarity_score = check_nlp(itemvarieties,X_list,product)\n",
    "                \n",
    "            if similarity_score>best_choice[\"similarity score\"]:\n",
    "                best_choice = {\"similarity score\":similarity_score}\n",
    "                deets = item.split(\"</\")\n",
    "                for deet in deets:\n",
    "                    for concern in concerns:\n",
    "                        inclusion_str = \"<a:\"+concern\n",
    "                        exclusion_str = \"OASNFOIAJDFOAJFDOINOANEFADUB\"\n",
    "                        if concern==\"organic\":\n",
    "                            exclusion_str = \"organic100\"\n",
    "                        if inclusion_str in deet and exclusion_str not in deet:\n",
    "                            best_choice[concern]=deet.split(concern)[-1].strip(\">\").replace(\"&amp;\",\"&\")\n",
    "    return best_choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(organic_info(\"5520336912\",\"Lemon\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if opID==\"None\":\n",
    "    organic = 0\n",
    "else:\n",
    "    \n",
    "    organic = 1 # default for if the rest doesn't work\n",
    "    \n",
    "    ## come here and write the actual function to produce organic score\n",
    "    now = datetime.datetime.now()\n",
    "    organic_info_var = organic_info(opID,product,brand)\n",
    "    later = datetime.datetime.now()\n",
    "    duration = later- now\n",
    "    time_dict[\"find organic info\"] = duration\n",
    "    all_product_values[\"organic info\"] = organic_info_var\n",
    "    try:\n",
    "        if \"true\" == organic_info_var[\"organic100\"]:\n",
    "            organic = 3\n",
    "    except:\n",
    "        print(\"hit except 1\")\n",
    "        y=0\n",
    "    try:\n",
    "        if \"true\" == organic_info_var[\"organic\"]:\n",
    "            organic = 2\n",
    "    except:\n",
    "        print(\"hit except 2\")\n",
    "        y=0\n",
    "    try:\n",
    "        if \"true\" == organic_info_var[\"madewithorganic\"]:\n",
    "            print(\"worked where it was supposed to\")\n",
    "            organic = 1\n",
    "    except:\n",
    "        print(\"hit except 3\")\n",
    "        y=0\n",
    "\n",
    "all_product_values[\"organic\"] = organic\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####    # get rid of common words that are clutter, this list should grow with testing\n",
    "    trash_list = [\"Organic \",brand,company,\"Super \",\"Green \",\"Ounce\",\"ounce\",\"count\",\"Count\"]\n",
    "    for stink in trash_list:\n",
    "        product = product.replace(stink,\"\").strip()\n",
    "    for word in product.split():\n",
    "        for stink in trash_list:\n",
    "            if stink in word or word.isnumeric():\n",
    "                product = product.replace(word,\"\").strip()\n",
    "\n",
    "    #print(\"product before recombo is\",product)\n",
    "\n",
    "    # creating different product variations to test against\n",
    "    product_combos = [product]    \n",
    "    for word in product.split():\n",
    "        product_combos.append(product.replace(word,\"\").strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#organic_info(\"8150000355\",\"Annie's Homegrown Organic Pretzel Bunnies, 7 oz, Pack of 12\",\"Annie's Inc.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import nltk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nltk.download(\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Program to measure similarity between \n",
    "# two sentences using cosine similarity. \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "product = \"Super Green Leafy Greens Chips\"\n",
    "\n",
    "# default input to function, only need to tokenize and find stopwords once\n",
    "X_list = word_tokenize(product) \n",
    "sw = stopwords.words('english') \n",
    "\n",
    "def compare_terms(itemvarieties,X_list=X_list,sw=sw):\n",
    "    \n",
    "    l1 =[];l2 =[] \n",
    "\n",
    "    # tokenization \n",
    "\n",
    "    Y_list = word_tokenize(itemvarieties) \n",
    "\n",
    "    # sw contains the list of stopwords \n",
    "\n",
    "\n",
    "    # remove stop words from string \n",
    "    X_set = {w for w in X_list if not w in sw} \n",
    "    Y_set = {w for w in Y_list if not w in sw} \n",
    "\n",
    "    # form a set containing keywords of both strings \n",
    "    rvector = X_set.union(Y_set) \n",
    "    for w in rvector: \n",
    "        if w in X_set:\n",
    "            l1.append(1) # create a vector \n",
    "        else:\n",
    "            l1.append(0) \n",
    "        if w in Y_set:\n",
    "            l2.append(1) \n",
    "        else:\n",
    "            l2.append(0) \n",
    "\n",
    "    c = 0\n",
    "\n",
    "    # cosine formula \n",
    "    for i in range(len(rvector)): \n",
    "        c+= l1[i]*l2[i] \n",
    "    cosine = c / float((sum(l1)*sum(l2))**0.5) \n",
    "    return cosine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import requests\n",
    "\n",
    "product = \"Superco Green Cleaner & Degreaser\"\n",
    "safer_choice_url = f\"https://enviro.epa.gov/enviro/efservice/T_SAFERCHOICE/PRODUCT NAME/CONTAINING/{product}/JSON\"\n",
    "response = requests.get(safer_choice_url)\n",
    "\n",
    "dictionary = response.json()\n",
    "\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GREEN SEAL by Product "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "def find_greenseal(product):\n",
    "   \n",
    "    # default input to similarity function, only need to tokenize and find stopwords once\n",
    "    X_list = word_tokenize(product) \n",
    "\n",
    "    # create search url, define user agent, make request\n",
    "    search_product = product.replace(\" \",\"%20\")\n",
    "    my_url = f\"https://www.greenseal.org/api/search/{search_product}?service_type=\"\n",
    "    user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36\"\n",
    "    headers = {\"user-agent\":user_agent}\n",
    "    resp = requests.get(my_url, headers=headers)\n",
    "\n",
    "    # if succesful, employ bs4; return None of get request not success\n",
    "    if resp.status_code == 200:\n",
    "        page_soup = soup(resp.content, \"html.parser\")\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    # convert big json response to dictionary and enter results list\n",
    "    results_dict = json.loads(page_soup.text)\n",
    "    results_list = results_dict[\"results\"]    \n",
    "    \n",
    "    # return None if results is empty\n",
    "    if len(results_list)==0:\n",
    "        return None\n",
    "    \n",
    "    # initialize empty output list\n",
    "    item_info = []\n",
    "    \n",
    "    # iterate through each result, pulling out info we care about\n",
    "    for result in results_list:\n",
    "        mini_dict = {} #empty dictionary to append to list for this product\n",
    "        product_name = result[\"name\"]\n",
    "        if len(item_info)>=1: # if  already found hit, start nlp cross checking\n",
    "            similarity_score = check_nlp(product_name,X_list,product)\n",
    "            if similarity_score >= 0.3+ 0.05*len(item_info):\n",
    "                mini_dict[\"name\"] = product_name\n",
    "                mini_dict[\"company\"] = result[\"mfg\"]\n",
    "                mini_dict[\"category\"] = result[\"category\"]\n",
    "                item_info.append(mini_dict)\n",
    "        else:\n",
    "                mini_dict[\"name\"] = product_name\n",
    "                mini_dict[\"company\"] = result[\"mfg\"]\n",
    "                mini_dict[\"category\"] = result[\"category\"]\n",
    "                item_info.append(mini_dict)\n",
    "\n",
    "    return item_info \n",
    "      \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "results_dict = json.loads(page_soup.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "item_info = []\n",
    "results_list = results_dict[\"results\"]\n",
    "for result in results_list:\n",
    "    mini_dict = {}\n",
    "    mini_dict[\"name\"] = result[\"name\"]\n",
    "    mini_dict[\"company\"] = result[\"mfg\"]\n",
    "    mini_dict[\"category\"] = result[\"category\"]\n",
    "    item_info.append(mini_dict)\n",
    "\n",
    "print(item_info)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(find_greenseal(\"Tile Cleaner\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NSF International by Company"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "def find_NSF(company):\n",
    "\n",
    "    # default input to similarity function, only need to tokenize and find stopwords once\n",
    "    X_list = word_tokenize(company) \n",
    "\n",
    "    company = company.replace(\" \",\"%20\")\n",
    "    my_url = f\"http://info.nsf.org/Certified/Common/Company.asp?CompanyName={company}&x=0&y=0\"\n",
    "    user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36\"\n",
    "\n",
    "    headers = {\"user-agent\":user_agent}\n",
    "    resp = requests.get(my_url, headers=headers)\n",
    "\n",
    "    if resp.status_code == 200:\n",
    "        page_soup = soup(resp.content, \"html.parser\")\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    if \"No Matching Manufacturers Found\" in page_soup.text:\n",
    "        return None\n",
    "    \n",
    "    content_area = page_soup.findAll(\"div\",{\"id\":\"content-area\"})[0]\n",
    "    items = content_area.findAll(\"table\")\n",
    "    comps_and_cats = []\n",
    "    for item in items:\n",
    "        company_name = str(item.td.a.strong.font).split(\">\")[1].split(\"</\")[0].strip()\n",
    "        category = str(item.td).split(\"<br/>\")[1].strip(\"</td>\").strip().replace(\"\\r\",\"\").replace(\"\\n\",\"\").replace(\"  \",\"\")\n",
    "        \n",
    "        if len(comps_and_cats)<=1:\n",
    "            comps_and_cats.append([company_name,category])\n",
    "        else:\n",
    "            similarity_score = check_nlp(company_name,X_list,company)\n",
    "            allowable = 0.3+0.1*(len(comps_and_cats))\n",
    "            if similarity_score >= allowable:\n",
    "                omps_and_cats.append([company_name,category])\n",
    "        \n",
    "    return comps_and_cats\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "content_area = page_soup.findAll(\"div\",{\"id\":\"content-area\"})[0]\n",
    "items = content_area.findAll(\"table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "comps_and_cats = []\n",
    "for item in items:\n",
    "    company_name = str(item.td.a.strong.font).split(\">\")[1].split(\"</\")[0].strip()\n",
    "    category = str(item.td).split(\"<br/>\")[1].strip(\"</td>\").strip().replace(\"\\r\",\"\").replace(\"\\n\",\"\").replace(\"  \",\"\")\n",
    "    comps_and_cats.append([company_name,category])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fair Trade Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_fair_trade(company,check_parts=True,fb_connect=fb_connect):\n",
    "    \n",
    "    # first check firebase\n",
    "    result = fb_connect.get(f\"/brand-Fair Trade/{company}\",None)\n",
    "    if result is not None:\n",
    "        return int(result)\n",
    "\n",
    "    company = company.replace(\" \",\"+\")\n",
    "    fair_trade_url = f\"https://www.fairtradecertified.org/search/fair-trade-products?product_type=All&name={company}\"\n",
    "    user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36\"\n",
    "    company = company.replace(\"+\",\" \")\n",
    "    \n",
    "    headers = {\"user-agent\":user_agent}\n",
    "    resp = requests.get(fair_trade_url, headers=headers)\n",
    "\n",
    "    if resp.status_code==200:\n",
    "        page_soup = soup(resp.content,\"html.parser\")\n",
    "\n",
    "    companies = page_soup.findAll(\"h3\")\n",
    "    if len(companies)==0:\n",
    "        if check_parts is False:\n",
    "            z = 0\n",
    "        else:\n",
    "            z = max(check_fair_trade(x.strip(),check_parts=False) for x in company.split())\n",
    "    else:\n",
    "        z=1\n",
    "    \n",
    "    result = fb_connect.put(\"/brand-Fair Trade/\",company,z)\n",
    "    return z\n",
    "\n",
    "    # default input to similarity function, only need to tokenize and find stopwords once\n",
    "   # X_list = word_tokenize(company) \n",
    "\n",
    "   # for company in companies:\n",
    "    #    company = company.text\n",
    "     #   similarity_score = check_nlp(company,X_list,company)\n",
    "      #  if similarity_score >= 0.35:\n",
    "       #     return 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(check_fair_trade(\"Test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.datetime.now()\n",
    "fair_trade = check_fair_trade(brand)\n",
    "all_product_values[\"fair trade\"] = fair_trade\n",
    "later = datetime.datetime.now()\n",
    "duration = later- now\n",
    "time_dict[\"fair trade\"] = duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(brand,fair_trade)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECHO API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_facility(mini_dict):\n",
    "    status = mini_dict[\"CURR_COMP_STATUS\"]\n",
    "    if \"serious\" in status:\n",
    "        return 2\n",
    "    elif \"(s)\" in status:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def violation_score(return_list):\n",
    "    score=0\n",
    "    for mini_dict in return_list:\n",
    "        score+=evaluate_facility(mini_dict)\n",
    "    return score\n",
    "\n",
    "def ECHO_violations(brand):\n",
    "    brand = brand.upper()\n",
    "    api_url = f\"https://enviro.epa.gov/enviro/efservice/t_compliance_echo/NAME/CONTAINING/{brand}/JSON\"\n",
    "    response = requests.get(api_url)\n",
    "    if response.status_code == 200:\n",
    "        return_list = json.loads(response.text)\n",
    "        if len(return_list)==0:\n",
    "            return 0\n",
    "        else:\n",
    "            vio_ratio = violation_score(return_list)/len(return_list)\n",
    "            return vio_ratio*10\n",
    "    else:\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "now = datetime.datetime.now() #\n",
    "\n",
    "echo = ECHO_violations(brand)\n",
    "echo+= ECHO_violations(owner_company)\n",
    "\n",
    "later = datetime.datetime.now() #\n",
    "duration = later- now\n",
    "time_dict[\"ECHO\"] = duration #\n",
    "\n",
    "echo = endpoints(echo,lower=0,upper=5)\n",
    "\n",
    "echo *= (0-1)\n",
    "\n",
    "all_product_values[\"echo\"] = echo\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(echo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-GMO Project Check \n",
    "still need to rewrite so that will check brand, not product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonGMO(product,brand,first_try=True):\n",
    "    \n",
    "    recurse = False # initialize value\n",
    "    api_url = f\"https://ws2.nongmoproject.org/api/v1/get_brands_products_by_keyword?keyword={product}&page=1\"\n",
    "    response = requests.get(api_url)\n",
    "    if response.status_code==200:\n",
    "        results_dict = json.loads(response.text)\n",
    "        results_list = results_dict[\"data\"]\n",
    "    else:\n",
    "        recurse = True\n",
    "    \n",
    "    if len(results_list)==0 or recurse is True:\n",
    "        \n",
    "        if first_try:\n",
    "            if brand == \"\" and len(product.split())<=2:\n",
    "                for x in brand.split():\n",
    "                    if nonGMO(x,\"none\",first_try=False)==1:\n",
    "                        return 1\n",
    "            elif brand!=\"\":\n",
    "                return nonGMO(brand,\"\")\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    # default input to similarity function, only need to tokenize and find stopwords once\n",
    "    X_list = word_tokenize(product) \n",
    "\n",
    "    for match in results_list:\n",
    "        name = match[\"name\"]\n",
    "        similarity_score = check_nlp(name,X_list,product)\n",
    "        if similarity_score >= 0.35:\n",
    "            return 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.datetime.now() #\n",
    "\n",
    "nGMO = nonGMO(product,brand)\n",
    "all_product_values[\"nonGMO\"] = nGMO\n",
    "\n",
    "\n",
    "later = datetime.datetime.now() #\n",
    "duration = later - now\n",
    "time_dict[\"nonGMO\"] = duration #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(nGMO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CDP A-List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CDP_A(company):\n",
    "\n",
    "    with open(\"cdp_dict.json\",\"r\") as json_file:\n",
    "        cdp_dict = json.loads(json_file.readline())\n",
    "    \n",
    "    possible = []\n",
    "    company = company.lower()\n",
    "    \n",
    "    for key in cdp_dict.keys():\n",
    "        if company in key.lower() or key.lower() in company:\n",
    "            possible.append([key,cdp_dict[key]])\n",
    "    if len(possible)==0:\n",
    "        return 0\n",
    "    elif len(possible)==1:\n",
    "        return possible[0][1]\n",
    "    else:\n",
    "        # default input to similarity function, only need to tokenize and find stopwords once\n",
    "        X_list = word_tokenize(company) \n",
    "\n",
    "\n",
    "        round2_dict = {}\n",
    "        for option in possible:\n",
    "            sim_score = check_nlp(option[0],X_list,company)\n",
    "            round2_dict[sim_score] = option\n",
    "        best_list = round2_dict[max(round2_dict.keys())]\n",
    "        return best_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.datetime.now()\n",
    "\n",
    "\n",
    "cdpA = CDP_A(brand)\n",
    "cdpA+= CDP_A(owner_company)/2\n",
    "cdpA = endpoints(cdpA,lower=0,upper=3)\n",
    "all_product_values[\"cdpA\"] = cdpA\n",
    "\n",
    "later = datetime.datetime.now()\n",
    "duration = later- now\n",
    "time_dict[\"cdpA\"] = duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(cdpA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rainforest Alliance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ra_check(brand,fb_connect=fb_connect):\n",
    "    \n",
    "    result = fb_connect.get(f\"/brand-RA/{brand}\",None)\n",
    "    if result is not None:\n",
    "        return result\n",
    "    \n",
    "    # request module and parsing\n",
    "    ra_url = f\"https://www.rainforest-alliance.org/find-certified?location=330&category=&keyword={brand}&op=submit\"\n",
    "    user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36\"\n",
    "\n",
    "    headers = {\"user-agent\":user_agent}\n",
    "    resp = requests.get(ra_url, headers=headers)\n",
    "\n",
    "    if resp.status_code!=200:\n",
    "        return None\n",
    "    else:\n",
    "        page_soup = soup(resp.text,\"html.parser\")\n",
    "        \n",
    "    companies = page_soup.findAll(\"div\",{\"class\":\"cp-teaser-info-content\"})\n",
    "    \n",
    "    if len(companies)==0:\n",
    "        z = 0\n",
    "    else:\n",
    "        z = 1\n",
    "    \n",
    "    result2 = fb_connect.put(\"/brand-RA/\",brand,z)\n",
    "    return z\n",
    "    \n",
    "    #useful_dict = {}\n",
    "    #for company in companies:\n",
    "     #   return 1\n",
    "  #  return 0\n",
    "        \n",
    "        #products = company.div.findAll(\"span\")\n",
    "        #cleaned_products = []\n",
    "        #for product in products:\n",
    "        #    if products.index(product)!=0:\n",
    "         #       cleaned_products.append(product.text.strip().strip(\",\"))\n",
    "        #useful_dict[comp_name] = cleaned_products\n",
    "        \n",
    "        \n",
    " #   return useful_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.datetime.now()\n",
    "\n",
    "RA = ra_check(brand)\n",
    "RA += ra_check(owner_company)/2\n",
    "all_product_values[\"rainforest alliance\"] = RA\n",
    "\n",
    "\n",
    "later = datetime.datetime.now()\n",
    "duration = later- now\n",
    "time_dict[\"rainforest alliance\"] = duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(RA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def find_ingredients(product,brand,company):\n",
    "\n",
    "    all_results=[]\n",
    "    my_api_key = \"9ZTbuCeX9dP5keqlGJZyWNdg1c1xCu3p9pLhJ1MS\"\n",
    "    query = product.replace(\" \",\"%20\") # format more friendly\n",
    "    # make request with product name, brand owner, limit to 25 results\n",
    "    response = requests.get(f\"https://api.nal.usda.gov/fdc/v1/foods/search?api_key={my_api_key}&query={query}&brandOwner={brand}&pageSize=25\")\n",
    "    big_dict = response.json() # json format\n",
    "    foods = big_dict[\"foods\"] # list of all foods that match result\n",
    "    \n",
    "    # if less than 10 foods match search with that brand, check again replacing brand with company\n",
    "    if len(foods)<10:\n",
    "        response2 = requests.get(f\"https://api.nal.usda.gov/fdc/v1/foods/search?api_key={my_api_key}&query={query}&brandOwner={company}&pageSize=25\")\n",
    "        big_dict_2 = response.json()\n",
    "        foods2 = big_dict[\"foods\"]\n",
    "        foods.extend(foods2)\n",
    "    \n",
    "    ingredients = set([]) #initialize empty set\n",
    "    for food in foods:\n",
    "        if \"ingredients\" in food.keys(): # if food has ingredients, get the string\n",
    "            string_mini_ingred = food[\"ingredients\"]\n",
    "            lst_mini_ingred = string_mini_ingred.split(',')\n",
    "            for term in lst_mini_ingred:\n",
    "                halves = term.split(\"(\") # splitting by commas leaves elements with parantheses, so split further\n",
    "                for half in halves:\n",
    "                    if \"FLAVOR\" not in half: # get rid of \"Natural and Artifical flavor\", \"maintains flavor\", etc\n",
    "                        ingredients.add(half.strip().strip(\".\").strip(\")\")) #clean up and add to ingredients set\n",
    "\n",
    "\n",
    "    return ingredients\n",
    "            \n",
    "        #keys = food.keys()\n",
    "        #if \"ingredients\" in keys:\n",
    "            #ingred_str = food[\"ingredients\"]\n",
    "            #all_matches.append({\"name\":food[\"description\"],\"ingredients\":ingred_str})\n",
    "    #return all_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating final score\n",
    "\n",
    "final_score = 5 # initialize\n",
    "\n",
    "used = 0 # num of keys that were not none\n",
    "keys = [\"organic\",\"fair trade\",\"echo\",\"nonGMO\",\"cdpA\",\"rainforest alliance\"]\n",
    "#keys.append(\"ingredient score\")\n",
    "for key in keys:\n",
    "    temp_val = all_product_values[key]\n",
    "    if temp_val != None:\n",
    "        final_score+=temp_val\n",
    "        used+=1\n",
    "        \n",
    "final_score = final_score / (used/len(keys)) # scale up score to take into account to avoid punishing for None\n",
    "        \n",
    "final_score = endpoints(final_score,lower=0,upper=10)\n",
    "\n",
    "all_product_values[\"pre-score\"] = final_score\n",
    "\n",
    "all_product_values[\"Eco-Score\"] = round(final_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'brand': 'Pepperidge Farm', 'product name': 'Goldfish', 'owner company': 'Campbell Soup Company', 'organic operation ID': 'None', 'organic': 0, 'fair trade': 1, 'echo': -2.0, 'nonGMO': None, 'cdpA': 0.0, 'rainforest alliance': 0.0, 'pre-score': 4.8, 'Eco-Score': 5}\n"
     ]
    }
   ],
   "source": [
    "print(all_product_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'imports': '0:00:01.141998', 'lookup upc': '0:00:00.451647', 'owner company': '0:00:00.905563', 'creating opID_dict': '0:00:00.060752', 'find opID': '0:00:00.216261', 'fair trade': '0:00:00.213311', 'ECHO': '0:00:03.150667', 'nonGMO': '0:00:00.508218', 'cdpA': '0:00:00.004653', 'rainforest alliance': '0:00:00.425389'}\n"
     ]
    }
   ],
   "source": [
    "time_dict_2 = {}\n",
    "for key in time_dict.keys():\n",
    "    time_dict_2[key] = str(time_dict[key])\n",
    "print(time_dict_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with open(f\"v1_{query}_times.json\",\"w\") as file:\n",
    "    string = json.dumps(time_dict_2)\n",
    "    file.write(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
