{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\programdata\\anaconda3\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\programdata\\anaconda3\\lib\\site-packages (from bs4) (4.8.0)\n",
      "Requirement already satisfied: soupsieve>=1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (1.9.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (2.22.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (1.24.2)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2019.9.11)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen as uReq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as soup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creates a dictionary with all known info from upcitemdb.com\n",
    "\n",
    "def lookup(query):\n",
    "    ## append search UPC to the database website\n",
    "    my_url = \"https://www.upcitemdb.com/upc/\"+query\n",
    "    \n",
    "    #open urllib client, accounting for page not-existing\n",
    "    try:\n",
    "        uClient = uReq(my_url)\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "    # grab page html, save as soup\n",
    "    page_html = uClient.read()\n",
    "    uClient.close()\n",
    "    page_soup = soup(page_html,\"html.parser\")\n",
    "    \n",
    "    # find the table of item details, use first bc only 1\n",
    "    container = page_soup.findAll(\"table\",{\"class\":\"detail-list\"})[0]\n",
    "    \n",
    "    ## create blank dictionary\n",
    "    info_dict={}\n",
    "    \n",
    "    ## find product name\n",
    "    \n",
    "    # climb into blah is associated with blah\n",
    "    pieces = str(page_soup.head.findAll(\"meta\")[1].meta).split(\",\")\n",
    "    #get rid of first thing\n",
    "    pieces.pop(0)\n",
    "    \n",
    "    #create empty string\n",
    "    output_str = \"\"\n",
    "    # a will serve as a toggle switch to append pieces to output or not\n",
    "    a=True\n",
    "    # going through each term, toggle off a once we hit upc lookup database\n",
    "    for piece in pieces:    \n",
    "        if piece==\" upc lookup database\":\n",
    "            a=False\n",
    "        if a:\n",
    "            output_str+=piece\n",
    "    \n",
    "    def clean(x):\n",
    "        x=x.replace(\"&amp\",\"&\")\n",
    "        for char in x:\n",
    "            if char.isnumeric():\n",
    "                tokens = x.split(char)\n",
    "                return tokens[0]\n",
    "    info_dict[\"product name\"] = clean(output_str)\n",
    "\n",
    "    \n",
    "    ## find all other info\n",
    "    trs = container.findAll(\"tr\")\n",
    "    \n",
    "    ## loop through info things\n",
    "    for tr in trs:\n",
    "        \n",
    "        # make list, containing key line and value line\n",
    "        tds = tr.findAll(\"td\")\n",
    "        \n",
    "        ## parse out key and value, save to info dict\n",
    "        key = str(tds[0]).strip(\"<td>\").strip(\"</\")\n",
    "        value = str(tds[1]).strip(\"<td>\").strip(\"</\").strip().strip(\"\\t\")\n",
    "        # don't care ab last-scanned\n",
    "        if key != \"Last Scanned:\":\n",
    "            info_dict[key]=value\n",
    "    return info_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'product name': \" Annie's Homegrown Organic Pretzel Bunnies 7 oz Pack of 12\",\n",
       " 'UPC-A:': '0 13562 30058 7',\n",
       " 'EAN-13:': '0 013562 300587',\n",
       " 'Amazon ASIN:': 'B00511MLK0',\n",
       " 'Country of Registration:': 'United States',\n",
       " 'Brand:': \"Annie's, Inc.\",\n",
       " 'Model #:': 'SPK-23102',\n",
       " 'Weight:': '10 Pounds',\n",
       " 'Product Dimension:': '9 X 7 X 3 inches'}"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup(\"013562300587\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#open urllib client, accounting for page not-existing\n",
    "\n",
    "uClient = uReq(\"https://www.upcitemdb.com/upc/071641053649\")\n",
    "\n",
    "\n",
    "# grab page html, save as soup\n",
    "page_html = uClient.read()\n",
    "uClient.close()\n",
    "page_soup = soup(page_html,\"html.parser\")\n",
    "\n",
    "pieces = str(page_soup.head.findAll(\"meta\")[1].meta).split(\",\")\n",
    "pieces.pop(0)\n",
    "print(\"pieces before splicing:\")\n",
    "print(pieces)\n",
    "\n",
    "output_str = \"\"\n",
    "a=True\n",
    "for piece in pieces:    \n",
    "    if piece==\" upc lookup database\":\n",
    "        a=False\n",
    "    print(\"piece\",piece,\"a\",a)\n",
    "    if a:\n",
    "        output_str+=piece\n",
    "print(output_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'product name': \" Annie's Low Sodium Mac Mild Cheddar Cheese 6 oz\",\n",
       " 'UPC-A:': '0 13562 30011 2',\n",
       " 'EAN-13:': '0 013562 300112',\n",
       " 'Amazon ASIN:': 'B0047240A8',\n",
       " 'Country of Registration:': 'United States',\n",
       " 'Brand:': \"Annie's\",\n",
       " 'Model #:': 'SPK-1390111',\n",
       " 'Color:': 'Pink',\n",
       " 'Weight:': '0 pounds',\n",
       " 'Product Dimension:': '8 X 3 X 5.5 inches'}"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup(\"013562300112\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_company(brand):\n",
    "\n",
    "    def side_box_check(brand,page_soup):\n",
    "        parents = []\n",
    "        side_box_list = page_soup.findAll(\"div\",{\"class\":\"zloOqf PZPZlf\"})\n",
    "        for item in side_box_list:\n",
    "            if \"Parent\" in str(item):\n",
    "                mini_set = item.findAll(\"span\",{\"class\":\"LrzXr kno-fv\"})\n",
    "                mini_set = str(mini_set[0]).split(\",\")\n",
    "                for f1 in mini_set:\n",
    "                    for snippet in str(f1).split(\">\"):\n",
    "                        if \"</a\" in snippet:\n",
    "                            parents.append(snippet.strip(\"</a\").replace(\"&amp;\",\"&\"))\n",
    "        if len(parents)!=0:\n",
    "            return parents\n",
    "        else:\n",
    "            return table_check(brand,page_soup)\n",
    "        \n",
    "    def table_check(brand,page_soup):\n",
    "        try:\n",
    "            table = page_soup.findAll(\"div\",{\"class\":\"webanswers-webanswers_table__webanswers-table\"})[0]\n",
    "        except:\n",
    "            return lame_box_check(brand,page_soup)\n",
    "        rows = table.findAll(\"tr\")\n",
    "        for tr in rows:\n",
    "            if \"Owner\" in str(tr):\n",
    "                owner_line = str(tr.findAll(\"td\")[1])\n",
    "                trim1 = owner_line.split(\">\")[1]\n",
    "                trim2 = trim1.strip(\"</td>\")\n",
    "                return trim2.split(\",\")\n",
    "    \n",
    "    def lame_box_check(brand,page_soup):\n",
    "        output = []\n",
    "        not_comp = [\"company\",brand,\"parent\"]\n",
    "        try:\n",
    "            info = page_soup.findAll(\"span\",{\"class\":\"e24Kjd\"})[0]\n",
    "        except IndexError:\n",
    "            return brand\n",
    "        bolded = info.findAll(\"b\")\n",
    "        for bold in bolded:\n",
    "            bold = str(bold).strip(\"<b>\").strip(\"</b>\")\n",
    "            if bold not in not_comp:\n",
    "                output.append(bold)\n",
    "        for option in output:\n",
    "            if option in brand or brand in option:\n",
    "                output.pop(output.index(option))\n",
    "        return output\n",
    "\n",
    "    \n",
    "    my_url = f\"https://www.google.com/search?q={brand}+parent+company\"\n",
    "    user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36\"\n",
    "\n",
    "    headers = {\"user-agent\":user_agent}\n",
    "    resp = requests.get(my_url, headers=headers)\n",
    "\n",
    "    if resp.status_code == 200:\n",
    "        page_soup = soup(resp.content, \"html.parser\")\n",
    "    else:\n",
    "        return brand\n",
    "    \n",
    "    list_tag = page_soup.findAll(\"a\",{\"class\":\"FLP8od\"})\n",
    "    if len(list_tag)==0:\n",
    "        list_tag = page_soup.findAll(\"div\",{\"class\":\"Z0LcW AZCkJd\"})\n",
    "        if len(list_tag)==0:\n",
    "            list_tag = page_soup.findAll(\"div\",{\"class\":\"Z0LcW\"})\n",
    "            if len(list_tag)==0:\n",
    "                output = []\n",
    "                intake = side_box_check(brand,page_soup)\n",
    "                if isinstance(intake,str):\n",
    "                    return intake\n",
    "                for item in intake:\n",
    "                    output.append(item.replace(\"&amp;\",\"&\"))\n",
    "                return output\n",
    "    full_tag = str(list_tag[0])\n",
    "    splitted = full_tag.split(\">\")\n",
    "    pre_company = splitted[1]\n",
    "    post_company = pre_company.split(\"</\")[0]\n",
    "    return [post_company.replace(\"&amp;\",\"&\")]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['General Mills']\n"
     ]
    }
   ],
   "source": [
    "print(find_company(\"Annie's, Inc.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ingredients(product):\n",
    "    import requests\n",
    "    my_api_key = \"9ZTbuCeX9dP5keqlGJZyWNdg1c1xCu3p9pLhJ1MS\"\n",
    "    query = product.replace(\" \",\"%20\")\n",
    "    response = requests.get(f\"https://api.nal.usda.gov/fdc/v1/foods/search?api_key={my_api_key}&query={query}\")\n",
    "    big_dict = response.json()\n",
    "    foods = big_dict[\"foods\"]\n",
    "    all_matches = []\n",
    "    for food in foods:\n",
    "        keys = food.keys()\n",
    "        if \"ingredients\" in keys:\n",
    "            ingred_str = food[\"ingredients\"]\n",
    "            ingredients = ingred_str.lower().split(\",\")\n",
    "            all_matches.append({\"name\":food[\"description\"],\"ingredients\":ingredients})\n",
    "    return all_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'NAKED, JUICE SMOOTHIE, BERRY BLAST', 'ingredients': ['apple juice', ' banana puree', ' strawberry puree', ' blackberry puree', ' raspberry puree', ' natural flavors.']}, {'name': 'NAKED, PROTEIN JUICE SMOOTHIE, BERRY', 'ingredients': ['apple juice', ' banana puree', ' strawberry puree', ' soy protein isolate', ' whey protein concentrate', ' blueberry puree', ' soy lecithin', ' fruit and vegetable juice for color', ' natural flavors', ' ascorbic acid (vitamin c).']}, {'name': 'NAKED, 100% JUICE SMOOTHIE, BERRY VEGGIE', 'ingredients': ['sweet cherry puree', ' purple carrot juice from concentrate (water', ' purple carrot juice concentrate)', ' red beet juice from concentrate (water', ' red beet juice concentrate)', ' sweet potato puree', ' strawberry puree', ' plum puree', ' sweet corn puree', ' apple puree', ' chick pea puree', ' lemon juice', ' chicory root fiber', ' natural flavors', ' ascorbic acid (vitamin c)', ' d-alpha tocopheryl acetate (vitamin e)', ' niacinamide', ' d-calcium pantothenate (vitamin b5)', ' beta carotene (vitamin a)', ' pyridoxine hydrochloride (vitamin b6)', ' cyanocobalamin (vitamin b12).']}, {'name': 'NAKED, ALMONDMILK JUICE SMOOTHIE, BERRY ALMOND', 'ingredients': ['apple juice', ' almondmilk (filtered water', ' almonds)', ' blackberry puree', ' banana puree', ' blueberry puree', ' strawberry puree', ' raspberry puree', ' soy protein isolate', ' natural flavors.']}, {'name': 'NAKED CRUNCHY KALE, NAKED', 'ingredients': ['*kale', ' *red bell peppers', ' *sunflower seeds', ' *cashews', ' *lemon juice', ' *chickpea miso (*rice', ' *chickpeas', ' sea salt', ' water', ' koji spores (aspergillus oryzae))', ' himalayan salt.']}, {'name': 'NAKED CUPCAKES', 'ingredients': ['dried cane syrup', ' water', ' eggs', ' tapioca starch', ' rice flour', ' palm oil', ' canola oil', ' cocoa powder (processed with alkali)', ' brown rice flour', ' modified cellulose', ' baking powder (sodium acid pyrophosphate', ' sodium bicarbonate', ' corn starch', ' monocalcium phosphate)', ' natural flavor', ' sunflower lecithin', ' salt', ' vanilla extract', ' xanthan gum.']}, {'name': 'NAKED MEDLEY', 'ingredients': ['raisins', ' almonds', ' cashews.']}, {'name': 'NAKED SHRIMP', 'ingredients': ['shrimp', ' salt and water']}, {'name': 'NAKED SHRIMP', 'ingredients': ['shrimp', ' salt and water']}]\n"
     ]
    }
   ],
   "source": [
    "print(find_ingredients(\"Naked Berry\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DOWNLOAD ZIP FILE\n",
    "\n",
    "response = requests.get(f\"https://organicapi.ams.usda.gov/IntegrityPubDataServices/OidPublicDataService.svc/rest/GetAllOperationsPublicData?api_key={my_api_key}\",stream=True)\n",
    "target_path = 'organic_ops.zip'\n",
    "handle = open(target_path, \"wb\")\n",
    "for chunk in response.iter_content(chunk_size=512):\n",
    "    if chunk:  # filter out keep-alive new chunks\n",
    "        handle.write(chunk)\n",
    "handle.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as soup\n",
    "\n",
    "my_api_key = \"9ZTbuCeX9dP5keqlGJZyWNdg1c1xCu3p9pLhJ1MS\"\n",
    "json = {\"startldx\":\"1\",\"count\":\"1000\",\"countries\":[\"USA\"]}\n",
    "response = requests.post(f\"https://organicapi.ams.usda.gov/IntegrityPubDataServices/OidPublicDataService.svc/rest/Operations?api_key={my_api_key}\",json=json)\n",
    "all_text = response.text\n",
    "\n",
    "page_soup = soup(all_text,\"lxml\")\n",
    "page2 = soup(str(page_soup),\"html.parser\")\n",
    "operations_list = page2.html.body.operationsresult.operations.findAll(\"a:operation\")\n",
    "\n",
    "state_dict = {}\n",
    "def midinfo(x):\n",
    "    x = str(x)\n",
    "    return x.split(\">\")[1].split(\"<\")[0]\n",
    "for operation in operations_list:\n",
    "    operationname = midinfo(operation.findAll(\"a:operationname\")[0])\n",
    "    operationstatus = midinfo(operation.findAll(\"a:nopoperationstatus\")[0])\n",
    "    addresses = operation.findAll(\"a:addresses\")\n",
    "    address = addresses[0].findAll(\"a:address\")\n",
    "    state_untrimmed = str(address[0].findAll(\"a:stateorprovince\")[0])\n",
    "    state = midinfo(state_untrimmed)\n",
    "    print(operationname,operationstatus,state)\n",
    "    if state in state_dict.keys():\n",
    "        state_dict[state]+=1\n",
    "    else:\n",
    "        state_dict[state]=1\n",
    "print(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all_text = \"\"\"    <Operation>\n",
    "      <op_certifierName>[OTCO] Oregon Tilth Certified Organic</op_certifierName>\n",
    "      <op_nopOpID>8150000053</op_nopOpID>\n",
    "      <op_name>Wong Potatoes, Inc</op_name>\n",
    "      <op_clientID>OT-017900</op_clientID>\n",
    "      <op_contFirstName>Daniel</op_contFirstName>\n",
    "      <op_contLastName>Chin</op_contLastName>\n",
    "      <op_status>Certified</op_status>\n",
    "      <op_statusEffectiveDate>2014-11-06</op_statusEffectiveDate>\n",
    "      <op_nopAnniversaryDate>2020-04-01</op_nopAnniversaryDate>\n",
    "      <op_lastUpdatedDate>2019-05-15</op_lastUpdatedDate>\n",
    "      <opSC_HANDLING>Certified</opSC_HANDLING>\n",
    "      <opSC_HANDLING_ED>2014-11-06</opSC_HANDLING_ED>\n",
    "      <op_phone>(541) 798-5353</op_phone>\n",
    "      <op_email>chinfarms@gmail.com</op_email>\n",
    "      <opMA_line1>17600 Hwy 39</opMA_line1>\n",
    "      <opMA_city>Klamath Falls</opMA_city>\n",
    "      <opMA_state>Oregon</opMA_state>\n",
    "      <opMA_country>United States of America (the)</opMA_country>\n",
    "      <opMA_zip>97603</opMA_zip>\n",
    "    </Operation>\n",
    "    <Operation>\n",
    "      <op_certifierName>[OTCO] Oregon Tilth Certified Organic</op_certifierName>\n",
    "      <op_nopOpID>8150000052</op_nopOpID>\n",
    "      <op_name>High Mountain LLC</op_name>\n",
    "      <op_clientID>OT-004440</op_clientID>\n",
    "      <op_contFirstName>Kevin</op_contFirstName>\n",
    "      <op_contLastName>Christensen</op_contLastName>\n",
    "      <op_status>Suspended</op_status>\n",
    "      <op_statusEffectiveDate>2011-11-16</op_statusEffectiveDate>\n",
    "      <op_lastUpdatedDate>2017-12-27</op_lastUpdatedDate>\n",
    "      <opSC_CR>Suspended</opSC_CR>\n",
    "      <opMA_line1>PO Box 968</opMA_line1>\n",
    "      <opMA_city>Mattawa</opMA_city>\n",
    "      <opMA_state>Washington</opMA_state>\n",
    "      <opMA_country>United States of America (the)</opMA_country>\n",
    "      <opMA_zip>99349</opMA_zip>\n",
    "    </Operation>\n",
    "    <Operation>\n",
    "      <op_certifierName>[OTCO] Oregon Tilth Certified Organic</op_certifierName>\n",
    "      <op_nopOpID>8150000051</op_nopOpID>\n",
    "      <op_name>Marvin Lynch</op_name>\n",
    "      <op_otherNames>DBA O'lynch Dairy</op_otherNames>\n",
    "      <op_clientID>OT-007458</op_clientID>\n",
    "      <op_contFirstName>Marvin</op_contFirstName>\n",
    "      <op_contLastName>Lynch</op_contLastName>\n",
    "      <op_status>Certified</op_status>\n",
    "      <op_statusEffectiveDate>2009-05-04</op_statusEffectiveDate>\n",
    "      <op_nopAnniversaryDate>2020-01-01</op_nopAnniversaryDate>\n",
    "      <op_lastUpdatedDate>2019-05-15</op_lastUpdatedDate>\n",
    "      <opSC_CR>Certified</opSC_CR>\n",
    "      <opSC_CR_ED>2009-05-04</opSC_CR_ED>\n",
    "      <opSC_LS>Certified</opSC_LS>\n",
    "      <opSC_LS_ED>2009-05-04</opSC_LS_ED>\n",
    "      <op_phone>(563) 852-5285</op_phone>\n",
    "      <op_email>marvlynch@yahoo.com</op_email>\n",
    "      <opMA_line1>24764 Hwy 151 W</opMA_line1>\n",
    "      <opMA_city>Cascade</opMA_city>\n",
    "      <opMA_state>Iowa</opMA_state>\n",
    "      <opMA_country>United States of America (the)</opMA_country>\n",
    "      <opMA_zip>52033</opMA_zip>\n",
    "    </Operation>\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "huge_dict={}\n",
    "all_text = all_text.replace(\"\\n\",\"\")\n",
    "all_operations = all_text.split(\"</Operation>\")\n",
    "concerns = [\"op_nopOpID\",\"op_name\",\"opMA_state\"]\n",
    "for operation in all_operations:\n",
    "    if \"United States\" in operation and \"status>Certified<\" in operation:\n",
    "        internal_dict = {}\n",
    "        deets = operation.split(\"</\")\n",
    "        for deet in deets:\n",
    "            for concern in concerns:\n",
    "                if \"<\"+concern+\">\" in deet:\n",
    "                    internal_dict[concern]=deet.split(\">\")[-1]\n",
    "        huge_dict[internal_dict[\"op_name\"]] = internal_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##CREATING DICTIONARY OF ALL ORGANIC OPERATIONS\n",
    "\n",
    "all_text = \"\"\n",
    "op_file = open(\"organic_ops_file.txt\",\"r\",encoding=\"utf-8\")\n",
    "for line in op_file.readlines():\n",
    "    all_text+=line.strip(\"\\n\")\n",
    "op_file.close()\n",
    "\n",
    "huge_dict={}\n",
    "all_text = all_text.replace(\"\\n\",\"\")\n",
    "all_operations = all_text.split(\"</Operation>\")\n",
    "concerns = [\"op_nopOpID\",\"op_name\",\"opMA_state\"]\n",
    "for operation in all_operations:\n",
    "    if \"United States\" in operation and \"status>Certified<\" in operation:\n",
    "        internal_dict = {}\n",
    "        deets = operation.split(\"</\")\n",
    "        for deet in deets:\n",
    "            for concern in concerns:\n",
    "                if \"<\"+concern+\">\" in deet:\n",
    "                    internal_dict[concern]=deet.split(\">\")[-1]\n",
    "        huge_dict[internal_dict[\"op_name\"]] = internal_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WRITING INTO JSON FILE\n",
    "\n",
    "import json\n",
    "\n",
    "json_format = json.dumps(mega_dict)\n",
    "f = open(\"v2_organic_operations_dict.json\",\"w\")\n",
    "f.write(json_format)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating MegaDict\n",
    "import json\n",
    "\n",
    "def create_dict(filename=\"v2_organic_operations_dict.json\"):    \n",
    "    with open(filename,\"r\") as json_file:\n",
    "        mega_dict = json.load(json_file)\n",
    "        if type(mega_dict)==\"str\":\n",
    "            print(\"oops string error\")\n",
    "        else:\n",
    "\n",
    "            return mega_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## trying to fix capitalization in the dictionary\n",
    "import string\n",
    "mega_dict = {}\n",
    "for key in huge_dict.keys():\n",
    "    if key.isupper():\n",
    "        fixed = string.capwords(key)\n",
    "        huge_dict[key][\"op_name\"] = fixed\n",
    "        mega_dict[fixed] = huge_dict[key]\n",
    "    else:\n",
    "        mega_dict[key] = huge_dict[key]\n",
    "print(mega_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not mega_dict:\n",
    "    mega_dict = create_dict()\n",
    "\n",
    "def find_operation_id(brand,mega_dict=mega_dict,check_terms=True,nlp = True):\n",
    "    brand = brand.replace(\",\",\"\")\n",
    "    found_keys = []\n",
    "    dict_keys=mega_dict.keys()\n",
    "    for key in dict_keys:\n",
    "        if brand in key:\n",
    "            found_keys.insert(0,[key,mega_dict[key][\"op_nopOpID\"]])\n",
    "    if len(found_keys)==1:\n",
    "        return found_keys\n",
    "    elif len(found_keys)!=0 and nlp:\n",
    "        from nltk.corpus import stopwords \n",
    "        from nltk.tokenize import word_tokenize\n",
    "        \n",
    "        # default input to similarity function, only need to tokenize and find stopwords once\n",
    "        X_list = word_tokenize(brand) \n",
    "        sw = stopwords.words(\"english\") \n",
    "\n",
    "\n",
    "        ## NLP filtration method for products\n",
    "        def check_brand(hit,X_list=X_list,sw=sw):\n",
    "\n",
    "            l1 =[];l2 =[] \n",
    "\n",
    "            # tokenization \n",
    "            Y_list = word_tokenize(hit) \n",
    "\n",
    "            # sw contains the list of stopwords \n",
    "\n",
    "\n",
    "            # remove stop words from string \n",
    "            X_set = {w for w in X_list if not w in sw} \n",
    "            Y_set = {w for w in Y_list if not w in sw} \n",
    "\n",
    "            # form a set containing keywords of both strings \n",
    "            rvector = X_set.union(Y_set) \n",
    "            for w in rvector: \n",
    "                if w in X_set:\n",
    "                    l1.append(1) # create a vector \n",
    "                else:\n",
    "                    l1.append(0) \n",
    "                if w in Y_set:\n",
    "                    l2.append(1) \n",
    "                else:\n",
    "                    l2.append(0) \n",
    "\n",
    "            c = 0\n",
    "\n",
    "            # cosine formula \n",
    "            for i in range(len(rvector)): \n",
    "                c+= l1[i]*l2[i] \n",
    "            cosine = c / float((sum(l1)*sum(l2))**0.5)\n",
    "            return cosine\n",
    "        \n",
    "        for hit in found_keys:\n",
    "            sim_dict = {}\n",
    "            name = hit[0]\n",
    "            similarity_score = check_brand(name)\n",
    "            sim_dict[similarity_score] = hit\n",
    "        max_sim = max(sim_dict.keys())\n",
    "        return sim_dict[max_sim]\n",
    "\n",
    "    else:\n",
    "        if check_terms:\n",
    "            for term in brand.split():\n",
    "                found_keys.extend(find_operation_id(term,mega_dict,check_terms=False,nlp=nlp))\n",
    "        found_keys.extend(find_operation_id(find_company(brand),mega_dict,nlp=nlp))\n",
    "        return found_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\"Annie's Inc.\", '8150000355']]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_operation_id(\"Annie's\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Receiving Operation Products based on op_code\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as soup\n",
    "#\n",
    "op_id = \"5561001956\"\n",
    "#\n",
    "my_api_key = \"9ZTbuCeX9dP5keqlGJZyWNdg1c1xCu3p9pLhJ1MS\"\n",
    "json = {\"startIdx\":\"1\",\"count\":\"1000\",\"operationId\":op_id}\n",
    "response = requests.post(f\"https://organicapi.ams.usda.gov/IntegrityPubDataServices/OidPublicDataService.svc/rest/Items?api_key={my_api_key}\",json=json)\n",
    "# parsing into a list of items\n",
    "all_text = response.text\n",
    "page_soup = soup(all_text,\"lxml\")\n",
    "page2 = soup(str(page_soup),\"html.parser\")\n",
    "\n",
    "items = page2.findAll(\"a:item\")\n",
    "\n",
    "if len(items)==0:\n",
    "    return \"Error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Receiving Operation Products based on op_code\n",
    "\n",
    "def find_itemvarieties(item):\n",
    "    deets = item.split(\"</\")\n",
    "    for deet in deets:\n",
    "        if \"<a:itemvarieties>\" in deet:\n",
    "            itemvarieties = deet.split(\">\")[-1]\n",
    "            return itemvarieties\n",
    "\n",
    "def clean_product(product):\n",
    "    \n",
    "    def has_num(string):\n",
    "        return any(i.isnumeric() for i in string)\n",
    "    \n",
    "    out_list = []\n",
    "    tokens = product.split(\",\")\n",
    "    for token in tokens:\n",
    "        if not has_num(token):\n",
    "            out_list.append(token)\n",
    "    return \"\".join(out_list)\n",
    "    \n",
    "        \n",
    "def organic_info(op_id,product,brand,nlp=True):\n",
    "    \n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup as soup\n",
    "    if nlp:\n",
    "        from nltk.corpus import stopwords \n",
    "        from nltk.tokenize import word_tokenize \n",
    "        \n",
    "    product = clean_product(product)\n",
    "    \n",
    "    do_trash=False\n",
    "    if do_trash:\n",
    "        # get rid of common words that are clutter, this list should grow with testing\n",
    "        trash_list = [\"Organic \",brand,\"Super \",\"Green \",\"Ounce\",\"ounce\",\"count\",\"Count\"]\n",
    "        for stink in trash_list:\n",
    "            product = product.replace(stink,\"\").strip()\n",
    "        for word in product.split():\n",
    "            for stink in trash_list:\n",
    "                if stink in word or word.isnumeric():\n",
    "                    product = product.replace(word,\"\").strip()\n",
    "        product = product.split(\",\")[0]\n",
    "        product = product.replace(brand,\"\").strip()\n",
    "    print(\"product is\",product)\n",
    "    \n",
    "    if nlp:\n",
    "\n",
    "        # default input to similarity function, only need to tokenize and find stopwords once\n",
    "        X_list = word_tokenize(product) \n",
    "        sw = stopwords.words(\"english\") \n",
    "\n",
    "\n",
    "        ## NLP filtration method for products\n",
    "        def check_item(itemvarieties,X_list=X_list,sw=sw):\n",
    "\n",
    "            l1 =[];l2 =[] \n",
    "\n",
    "            # tokenization \n",
    "\n",
    "            Y_list = word_tokenize(itemvarieties) \n",
    "\n",
    "            # sw contains the list of stopwords \n",
    "\n",
    "\n",
    "            # remove stop words from string \n",
    "            X_set = {w for w in X_list if not w in sw} \n",
    "            Y_set = {w for w in Y_list if not w in sw} \n",
    "\n",
    "            # form a set containing keywords of both strings \n",
    "            rvector = X_set.union(Y_set) \n",
    "            for w in rvector: \n",
    "                if w in X_set:\n",
    "                    l1.append(1) # create a vector \n",
    "                else:\n",
    "                    l1.append(0) \n",
    "                if w in Y_set:\n",
    "                    l2.append(1) \n",
    "                else:\n",
    "                    l2.append(0) \n",
    "\n",
    "            c = 0\n",
    "\n",
    "            # cosine formula \n",
    "            for i in range(len(rvector)): \n",
    "                c+= l1[i]*l2[i] \n",
    "            cosine = c / float((sum(l1)*sum(l2))**0.5)\n",
    "            return cosine\n",
    "\n",
    "    ## API request\n",
    "    my_api_key = \"9ZTbuCeX9dP5keqlGJZyWNdg1c1xCu3p9pLhJ1MS\"\n",
    "    json = {\"startIdx\":\"1\",\"count\":\"1000\",\"operationId\":op_id}\n",
    "    response = requests.post(f\"https://organicapi.ams.usda.gov/IntegrityPubDataServices/OidPublicDataService.svc/rest/Items?api_key={my_api_key}\",json=json)\n",
    "\n",
    "    # parsing into a list of items\n",
    "    all_text = response.text\n",
    "    page_soup = soup(all_text,\"lxml\")\n",
    "    page2 = soup(str(page_soup),\"html.parser\")\n",
    "    items = page2.findAll(\"a:item\")\n",
    "\n",
    "    if len(items)==0:\n",
    "        return None\n",
    "\n",
    "    # TEST INFORMATION\n",
    "    #company = \"Forager Project\"\n",
    "    #brand = \"Forager Project\"\n",
    "    #product = \"Super Green Leafy Greens Chips\"\n",
    "    #\n",
    "\n",
    "    # create list of API response terms we care about\n",
    "    concerns = [\"itemname\",\"itemvarieties\",\"madewithorganic\",\"organic\",\"organic100\"]\n",
    "    possible_matches = []\n",
    "    for item in items:\n",
    "        cont = True\n",
    "        item = str(item)\n",
    "        if \"status>Certified<\" in item:\n",
    "            try:\n",
    "                itemvarieties = find_itemvarieties(item)\n",
    "                if nlp:\n",
    "                    similarity_score = check_item(itemvarieties)\n",
    "            except:\n",
    "                cont=False\n",
    "            if cont:\n",
    "                cont2=False\n",
    "                if nlp and similarity_score>=0.2+0.05*len(possible_matches):\n",
    "                    internal_dict = {\"similarity_score\":similarity_score}\n",
    "                    cont2 = True\n",
    "                elif not nlp:\n",
    "                    internal_dict = {\"similarity_score\":\"N/A\"}\n",
    "                    cont2=True\n",
    "                if cont2:\n",
    "                    deets = item.split(\"</\")\n",
    "                    for deet in deets:\n",
    "                        for concern in concerns:\n",
    "                            if \"<a:\"+concern+\">\" in deet:\n",
    "                                internal_dict[concern]=deet.split(\">\")[-1].replace(\"&amp;\",\"&\")\n",
    "                    possible_matches.append(internal_dict)\n",
    "    return possible_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # get rid of common words that are clutter, this list should grow with testing\n",
    "    trash_list = [\"Organic \",brand,company,\"Super \",\"Green \",\"Ounce\",\"ounce\",\"count\",\"Count\"]\n",
    "    for stink in trash_list:\n",
    "        product = product.replace(stink,\"\").strip()\n",
    "    for word in product.split():\n",
    "        for stink in trash_list:\n",
    "            if stink in word or word.isnumeric():\n",
    "                product = product.replace(word,\"\").strip()\n",
    "\n",
    "    #print(\"product before recombo is\",product)\n",
    "\n",
    "    # creating different product variations to test against\n",
    "    product_combos = [product]    \n",
    "    for word in product.split():\n",
    "        product_combos.append(product.replace(word,\"\").strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "product is Annie's Homegrown Organic Pretzel Bunnies\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'similarity_score': 0.20412414523193154,\n",
       "  'itemname': 'Other',\n",
       "  'itemvarieties': 'Honey Wheat Pretzels Bunnies',\n",
       "  'organic': 'true'},\n",
       " {'similarity_score': 0.2886751345948129,\n",
       "  'itemname': 'Other',\n",
       "  'itemvarieties': 'Pretzels Bunnies',\n",
       "  'organic': 'true'}]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "organic_info(\"8150000355\",\"Annie's Homegrown Organic Pretzel Bunnies, 7 oz, Pack of 12\",\"Annie's Inc.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import nltk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nltk.download(\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Program to measure similarity between \n",
    "# two sentences using cosine similarity. \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "product = \"Super Green Leafy Greens Chips\"\n",
    "\n",
    "# default input to function, only need to tokenize and find stopwords once\n",
    "X_list = word_tokenize(product) \n",
    "sw = stopwords.words('english') \n",
    "\n",
    "def compare_terms(itemvarieties,X_list=X_list,sw=sw):\n",
    "    \n",
    "    l1 =[];l2 =[] \n",
    "\n",
    "    # tokenization \n",
    "\n",
    "    Y_list = word_tokenize(itemvarieties) \n",
    "\n",
    "    # sw contains the list of stopwords \n",
    "\n",
    "\n",
    "    # remove stop words from string \n",
    "    X_set = {w for w in X_list if not w in sw} \n",
    "    Y_set = {w for w in Y_list if not w in sw} \n",
    "\n",
    "    # form a set containing keywords of both strings \n",
    "    rvector = X_set.union(Y_set) \n",
    "    for w in rvector: \n",
    "        if w in X_set:\n",
    "            l1.append(1) # create a vector \n",
    "        else:\n",
    "            l1.append(0) \n",
    "        if w in Y_set:\n",
    "            l2.append(1) \n",
    "        else:\n",
    "            l2.append(0) \n",
    "\n",
    "    c = 0\n",
    "\n",
    "    # cosine formula \n",
    "    for i in range(len(rvector)): \n",
    "        c+= l1[i]*l2[i] \n",
    "    cosine = c / float((sum(l1)*sum(l2))**0.5) \n",
    "    return cosine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please Enter the Barcode of the product you wish to scan:\n",
      "013562300587\n"
     ]
    }
   ],
   "source": [
    "## testing flow so far\n",
    "\n",
    "code = input(\"Please Enter the Barcode of the product you wish to scan:\\n\")\n",
    "code_info = lookup(code)\n",
    "product= code_info[\"product name\"]\n",
    "brand = code_info[\"Brand:\"]\n",
    "op_id = find_operation_id(brand)[0][1]\n",
    "org_inf = organic_info(op_id,product,brand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'similarity_score': 0.31622776601683794, 'itemname': 'Other', 'itemvarieties': 'Pretzels Bunnies', 'organic': 'true'}, {'similarity_score': 0.31622776601683794, 'itemname': 'Other', 'itemvarieties': 'Cheddar Bunnies', 'organic': 'true'}, {'similarity_score': 0.31622776601683794, 'itemname': 'Other', 'itemvarieties': 'Cheddar Bunnies', 'madewithorganic': 'true'}, {'similarity_score': 0.31622776601683794, 'itemname': 'Other', 'itemvarieties': 'Muddy Bunnies', 'organic': 'true'}, {'similarity_score': 0.31622776601683794, 'itemname': 'Other', 'itemvarieties': 'Cocoa Bunnies', 'organic': 'true'}, {'similarity_score': 0.31622776601683794, 'itemname': 'Other', 'itemvarieties': 'Berry Bunnies', 'organic': 'true'}]\n"
     ]
    }
   ],
   "source": [
    "print(org_inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import requests\n",
    "\n",
    "product = \"Superco Green Cleaner & Degreaser\"\n",
    "safer_choice_url = f\"https://enviro.epa.gov/enviro/efservice/T_SAFERCHOICE/PRODUCT NAME/CONTAINING/{product}/JSON\"\n",
    "response = requests.get(safer_choice_url)\n",
    "\n",
    "dictionary = response.json()\n",
    "\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GREEN SEAL by Product "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_greenseal(product,nlp=False):\n",
    "\n",
    "    import requests\n",
    "    import json\n",
    "    from bs4 import BeautifulSoup as soup\n",
    "    if nlp:\n",
    "        from nltk.corpus import stopwords \n",
    "        from nltk.tokenize import word_tokenize\n",
    "\n",
    "        # default input to similarity function, only need to tokenize and find stopwords once\n",
    "        X_list = word_tokenize(product) \n",
    "        sw = stopwords.words(\"english\") \n",
    "\n",
    "        ## NLP filtration method for products\n",
    "        def check_product(product_test,X_list=X_list,sw=sw):\n",
    "\n",
    "            l1 =[];l2 =[] \n",
    "\n",
    "            # tokenization \n",
    "\n",
    "            Y_list = word_tokenize(product_test) \n",
    "\n",
    "            # sw contains the list of stopwords \n",
    "\n",
    "\n",
    "            # remove stop words from string \n",
    "            X_set = {w for w in X_list if not w in sw} \n",
    "            Y_set = {w for w in Y_list if not w in sw} \n",
    "\n",
    "            # form a set containing keywords of both strings \n",
    "            rvector = X_set.union(Y_set) \n",
    "            for w in rvector: \n",
    "                if w in X_set:\n",
    "                    l1.append(1) # create a vector \n",
    "                else:\n",
    "                    l1.append(0) \n",
    "                if w in Y_set:\n",
    "                    l2.append(1) \n",
    "                else:\n",
    "                    l2.append(0) \n",
    "\n",
    "            c = 0\n",
    "\n",
    "            # cosine formula \n",
    "            for i in range(len(rvector)): \n",
    "                c+= l1[i]*l2[i] \n",
    "            cosine = c / float((sum(l1)*sum(l2))**0.5)\n",
    "            return cosine\n",
    "    # create search url, define user agent, make request\n",
    "    search_product = product.replace(\" \",\"%20\")\n",
    "    my_url = f\"https://www.greenseal.org/api/search/{search_product}?service_type=\"\n",
    "    user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36\"\n",
    "    headers = {\"user-agent\":user_agent}\n",
    "    resp = requests.get(my_url, headers=headers)\n",
    "\n",
    "    # if succesful, employ bs4; return None of get request not success\n",
    "    if resp.status_code == 200:\n",
    "        page_soup = soup(resp.content, \"html.parser\")\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    # convert big json response to dictionary and enter results list\n",
    "    results_dict = json.loads(page_soup.text)\n",
    "    results_list = results_dict[\"results\"]    \n",
    "    \n",
    "    # return None if results is empty\n",
    "    if len(results_list)==0:\n",
    "        return None\n",
    "    \n",
    "    # initialize empty output list\n",
    "    item_info = []\n",
    "    \n",
    "    # iterate through each result, pulling out info we care about\n",
    "    for result in results_list:\n",
    "        mini_dict = {} #empty dictionary to append to list for this product\n",
    "        product_name = result[\"name\"]\n",
    "        if nlp and len(item_info)>=1: # if allowing nlp and already found hit, start nlp cross checking\n",
    "            similarity_score = check_product(product_name)\n",
    "            if similarity_score >= 0.3+ 0.05*len(item_info):\n",
    "                mini_dict[\"name\"] = product_name\n",
    "                mini_dict[\"company\"] = result[\"mfg\"]\n",
    "                mini_dict[\"category\"] = result[\"category\"]\n",
    "                item_info.append(mini_dict)\n",
    "        else:\n",
    "                mini_dict[\"name\"] = product_name\n",
    "                mini_dict[\"company\"] = result[\"mfg\"]\n",
    "                mini_dict[\"category\"] = result[\"category\"]\n",
    "                item_info.append(mini_dict)\n",
    "\n",
    "    return item_info \n",
    "      \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "results_dict = json.loads(page_soup.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "item_info = []\n",
    "results_list = results_dict[\"results\"]\n",
    "for result in results_list:\n",
    "    mini_dict = {}\n",
    "    mini_dict[\"name\"] = result[\"name\"]\n",
    "    mini_dict[\"company\"] = result[\"mfg\"]\n",
    "    mini_dict[\"category\"] = result[\"category\"]\n",
    "    item_info.append(mini_dict)\n",
    "\n",
    "print(item_info)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'Surety™ Tub & Tile Cleaner', 'company': 'U S Chemical', 'category': 'Industrial & Institutional Cleaning Products'}, {'name': 'Tub & Tile Cleaner', 'company': 'Brandless', 'category': 'Household Cleaning Products'}, {'name': 'Surety™ microTECH™ Tub & Tile Cleaner', 'company': 'U S Chemical', 'category': 'Industrial & Institutional Cleaning Products'}, {'name': 'Surety™ MixMATE™ Tub & Tile Cleaner', 'company': 'U S Chemical', 'category': 'Industrial & Institutional Cleaning Products'}, {'name': 'Basin Tub & Tile Cleaner', 'company': 'Pak-It', 'category': 'Industrial & Institutional Cleaning Products'}, {'name': 'EcoPal Tub & Tile Cleaner', 'company': 'Al-Bahar Industries', 'category': 'Industrial & Institutional Cleaning Products'}, {'name': 'Focus TC 66 Tub & Tile Cleaner', 'company': 'American Cleaning Solutions', 'category': 'Industrial & Institutional Cleaning Products'}, {'name': 'Building Pro TTC Tub & Tile Cleaner', 'company': 'JAD Corporation of America', 'category': 'Industrial & Institutional Cleaning Products'}, {'name': 'Green Life/La Vida Verde Tub & Tile Cleaner', 'company': 'Snappy Solutions, Inc.', 'category': 'Industrial & Institutional Cleaning Products'}]\n"
     ]
    }
   ],
   "source": [
    "print(find_greenseal(\"Tile Cleaner\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NSF International by Company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_NSF(company,nlp=False):\n",
    "\n",
    "    if nlp:\n",
    "        from nltk.corpus import stopwords \n",
    "        from nltk.tokenize import word_tokenize\n",
    "        \n",
    "        # default input to similarity function, only need to tokenize and find stopwords once\n",
    "        X_list = word_tokenize(company) \n",
    "        sw = stopwords.words(\"english\") \n",
    "\n",
    "        ## NLP filtration method for products\n",
    "        def check_comp(company_name,X_list=X_list,sw=sw):\n",
    "\n",
    "            l1 =[];l2 =[] \n",
    "\n",
    "            # tokenization \n",
    "\n",
    "            Y_list = word_tokenize(company_name) \n",
    "\n",
    "            # sw contains the list of stopwords \n",
    "\n",
    "\n",
    "            # remove stop words from string \n",
    "            X_set = {w for w in X_list if not w in sw} \n",
    "            Y_set = {w for w in Y_list if not w in sw} \n",
    "\n",
    "            # form a set containing keywords of both strings \n",
    "            rvector = X_set.union(Y_set) \n",
    "            for w in rvector: \n",
    "                if w in X_set:\n",
    "                    l1.append(1) # create a vector \n",
    "                else:\n",
    "                    l1.append(0) \n",
    "                if w in Y_set:\n",
    "                    l2.append(1) \n",
    "                else:\n",
    "                    l2.append(0) \n",
    "\n",
    "            c = 0\n",
    "\n",
    "            # cosine formula \n",
    "            for i in range(len(rvector)): \n",
    "                c+= l1[i]*l2[i] \n",
    "            cosine = c / float((sum(l1)*sum(l2))**0.5)\n",
    "            return cosine\n",
    "\n",
    "\n",
    "    company = company.replace(\" \",\"%20\")\n",
    "    my_url = f\"http://info.nsf.org/Certified/Common/Company.asp?CompanyName={company}&x=0&y=0\"\n",
    "    user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36\"\n",
    "\n",
    "    headers = {\"user-agent\":user_agent}\n",
    "    resp = requests.get(my_url, headers=headers)\n",
    "\n",
    "    if resp.status_code == 200:\n",
    "        page_soup = soup(resp.content, \"html.parser\")\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    if \"No Matching Manufacturers Found\" in page_soup.text:\n",
    "        return None\n",
    "    \n",
    "    content_area = page_soup.findAll(\"div\",{\"id\":\"content-area\"})[0]\n",
    "    items = content_area.findAll(\"table\")\n",
    "    comps_and_cats = []\n",
    "    for item in items:\n",
    "        company_name = str(item.td.a.strong.font).split(\">\")[1].split(\"</\")[0].strip()\n",
    "        category = str(item.td).split(\"<br/>\")[1].strip(\"</td>\").strip().replace(\"\\r\",\"\").replace(\"\\n\",\"\").replace(\"  \",\"\")\n",
    "        if nlp:\n",
    "            if len(comps_and_cats)<=1:\n",
    "                comps_and_cats.append([company_name,category])\n",
    "            else:\n",
    "                similarity_score = check_comp(company_name)\n",
    "                allowable = 0.3+0.1*(len(comps_and_cats))\n",
    "                if similarity_score >= allowable:\n",
    "                    omps_and_cats.append([company_name,category])\n",
    "        elif not nlp:\n",
    "            comps_and_cats.append([company_name,category])\n",
    "\n",
    "        \n",
    "    return comps_and_cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Champion Chemical Co.', 'NSF Registered Proprietary Substances and Nonfood Compounds'], ['Champion Industries, Inc.', 'NSF/ANSI 2 - Food Equipment']]\n"
     ]
    }
   ],
   "source": [
    "print(find_NSF(\"Champion\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Champion Chemical Co.', 'NSF Registered Proprietary Substances and Nonfood Compounds'], ['Champion Industries, Inc.', 'NSF/ANSI 2 - Food Equipment'], ['Champion Industries, Inc.', 'NSF/ANSI 3 - Commercial Warewashing Equipment'], ['Champion Industries, Inc.', 'NSF/ANSI 13 - Refuse Processors and Processing Systems'], ['Champion Packaging &amp; Distribution, Inc.', 'NSF Registered Proprietary Substances and Nonfood Compounds'], ['Champion Packaging and Distribution Inc.', 'NSF/ANSI/CAN 60 - Drinking Water Treatment Chemicals - Health Effects'], ['Champion-Arrowhead', 'NSF/ANSI/CAN 61 - Drinking Water System Components - Health Effects'], ['Champion-Arrowhead', 'NSF/ANSI/CAN 61 - Drinking Water System Components - Health Effects'], ['Champion-Arrowhead', 'NSF Mechanical Plumbing Products Program'], ['Champion-Arrowhead', 'NSF/ANSI 372 - Drinking Water System Components - Lead Content'], ['ChampionX LLC', 'NSF/ANSI/CAN 60 - Drinking Water Treatment Chemicals - Health Effects'], ['Crash Champions, LLC', 'NSF P458 - Automotive Collision Repair Shop Certification Protocol']]\n"
     ]
    }
   ],
   "source": [
    "print(find_NSF(\"Champion\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "content_area = page_soup.findAll(\"div\",{\"id\":\"content-area\"})[0]\n",
    "items = content_area.findAll(\"table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "comps_and_cats = []\n",
    "for item in items:\n",
    "    company_name = str(item.td.a.strong.font).split(\">\")[1].split(\"</\")[0].strip()\n",
    "    category = str(item.td).split(\"<br/>\")[1].strip(\"</td>\").strip().replace(\"\\r\",\"\").replace(\"\\n\",\"\").replace(\"  \",\"\")\n",
    "    comps_and_cats.append([company_name,category])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fair Trade Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_fair_trade(company,nlp=False):\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup as soup\n",
    "\n",
    "    company = company.replace(\" \",\"+\")\n",
    "    fair_trade_url = f\"https://www.fairtradecertified.org/search/fair-trade-products?product_type=All&name={company}\"\n",
    "    user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36\"\n",
    "    company = company.replace(\"+\",\" \")\n",
    "\n",
    "    headers = {\"user-agent\":user_agent}\n",
    "    resp = requests.get(fair_trade_url, headers=headers)\n",
    "\n",
    "    if resp.status_code==200:\n",
    "        page_soup = soup(resp.content,\"html.parser\")\n",
    "\n",
    "    companies = page_soup.findAll(\"h3\")    \n",
    "    if len(companies)==0:\n",
    "        return None\n",
    "    \n",
    "    if not nlp:\n",
    "        return [company.text for company in companies]\n",
    "    else:\n",
    "\n",
    "        from nltk.corpus import stopwords \n",
    "        from nltk.tokenize import word_tokenize\n",
    "\n",
    "        # default input to similarity function, only need to tokenize and find stopwords once\n",
    "        X_list = word_tokenize(company) \n",
    "        sw = stopwords.words(\"english\") \n",
    "\n",
    "        ## NLP filtration method for products\n",
    "        def check_comp(company_name,X_list=X_list,sw=sw):\n",
    "\n",
    "            l1 =[];l2 =[] \n",
    "\n",
    "            # tokenization \n",
    "\n",
    "            Y_list = word_tokenize(company_name) \n",
    "\n",
    "            # sw contains the list of stopwords \n",
    "\n",
    "\n",
    "            # remove stop words from string \n",
    "            X_set = {w for w in X_list if not w in sw} \n",
    "            Y_set = {w for w in Y_list if not w in sw} \n",
    "\n",
    "            # form a set containing keywords of both strings \n",
    "            rvector = X_set.union(Y_set) \n",
    "            for w in rvector: \n",
    "                if w in X_set:\n",
    "                    l1.append(1) # create a vector \n",
    "                else:\n",
    "                    l1.append(0) \n",
    "                if w in Y_set:\n",
    "                    l2.append(1) \n",
    "                else:\n",
    "                    l2.append(0) \n",
    "\n",
    "            c = 0\n",
    "\n",
    "            # cosine formula \n",
    "            for i in range(len(rvector)): \n",
    "                c+= l1[i]*l2[i] \n",
    "            cosine = c / float((sum(l1)*sum(l2))**0.5)\n",
    "            return cosine\n",
    "\n",
    "\n",
    "        \n",
    "        output_list = []\n",
    "        for company in companies:\n",
    "            company = company.text\n",
    "            similarity_score = check_comp(company)\n",
    "            if similarity_score >= len(output_list)*0.05 + 0.3:\n",
    "                output_list.append(company)\n",
    "        return output_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Peet's Coffee & Tea\"]\n"
     ]
    }
   ],
   "source": [
    "print(check_fair_trade(\"Peet's\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECHO API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests \n",
    "\n",
    "def evaluate_facility(mini_dict):\n",
    "    status = mini_dict[\"CURR_COMP_STATUS\"]\n",
    "    if \"serious\" in status:\n",
    "        return 2\n",
    "    elif \"(s)\" in status:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def violation_score(return_list):\n",
    "    score=0\n",
    "    for mini_dict in return_list:\n",
    "        score+=evaluate_facility(mini_dict)\n",
    "    return score\n",
    "\n",
    "def ECHO_violations(brand):\n",
    "    brand = brand.upper()\n",
    "    print(brand)\n",
    "    api_url = f\"https://enviro.epa.gov/enviro/efservice/t_compliance_echo/NAME/CONTAINING/{brand}/JSON\"\n",
    "    response = requests.get(api_url)\n",
    "    if response.status_code == 200:\n",
    "        return_list = json.loads(response.text)\n",
    "        if len(return_list)==0:\n",
    "            return None\n",
    "        else:\n",
    "            vio_ratio = violation_score(return_list)/len(return_list)\n",
    "            return vio_ratio*5  \n",
    "    else:\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FORAGER\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(ECHO_violations(\"Forager\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-GMO Project Check \n",
    "still need to rewrite so that will check brand, not product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonGMO(product,brand,nlp=False,first_try=True):\n",
    "    \n",
    "    import json\n",
    "    \n",
    "    combo = brand+\" \"+product\n",
    "    combo = combo.strip()\n",
    "    api_url = f\"https://ws2.nongmoproject.org/api/v1/get_brands_products_by_keyword?keyword={combo}&page=1\"\n",
    "    response = requests.get(api_url)\n",
    "    if response.status_code==200:\n",
    "        results_dict = json.loads(response.text)\n",
    "        results_list = results_dict[\"data\"]\n",
    "    else:\n",
    "        return None\n",
    "        \n",
    "\n",
    "\n",
    "    if nlp:\n",
    "\n",
    "        from nltk.corpus import stopwords \n",
    "        from nltk.tokenize import word_tokenize\n",
    "\n",
    "        # default input to similarity function, only need to tokenize and find stopwords once\n",
    "        X_list = word_tokenize(product) \n",
    "        sw = stopwords.words(\"english\") \n",
    "\n",
    "        ## NLP filtration method for products\n",
    "        def check_comp(company_name,X_list=X_list,sw=sw):\n",
    "\n",
    "            l1 =[];l2 =[] \n",
    "\n",
    "            # tokenization \n",
    "\n",
    "            Y_list = word_tokenize(company_name) \n",
    "\n",
    "            # sw contains the list of stopwords \n",
    "\n",
    "\n",
    "            # remove stop words from string \n",
    "            X_set = {w for w in X_list if not w in sw} \n",
    "            Y_set = {w for w in Y_list if not w in sw} \n",
    "\n",
    "            # form a set containing keywords of both strings \n",
    "            rvector = X_set.union(Y_set) \n",
    "            for w in rvector: \n",
    "                if w in X_set:\n",
    "                    l1.append(1) # create a vector \n",
    "                else:\n",
    "                    l1.append(0) \n",
    "                if w in Y_set:\n",
    "                    l2.append(1) \n",
    "                else:\n",
    "                    l2.append(0) \n",
    "\n",
    "            c = 0\n",
    "\n",
    "            # cosine formula \n",
    "            for i in range(len(rvector)): \n",
    "                c+= l1[i]*l2[i] \n",
    "            cosine = c / float((sum(l1)*sum(l2))**0.5)\n",
    "            return cosine\n",
    "\n",
    "\n",
    "    matches = []\n",
    "\n",
    "\n",
    "    for match in results_list:\n",
    "        name = match[\"name\"]\n",
    "        if len(matches)==0 or not nlp:\n",
    "            matches.append({\"name\":name,\"total_products\":match[\"total_products\"]})\n",
    "        else:\n",
    "            similarity_score = check_comp(name)\n",
    "            if similarity_score >= 0.3 + 0.05*len(matches):\n",
    "                matches.append({\"name\":name,\"total_products\":match[\"total_products\"]})\n",
    "\n",
    "    if len(matches)>0:\n",
    "        return matches\n",
    "    elif first_try:\n",
    "        return nonGMO(brand,\"\",nlp,first_try=False)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': \"Annie's\", 'total_products': 114}, {'name': \"Annie's Bakery\", 'total_products': 10}, {'name': \"Annie's Best\", 'total_products': 3}]\n"
     ]
    }
   ],
   "source": [
    "print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': \"Annie's\", 'total_products': 114}, {'name': \"Annie's Bakery\", 'total_products': 10}, {'name': \"Annie's Best\", 'total_products': 3}]\n"
     ]
    }
   ],
   "source": [
    "print(nonGMO(\"bunnies\",\"Annie's\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CDP A-List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CDP_A(company,nlp=False):\n",
    "    import json\n",
    "    with open(\"cdp_dict.json\",\"r\") as json_file:\n",
    "        cdp_dict = json.loads(json_file.readline())\n",
    "    \n",
    "    possible = []\n",
    "    company = company.lower()\n",
    "    \n",
    "    for key in cdp_dict.keys():\n",
    "        if company in key.lower() or key.lower() in company:\n",
    "            possible.append([key,cdp_dict[key]])\n",
    "    if len(possible)==0:\n",
    "        return None\n",
    "    elif len(possible)==1:\n",
    "        return possible[0][1]\n",
    "    else:\n",
    "        if not nlp:\n",
    "            added = 0\n",
    "            for option in possible:\n",
    "                added+=option[1]\n",
    "            return added/len(possible)\n",
    "        else:\n",
    "            from nltk.corpus import stopwords \n",
    "            from nltk.tokenize import word_tokenize\n",
    "\n",
    "            # default input to similarity function, only need to tokenize and find stopwords once\n",
    "            X_list = word_tokenize(company) \n",
    "            sw = stopwords.words(\"english\") \n",
    "\n",
    "            ## NLP filtration method for products\n",
    "            def check_comp(company_name,X_list=X_list,sw=sw):\n",
    "\n",
    "                l1 =[];l2 =[] \n",
    "\n",
    "                # tokenization \n",
    "\n",
    "                Y_list = word_tokenize(company_name) \n",
    "\n",
    "                # sw contains the list of stopwords \n",
    "\n",
    "\n",
    "                # remove stop words from string \n",
    "                X_set = {w for w in X_list if not w in sw} \n",
    "                Y_set = {w for w in Y_list if not w in sw} \n",
    "\n",
    "                # form a set containing keywords of both strings \n",
    "                rvector = X_set.union(Y_set) \n",
    "                for w in rvector: \n",
    "                    if w in X_set:\n",
    "                        l1.append(1) # create a vector \n",
    "                    else:\n",
    "                        l1.append(0) \n",
    "                    if w in Y_set:\n",
    "                        l2.append(1) \n",
    "                    else:\n",
    "                        l2.append(0) \n",
    "\n",
    "                c = 0\n",
    "\n",
    "                # cosine formula \n",
    "                for i in range(len(rvector)): \n",
    "                    c+= l1[i]*l2[i] \n",
    "                cosine = c / float((sum(l1)*sum(l2))**0.5)\n",
    "                return cosine\n",
    "            \n",
    "            round2_dict = {}\n",
    "            for option in possible:\n",
    "                sim_score = check_comp(option[0])\n",
    "                round2_dict[sim_score] = option\n",
    "            best_list = round2_dict[max(round2_dict.keys())]\n",
    "            return best_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(CDP_A(\"Deutsche\",nlp=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rainforest Alliance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ra_check(brand):\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup as soup\n",
    "\n",
    "    ra_url = f\"https://www.rainforest-alliance.org/find-certified?location=330&category=&keyword={brand}&op=submit\"\n",
    "    user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36\"\n",
    "\n",
    "    headers = {\"user-agent\":user_agent}\n",
    "    resp = requests.get(ra_url, headers=headers)\n",
    "\n",
    "    if resp.status_code!=200:\n",
    "        return None\n",
    "    else:\n",
    "        page_soup = soup(resp.text,\"html.parser\")\n",
    "        \n",
    "    companies = page_soup.findAll(\"div\",{\"class\":\"cp-teaser-info-content\"})\n",
    "    \n",
    "    useful_dict = {}\n",
    "    for company in companies:\n",
    "        comp_name = company.h4.span.text\n",
    "        products = company.div.findAll(\"span\")\n",
    "        cleaned_products = []\n",
    "        for product in products:\n",
    "            if products.index(product)!=0:\n",
    "                cleaned_products.append(product.text.strip().strip(\",\"))\n",
    "        useful_dict[comp_name] = cleaned_products\n",
    "        \n",
    "        \n",
    "    return useful_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"Peet's Coffee & Tea\": ['Coffee']}\n"
     ]
    }
   ],
   "source": [
    "print(ra_check(\"Peet's\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coffee\n",
      "\n",
      "Coffee\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
