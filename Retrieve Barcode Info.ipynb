{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\programdata\\anaconda3\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\programdata\\anaconda3\\lib\\site-packages (from bs4) (4.8.0)\n",
      "Requirement already satisfied: soupsieve>=1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (1.9.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (2.22.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (1.24.2)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2019.9.11)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen as uReq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as soup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creates a dictionary with all known info from upcitemdb.com\n",
    "\n",
    "def lookup(query):\n",
    "    ## append search UPC to the database website\n",
    "    my_url = \"https://www.upcitemdb.com/upc/\"+query\n",
    "    \n",
    "    #open urllib client, accounting for page not-existing\n",
    "    try:\n",
    "        uClient = uReq(my_url)\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "    # grab page html, save as soup\n",
    "    page_html = uClient.read()\n",
    "    uClient.close()\n",
    "    page_soup = soup(page_html,\"html.parser\")\n",
    "    \n",
    "    # find the table of item details, use first bc only 1\n",
    "    container = page_soup.findAll(\"table\",{\"class\":\"detail-list\"})[0]\n",
    "    \n",
    "    ## create blank dictionary\n",
    "    info_dict={}\n",
    "    \n",
    "    ## find product name\n",
    "    \n",
    "    # climb into blah is associated with blah\n",
    "    pieces = str(page_soup.head.findAll(\"meta\")[1].meta).split(\",\")\n",
    "    #get rid of first thing\n",
    "    pieces.pop(0)\n",
    "    \n",
    "    #create empty string\n",
    "    output_str = \"\"\n",
    "    # a will serve as a toggle switch to append pieces to output or not\n",
    "    a=True\n",
    "    # going through each term, toggle off a once we hit upc lookup database\n",
    "    for piece in pieces:    \n",
    "        if piece==\" upc lookup database\":\n",
    "            a=False\n",
    "        if a:\n",
    "            output_str+=piece\n",
    "    \n",
    "    def clean(x):\n",
    "        x=x.replace(\"&amp\",\"&\")\n",
    "        for char in x:\n",
    "            if char.isnumeric():\n",
    "                tokens = x.split(char)\n",
    "                return tokens[0]\n",
    "    info_dict[\"product name\"] = clean(output_str)\n",
    "\n",
    "    \n",
    "    ## find all other info\n",
    "    trs = container.findAll(\"tr\")\n",
    "    \n",
    "    ## loop through info things\n",
    "    for tr in trs:\n",
    "        \n",
    "        # make list, containing key line and value line\n",
    "        tds = tr.findAll(\"td\")\n",
    "        \n",
    "        ## parse out key and value, save to info dict\n",
    "        key = str(tds[0]).strip(\"<td>\").strip(\"</\")\n",
    "        value = str(tds[1]).strip(\"<td>\").strip(\"</\").strip().strip(\"\\t\")\n",
    "        # don't care ab last-scanned\n",
    "        if key != \"Last Scanned:\":\n",
    "            info_dict[key]=value\n",
    "    return info_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'product name': \" Annie's Homegrown Organic Pretzel Bunnies 7 oz Pack of 12\",\n",
       " 'UPC-A:': '0 13562 30058 7',\n",
       " 'EAN-13:': '0 013562 300587',\n",
       " 'Amazon ASIN:': 'B00511MLK0',\n",
       " 'Country of Registration:': 'United States',\n",
       " 'Brand:': \"Annie's, Inc.\",\n",
       " 'Model #:': 'SPK-23102',\n",
       " 'Weight:': '10 Pounds',\n",
       " 'Product Dimension:': '9 X 7 X 3 inches'}"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup(\"013562300587\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#open urllib client, accounting for page not-existing\n",
    "\n",
    "uClient = uReq(\"https://www.upcitemdb.com/upc/071641053649\")\n",
    "\n",
    "\n",
    "# grab page html, save as soup\n",
    "page_html = uClient.read()\n",
    "uClient.close()\n",
    "page_soup = soup(page_html,\"html.parser\")\n",
    "\n",
    "pieces = str(page_soup.head.findAll(\"meta\")[1].meta).split(\",\")\n",
    "pieces.pop(0)\n",
    "print(\"pieces before splicing:\")\n",
    "print(pieces)\n",
    "\n",
    "output_str = \"\"\n",
    "a=True\n",
    "for piece in pieces:    \n",
    "    if piece==\" upc lookup database\":\n",
    "        a=False\n",
    "    print(\"piece\",piece,\"a\",a)\n",
    "    if a:\n",
    "        output_str+=piece\n",
    "print(output_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'product name': \" Annie's Low Sodium Mac Mild Cheddar Cheese 6 oz\",\n",
       " 'UPC-A:': '0 13562 30011 2',\n",
       " 'EAN-13:': '0 013562 300112',\n",
       " 'Amazon ASIN:': 'B0047240A8',\n",
       " 'Country of Registration:': 'United States',\n",
       " 'Brand:': \"Annie's\",\n",
       " 'Model #:': 'SPK-1390111',\n",
       " 'Color:': 'Pink',\n",
       " 'Weight:': '0 pounds',\n",
       " 'Product Dimension:': '8 X 3 X 5.5 inches'}"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup(\"013562300112\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_company(brand):\n",
    "\n",
    "    def side_box_check(brand,page_soup):\n",
    "        parents = []\n",
    "        side_box_list = page_soup.findAll(\"div\",{\"class\":\"zloOqf PZPZlf\"})\n",
    "        for item in side_box_list:\n",
    "            if \"Parent\" in str(item):\n",
    "                mini_set = item.findAll(\"span\",{\"class\":\"LrzXr kno-fv\"})\n",
    "                mini_set = str(mini_set[0]).split(\",\")\n",
    "                for f1 in mini_set:\n",
    "                    for snippet in str(f1).split(\">\"):\n",
    "                        if \"</a\" in snippet:\n",
    "                            parents.append(snippet.strip(\"</a\").replace(\"&amp;\",\"&\"))\n",
    "        if len(parents)!=0:\n",
    "            return parents\n",
    "        else:\n",
    "            return table_check(brand,page_soup)\n",
    "        \n",
    "    def table_check(brand,page_soup):\n",
    "        try:\n",
    "            table = page_soup.findAll(\"div\",{\"class\":\"webanswers-webanswers_table__webanswers-table\"})[0]\n",
    "        except:\n",
    "            return lame_box_check(brand,page_soup)\n",
    "        rows = table.findAll(\"tr\")\n",
    "        for tr in rows:\n",
    "            if \"Owner\" in str(tr):\n",
    "                owner_line = str(tr.findAll(\"td\")[1])\n",
    "                trim1 = owner_line.split(\">\")[1]\n",
    "                trim2 = trim1.strip(\"</td>\")\n",
    "                return trim2.split(\",\")\n",
    "    \n",
    "    def lame_box_check(brand,page_soup):\n",
    "        output = []\n",
    "        not_comp = [\"company\",brand,\"parent\"]\n",
    "        try:\n",
    "            info = page_soup.findAll(\"span\",{\"class\":\"e24Kjd\"})[0]\n",
    "        except IndexError:\n",
    "            return brand\n",
    "        bolded = info.findAll(\"b\")\n",
    "        for bold in bolded:\n",
    "            bold = str(bold).strip(\"<b>\").strip(\"</b>\")\n",
    "            if bold not in not_comp:\n",
    "                output.append(bold)\n",
    "        for option in output:\n",
    "            if option in brand or brand in option:\n",
    "                output.pop(output.index(option))\n",
    "        return output\n",
    "\n",
    "    \n",
    "    my_url = f\"https://www.google.com/search?q={brand}+parent+company\"\n",
    "    user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36\"\n",
    "\n",
    "    headers = {\"user-agent\":user_agent}\n",
    "    resp = requests.get(my_url, headers=headers)\n",
    "\n",
    "    if resp.status_code == 200:\n",
    "        page_soup = soup(resp.content, \"html.parser\")\n",
    "    else:\n",
    "        return brand\n",
    "    \n",
    "    list_tag = page_soup.findAll(\"a\",{\"class\":\"FLP8od\"})\n",
    "    if len(list_tag)==0:\n",
    "        list_tag = page_soup.findAll(\"div\",{\"class\":\"Z0LcW AZCkJd\"})\n",
    "        if len(list_tag)==0:\n",
    "            list_tag = page_soup.findAll(\"div\",{\"class\":\"Z0LcW\"})\n",
    "            if len(list_tag)==0:\n",
    "                output = []\n",
    "                intake = side_box_check(brand,page_soup)\n",
    "                if isinstance(intake,str):\n",
    "                    return intake\n",
    "                for item in intake:\n",
    "                    output.append(item.replace(\"&amp;\",\"&\"))\n",
    "                return output\n",
    "    full_tag = str(list_tag[0])\n",
    "    splitted = full_tag.split(\">\")\n",
    "    pre_company = splitted[1]\n",
    "    post_company = pre_company.split(\"</\")[0]\n",
    "    return [post_company.replace(\"&amp;\",\"&\")]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['General Mills']\n"
     ]
    }
   ],
   "source": [
    "print(find_company(\"Annie's, Inc.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ingredients(product):\n",
    "    import requests\n",
    "    my_api_key = \"9ZTbuCeX9dP5keqlGJZyWNdg1c1xCu3p9pLhJ1MS\"\n",
    "    query = product.replace(\" \",\"%20\")\n",
    "    response = requests.get(f\"https://api.nal.usda.gov/fdc/v1/foods/search?api_key={my_api_key}&query={query}\")\n",
    "    big_dict = response.json()\n",
    "    foods = big_dict[\"foods\"]\n",
    "    for food in foods:\n",
    "        if \"ingredients\" in food.keys():\n",
    "            ingred_str = food[\"ingredients\"]\n",
    "            ingredients = ingred_str.lower().split(\",\")\n",
    "            return ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(find_ingredients(\"Naked strawberry banana\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DOWNLOAD ZIP FILE\n",
    "\n",
    "response = requests.get(f\"https://organicapi.ams.usda.gov/IntegrityPubDataServices/OidPublicDataService.svc/rest/GetAllOperationsPublicData?api_key={my_api_key}\",stream=True)\n",
    "target_path = 'organic_ops.zip'\n",
    "handle = open(target_path, \"wb\")\n",
    "for chunk in response.iter_content(chunk_size=512):\n",
    "    if chunk:  # filter out keep-alive new chunks\n",
    "        handle.write(chunk)\n",
    "handle.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as soup\n",
    "\n",
    "my_api_key = \"9ZTbuCeX9dP5keqlGJZyWNdg1c1xCu3p9pLhJ1MS\"\n",
    "json = {\"startldx\":\"1\",\"count\":\"1000\",\"countries\":[\"USA\"]}\n",
    "response = requests.post(f\"https://organicapi.ams.usda.gov/IntegrityPubDataServices/OidPublicDataService.svc/rest/Operations?api_key={my_api_key}\",json=json)\n",
    "all_text = response.text\n",
    "\n",
    "page_soup = soup(all_text,\"lxml\")\n",
    "page2 = soup(str(page_soup),\"html.parser\")\n",
    "operations_list = page2.html.body.operationsresult.operations.findAll(\"a:operation\")\n",
    "\n",
    "state_dict = {}\n",
    "def midinfo(x):\n",
    "    x = str(x)\n",
    "    return x.split(\">\")[1].split(\"<\")[0]\n",
    "for operation in operations_list:\n",
    "    operationname = midinfo(operation.findAll(\"a:operationname\")[0])\n",
    "    operationstatus = midinfo(operation.findAll(\"a:nopoperationstatus\")[0])\n",
    "    addresses = operation.findAll(\"a:addresses\")\n",
    "    address = addresses[0].findAll(\"a:address\")\n",
    "    state_untrimmed = str(address[0].findAll(\"a:stateorprovince\")[0])\n",
    "    state = midinfo(state_untrimmed)\n",
    "    print(operationname,operationstatus,state)\n",
    "    if state in state_dict.keys():\n",
    "        state_dict[state]+=1\n",
    "    else:\n",
    "        state_dict[state]=1\n",
    "print(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all_text = \"\"\"    <Operation>\n",
    "      <op_certifierName>[OTCO] Oregon Tilth Certified Organic</op_certifierName>\n",
    "      <op_nopOpID>8150000053</op_nopOpID>\n",
    "      <op_name>Wong Potatoes, Inc</op_name>\n",
    "      <op_clientID>OT-017900</op_clientID>\n",
    "      <op_contFirstName>Daniel</op_contFirstName>\n",
    "      <op_contLastName>Chin</op_contLastName>\n",
    "      <op_status>Certified</op_status>\n",
    "      <op_statusEffectiveDate>2014-11-06</op_statusEffectiveDate>\n",
    "      <op_nopAnniversaryDate>2020-04-01</op_nopAnniversaryDate>\n",
    "      <op_lastUpdatedDate>2019-05-15</op_lastUpdatedDate>\n",
    "      <opSC_HANDLING>Certified</opSC_HANDLING>\n",
    "      <opSC_HANDLING_ED>2014-11-06</opSC_HANDLING_ED>\n",
    "      <op_phone>(541) 798-5353</op_phone>\n",
    "      <op_email>chinfarms@gmail.com</op_email>\n",
    "      <opMA_line1>17600 Hwy 39</opMA_line1>\n",
    "      <opMA_city>Klamath Falls</opMA_city>\n",
    "      <opMA_state>Oregon</opMA_state>\n",
    "      <opMA_country>United States of America (the)</opMA_country>\n",
    "      <opMA_zip>97603</opMA_zip>\n",
    "    </Operation>\n",
    "    <Operation>\n",
    "      <op_certifierName>[OTCO] Oregon Tilth Certified Organic</op_certifierName>\n",
    "      <op_nopOpID>8150000052</op_nopOpID>\n",
    "      <op_name>High Mountain LLC</op_name>\n",
    "      <op_clientID>OT-004440</op_clientID>\n",
    "      <op_contFirstName>Kevin</op_contFirstName>\n",
    "      <op_contLastName>Christensen</op_contLastName>\n",
    "      <op_status>Suspended</op_status>\n",
    "      <op_statusEffectiveDate>2011-11-16</op_statusEffectiveDate>\n",
    "      <op_lastUpdatedDate>2017-12-27</op_lastUpdatedDate>\n",
    "      <opSC_CR>Suspended</opSC_CR>\n",
    "      <opMA_line1>PO Box 968</opMA_line1>\n",
    "      <opMA_city>Mattawa</opMA_city>\n",
    "      <opMA_state>Washington</opMA_state>\n",
    "      <opMA_country>United States of America (the)</opMA_country>\n",
    "      <opMA_zip>99349</opMA_zip>\n",
    "    </Operation>\n",
    "    <Operation>\n",
    "      <op_certifierName>[OTCO] Oregon Tilth Certified Organic</op_certifierName>\n",
    "      <op_nopOpID>8150000051</op_nopOpID>\n",
    "      <op_name>Marvin Lynch</op_name>\n",
    "      <op_otherNames>DBA O'lynch Dairy</op_otherNames>\n",
    "      <op_clientID>OT-007458</op_clientID>\n",
    "      <op_contFirstName>Marvin</op_contFirstName>\n",
    "      <op_contLastName>Lynch</op_contLastName>\n",
    "      <op_status>Certified</op_status>\n",
    "      <op_statusEffectiveDate>2009-05-04</op_statusEffectiveDate>\n",
    "      <op_nopAnniversaryDate>2020-01-01</op_nopAnniversaryDate>\n",
    "      <op_lastUpdatedDate>2019-05-15</op_lastUpdatedDate>\n",
    "      <opSC_CR>Certified</opSC_CR>\n",
    "      <opSC_CR_ED>2009-05-04</opSC_CR_ED>\n",
    "      <opSC_LS>Certified</opSC_LS>\n",
    "      <opSC_LS_ED>2009-05-04</opSC_LS_ED>\n",
    "      <op_phone>(563) 852-5285</op_phone>\n",
    "      <op_email>marvlynch@yahoo.com</op_email>\n",
    "      <opMA_line1>24764 Hwy 151 W</opMA_line1>\n",
    "      <opMA_city>Cascade</opMA_city>\n",
    "      <opMA_state>Iowa</opMA_state>\n",
    "      <opMA_country>United States of America (the)</opMA_country>\n",
    "      <opMA_zip>52033</opMA_zip>\n",
    "    </Operation>\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "huge_dict={}\n",
    "all_text = all_text.replace(\"\\n\",\"\")\n",
    "all_operations = all_text.split(\"</Operation>\")\n",
    "concerns = [\"op_nopOpID\",\"op_name\",\"opMA_state\"]\n",
    "for operation in all_operations:\n",
    "    if \"United States\" in operation and \"status>Certified<\" in operation:\n",
    "        internal_dict = {}\n",
    "        deets = operation.split(\"</\")\n",
    "        for deet in deets:\n",
    "            for concern in concerns:\n",
    "                if \"<\"+concern+\">\" in deet:\n",
    "                    internal_dict[concern]=deet.split(\">\")[-1]\n",
    "        huge_dict[internal_dict[\"op_name\"]] = internal_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##CREATING DICTIONARY OF ALL ORGANIC OPERATIONS\n",
    "\n",
    "all_text = \"\"\n",
    "op_file = open(\"organic_ops_file.txt\",\"r\",encoding=\"utf-8\")\n",
    "for line in op_file.readlines():\n",
    "    all_text+=line.strip(\"\\n\")\n",
    "op_file.close()\n",
    "\n",
    "huge_dict={}\n",
    "all_text = all_text.replace(\"\\n\",\"\")\n",
    "all_operations = all_text.split(\"</Operation>\")\n",
    "concerns = [\"op_nopOpID\",\"op_name\",\"opMA_state\"]\n",
    "for operation in all_operations:\n",
    "    if \"United States\" in operation and \"status>Certified<\" in operation:\n",
    "        internal_dict = {}\n",
    "        deets = operation.split(\"</\")\n",
    "        for deet in deets:\n",
    "            for concern in concerns:\n",
    "                if \"<\"+concern+\">\" in deet:\n",
    "                    internal_dict[concern]=deet.split(\">\")[-1]\n",
    "        huge_dict[internal_dict[\"op_name\"]] = internal_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WRITING INTO JSON FILE\n",
    "\n",
    "import json\n",
    "\n",
    "json_format = json.dumps(mega_dict)\n",
    "f = open(\"v2_organic_operations_dict.json\",\"w\")\n",
    "f.write(json_format)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import json\n",
    "\n",
    "def create_dict(filename=\"v2_organic_operations_dict.json\"):    \n",
    "    with open(filename,\"r\") as json_file:\n",
    "        mega_dict = json.load(json_file)\n",
    "        return mega_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## trying to fix capitalization in the dictionary\n",
    "import string\n",
    "mega_dict = {}\n",
    "for key in huge_dict.keys():\n",
    "    if key.isupper():\n",
    "        fixed = string.capwords(key)\n",
    "        huge_dict[key][\"op_name\"] = fixed\n",
    "        mega_dict[fixed] = huge_dict[key]\n",
    "    else:\n",
    "        mega_dict[key] = huge_dict[key]\n",
    "print(mega_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_operation_id(brand,mega_dict=mega_dict):\n",
    "    brand = brand.replace(\",\",\"\")\n",
    "    found_keys = []\n",
    "    dict_keys=mega_dict.keys()\n",
    "    for key in dict_keys:\n",
    "        if brand in key:\n",
    "            found_keys.insert(0,[key,mega_dict[key][\"op_nopOpID\"]])\n",
    "    if len(found_keys)!=0:\n",
    "        return found_keys\n",
    "    else:\n",
    "        for term in brand.split():\n",
    "            found_keys.extend(find_operation_id(term,mega_dict))\n",
    "        found_keys.extend(find_operation_id(find_company(brand),mega_dict))\n",
    "        return found_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\"Annie's Inc.\", '8150000355']]"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_operation_id(\"Annie's, Inc.\",\"General Mills\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Receiving Operation Products based on op_code\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as soup\n",
    "#\n",
    "op_id = \"5561001956\"\n",
    "#\n",
    "my_api_key = \"9ZTbuCeX9dP5keqlGJZyWNdg1c1xCu3p9pLhJ1MS\"\n",
    "json = {\"startIdx\":\"1\",\"count\":\"1000\",\"operationId\":op_id}\n",
    "response = requests.post(f\"https://organicapi.ams.usda.gov/IntegrityPubDataServices/OidPublicDataService.svc/rest/Items?api_key={my_api_key}\",json=json)\n",
    "# parsing into a list of items\n",
    "all_text = response.text\n",
    "page_soup = soup(all_text,\"lxml\")\n",
    "page2 = soup(str(page_soup),\"html.parser\")\n",
    "\n",
    "items = page2.findAll(\"a:item\")\n",
    "\n",
    "if len(items)==0:\n",
    "    return \"Error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Receiving Operation Products based on op_code\n",
    "\n",
    "def find_itemvarieties(item):\n",
    "    deets = item.split(\"</\")\n",
    "    for deet in deets:\n",
    "        if \"<a:itemvarieties>\" in deet:\n",
    "            itemvarieties = deet.split(\">\")[-1]\n",
    "            return itemvarieties\n",
    "\n",
    "def organic_info(op_id,product,brand):\n",
    "    \n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup as soup\n",
    "    from nltk.corpus import stopwords \n",
    "    from nltk.tokenize import word_tokenize \n",
    "\n",
    "    # get rid of common words that are clutter, this list should grow with testing\n",
    "    trash_list = [\"Organic \",brand,company,\"Super \",\"Green \",\"Ounce\",\"ounce\",\"count\",\"Count\"]\n",
    "    for stink in trash_list:\n",
    "        product = product.replace(stink,\"\").strip()\n",
    "    for word in product.split():\n",
    "        for stink in trash_list:\n",
    "            if stink in word or word.isnumeric():\n",
    "                product = product.replace(word,\"\").strip()\n",
    "    product = product.split(\",\")[0]\n",
    "    product = product.replace(brand,\"\").strip()\n",
    "    \n",
    "    # default input to similarity function, only need to tokenize and find stopwords once\n",
    "    X_list = word_tokenize(product) \n",
    "    sw = stopwords.words(\"english\") \n",
    "    \n",
    "    ## NLP filtration method for products\n",
    "    def check_item(itemvarieties,X_list=X_list,sw=sw):\n",
    "\n",
    "        l1 =[];l2 =[] \n",
    "\n",
    "        # tokenization \n",
    "\n",
    "        Y_list = word_tokenize(itemvarieties) \n",
    "\n",
    "        # sw contains the list of stopwords \n",
    "\n",
    "\n",
    "        # remove stop words from string \n",
    "        X_set = {w for w in X_list if not w in sw} \n",
    "        Y_set = {w for w in Y_list if not w in sw} \n",
    "\n",
    "        # form a set containing keywords of both strings \n",
    "        rvector = X_set.union(Y_set) \n",
    "        for w in rvector: \n",
    "            if w in X_set:\n",
    "                l1.append(1) # create a vector \n",
    "            else:\n",
    "                l1.append(0) \n",
    "            if w in Y_set:\n",
    "                l2.append(1) \n",
    "            else:\n",
    "                l2.append(0) \n",
    "\n",
    "        c = 0\n",
    "\n",
    "        # cosine formula \n",
    "        for i in range(len(rvector)): \n",
    "            c+= l1[i]*l2[i] \n",
    "        cosine = c / float((sum(l1)*sum(l2))**0.5)\n",
    "        return cosine\n",
    "    \n",
    "    ## API request\n",
    "    my_api_key = \"9ZTbuCeX9dP5keqlGJZyWNdg1c1xCu3p9pLhJ1MS\"\n",
    "    json = {\"startIdx\":\"1\",\"count\":\"1000\",\"operationId\":op_id}\n",
    "    response = requests.post(f\"https://organicapi.ams.usda.gov/IntegrityPubDataServices/OidPublicDataService.svc/rest/Items?api_key={my_api_key}\",json=json)\n",
    "\n",
    "    # parsing into a list of items\n",
    "    all_text = response.text\n",
    "    page_soup = soup(all_text,\"lxml\")\n",
    "    page2 = soup(str(page_soup),\"html.parser\")\n",
    "    items = page2.findAll(\"a:item\")\n",
    "\n",
    "    if len(items)==0:\n",
    "        return None\n",
    "\n",
    "    # TEST INFORMATION\n",
    "    #company = \"Forager Project\"\n",
    "    #brand = \"Forager Project\"\n",
    "    #product = \"Super Green Leafy Greens Chips\"\n",
    "    #\n",
    "\n",
    "    # create list of API response terms we care about\n",
    "    concerns = [\"itemname\",\"itemvarieties\",\"madewithorganic\",\"organic\",\"organic100\"]\n",
    "    possible_matches = []\n",
    "    for item in items:\n",
    "        cont = True\n",
    "        item = str(item)\n",
    "        if \"status>Certified<\" in item:\n",
    "            try:\n",
    "                itemvarieties = find_itemvarieties(item)\n",
    "                similarity_score = check_item(itemvarieties)\n",
    "            except:\n",
    "                cont=False\n",
    "            if cont and similarity_score>=0.3:\n",
    "                internal_dict = {\"similarity_score\":similarity_score}\n",
    "                deets = item.split(\"</\")\n",
    "                for deet in deets:\n",
    "                    for concern in concerns:\n",
    "                        if \"<a:\"+concern+\">\" in deet:\n",
    "                            internal_dict[concern]=deet.split(\">\")[-1]\n",
    "                possible_matches.append(internal_dict)\n",
    "    return possible_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # get rid of common words that are clutter, this list should grow with testing\n",
    "    trash_list = [\"Organic \",brand,company,\"Super \",\"Green \",\"Ounce\",\"ounce\",\"count\",\"Count\"]\n",
    "    for stink in trash_list:\n",
    "        product = product.replace(stink,\"\").strip()\n",
    "    for word in product.split():\n",
    "        for stink in trash_list:\n",
    "            if stink in word or word.isnumeric():\n",
    "                product = product.replace(word,\"\").strip()\n",
    "\n",
    "    #print(\"product before recombo is\",product)\n",
    "\n",
    "    # creating different product variations to test against\n",
    "    product_combos = [product]    \n",
    "    for word in product.split():\n",
    "        product_combos.append(product.replace(word,\"\").strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'similarity_score': 0.31622776601683794,\n",
       "  'itemname': 'Other',\n",
       "  'itemvarieties': 'Pretzels Bunnies',\n",
       "  'organic': 'true'},\n",
       " {'similarity_score': 0.31622776601683794,\n",
       "  'itemname': 'Other',\n",
       "  'itemvarieties': 'Cheddar Bunnies',\n",
       "  'organic': 'true'},\n",
       " {'similarity_score': 0.31622776601683794,\n",
       "  'itemname': 'Other',\n",
       "  'itemvarieties': 'Cheddar Bunnies',\n",
       "  'madewithorganic': 'true'},\n",
       " {'similarity_score': 0.31622776601683794,\n",
       "  'itemname': 'Other',\n",
       "  'itemvarieties': 'Muddy Bunnies',\n",
       "  'organic': 'true'},\n",
       " {'similarity_score': 0.31622776601683794,\n",
       "  'itemname': 'Other',\n",
       "  'itemvarieties': 'Cocoa Bunnies',\n",
       "  'organic': 'true'},\n",
       " {'similarity_score': 0.31622776601683794,\n",
       "  'itemname': 'Other',\n",
       "  'itemvarieties': 'Berry Bunnies',\n",
       "  'organic': 'true'}]"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "organic_info(\"8150000355\",\"Annie's Homegrown Organic Pretzel Bunnies, 7 oz, Pack of 12\",\"Annie's Inc.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import nltk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nltk.download(\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Program to measure similarity between \n",
    "# two sentences using cosine similarity. \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "product = \"Super Green Leafy Greens Chips\"\n",
    "\n",
    "# default input to function, only need to tokenize and find stopwords once\n",
    "X_list = word_tokenize(product) \n",
    "sw = stopwords.words('english') \n",
    "\n",
    "def compare_terms(itemvarieties,X_list=X_list,sw=sw):\n",
    "    \n",
    "    l1 =[];l2 =[] \n",
    "\n",
    "    # tokenization \n",
    "\n",
    "    Y_list = word_tokenize(itemvarieties) \n",
    "\n",
    "    # sw contains the list of stopwords \n",
    "\n",
    "\n",
    "    # remove stop words from string \n",
    "    X_set = {w for w in X_list if not w in sw} \n",
    "    Y_set = {w for w in Y_list if not w in sw} \n",
    "\n",
    "    # form a set containing keywords of both strings \n",
    "    rvector = X_set.union(Y_set) \n",
    "    for w in rvector: \n",
    "        if w in X_set:\n",
    "            l1.append(1) # create a vector \n",
    "        else:\n",
    "            l1.append(0) \n",
    "        if w in Y_set:\n",
    "            l2.append(1) \n",
    "        else:\n",
    "            l2.append(0) \n",
    "\n",
    "    c = 0\n",
    "\n",
    "    # cosine formula \n",
    "    for i in range(len(rvector)): \n",
    "        c+= l1[i]*l2[i] \n",
    "    cosine = c / float((sum(l1)*sum(l2))**0.5) \n",
    "    return cosine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please Enter the Barcode of the product you wish to scan:\n",
      "013562300587\n"
     ]
    }
   ],
   "source": [
    "code = input(\"Please Enter the Barcode of the product you wish to scan:\\n\")\n",
    "code_info = lookup(code)\n",
    "product= code_info[\"product name\"]\n",
    "brand = code_info[\"Brand:\"]\n",
    "op_id = find_operation_id(brand)[0][1]\n",
    "org_inf = organic_info(op_id,product,brand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'similarity_score': 0.31622776601683794, 'itemname': 'Other', 'itemvarieties': 'Pretzels Bunnies', 'organic': 'true'}, {'similarity_score': 0.31622776601683794, 'itemname': 'Other', 'itemvarieties': 'Cheddar Bunnies', 'organic': 'true'}, {'similarity_score': 0.31622776601683794, 'itemname': 'Other', 'itemvarieties': 'Cheddar Bunnies', 'madewithorganic': 'true'}, {'similarity_score': 0.31622776601683794, 'itemname': 'Other', 'itemvarieties': 'Muddy Bunnies', 'organic': 'true'}, {'similarity_score': 0.31622776601683794, 'itemname': 'Other', 'itemvarieties': 'Cocoa Bunnies', 'organic': 'true'}, {'similarity_score': 0.31622776601683794, 'itemname': 'Other', 'itemvarieties': 'Berry Bunnies', 'organic': 'true'}]\n"
     ]
    }
   ],
   "source": [
    "print(org_inf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
